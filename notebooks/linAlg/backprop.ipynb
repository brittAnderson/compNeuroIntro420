{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warm up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. What is a neural network?\n",
    "2. What is the difference between supervised and unsupervised learning? Give an example of each? Does anyone know an example other than what we have discussed in class for unsupervised learning?\n",
    "3. What is the activation function we have used for the perceptron and delta rule networks?\n",
    "4. What role does \"error\" play in the perceptron and delta learning rules?\n",
    "5. For a multilayer network how do you know how much of the \"error\" to pass back into the deeper layers of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some Answers\n",
    "\n",
    "1. nodes and edges and a whole bunch of other options.\n",
    "2. Supervised we are given the desired outputs to learn. Perceptron and Hopfield.\n",
    "3. Threshold function.\n",
    "4. It is part of the weight updating equation.\n",
    "5. That is what we will be working on today.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sigmoid Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our prior networks have been forms of threshold units. Check to see if our activation cleared a certain hurdle, and if so set its value to 1 or -1. But it is more common to scale the output continuously between a lower and upper bound. One of the intuitions is that this is like a probability that the neuron might fire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is a sigmoid function and what does it look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIZW7iIgXUrmLiHghlbuIiBf6fyiQXiPnM+RoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f8cc73c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyp\n",
    "def sig (z): return 1.0/(1 + np.exp(-1*z))\n",
    "xs = np.linspace(-5,5,101)\n",
    "ys = list(map(sig,xs))\n",
    "pyp.plot(xs,ys)\n",
    "pyp.text(.1,.1,\"Plot of the Sigmoid Function\")\n",
    "pyp.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Map**\n",
    "Many concepts from functional programming will make your code more readable and concise. Here I use the `map` function to apply a function to all the items in an array. For technical reasons I have to force this to be a list for matplotlib to plot it. But note how much shorter is this than a big for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More details on programming in a functional style in Python 3 can be found [here](https://docs.python.org/3/howto/functional.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A few questions**\n",
    "   1. Why is it called \"sigmoid?\"\n",
    "   2. What advantage does it offer over a threshold function?\n",
    "   3. Is it the only \"sigmoid\" function? Does it have other names?\n",
    "   4. Can you guess an an advantage to this particular form of the equation?\n",
    "   5. How do use this with a neural network, i.e. what is $z$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some Answers\n",
    "\n",
    "1. S shaped\n",
    "2. Differentiable\n",
    "3. No,\n",
    "![Sigmoid Functions](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Gjl-t%28x%29.svg/1000px-Gjl-t%28x%29.svg.png \"Sigmoid Functions Illustrated\")\n",
    "4. The exponential gives it some nice differentiation properties. Can you differentiate it? Go ahead. Why would you care about knowing the derivative?\n",
    "\n",
    " What do you need to know?\n",
    "\n",
    " a. Differentiate a quotient.\n",
    "\n",
    " b. Chain rule.\n",
    "\n",
    " c. Derivatives of sums and sums of derivatives\n",
    "\n",
    " d. Fancy rearrangements.\n",
    "\n",
    " e. The derivative of $e^z$.\n",
    "5. The weighted input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to use it?\n",
    "\n",
    "Have to input the activation of the neuron into the $z$ of the sigmoid function. That is we need, $\\frac{1}{1+e^{-(\\sum_i xw)}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,1])\n",
    "w = np.array([3,4])\n",
    "zee = np.dot(x,w)\n",
    "def sigmoid (xin):\n",
    "    return(1/(1+np.exp(-xin)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990889488055994"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(x,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Think about equations qualitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Remember, one of the goals of computational modelling is to get insights into the implications of our ideas and theories. Sometimes this means running a model to see what comes out of it. But it can also mean that we look at the equations that go into the model and think about their \"behavior\" to get some sense of how things will behave that have particular functional forms. \n",
    "How might you do that here? Think about how it is the same and differnt from the threshold version. Think about extreme values: what happens at the extremes? Where is the derivative most extreme?  What happens if the dot product of a weight vector and input vector are large? Or very small (and what does small mean here)? What about negative extremes and positive extremes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why are we starting this discussion of the backpropagation algorithim with all this discussion of activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. What is being backpropagated?\n",
    "2. What is it we want our network to do?\n",
    "3. How do we guide it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Answers\n",
    "1. Cost\n",
    "2. Move to the point of minimal cost\n",
    "3. We use the derivative (just like we have all course long).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Why use a cost instead of a single classification of right or wrong like we had been doing?\n",
    "Many networks have a cost function. We may want to know more than just whether you were right or wrong, but how wrong? In a continuous case being \"right\" might not even really be possible - what is the value of $\\pi$\" ? Our computers cannot give render sufficient precision. There is not \"right\" cost function either, but what might you suggest that we use, and why?\n",
    "\n",
    "What would you suggest as the cost function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's always a good guess and a resonable starting point\n",
    "\n",
    "$$C(\\mathbf{w}) = \\frac{1}{2\\mathrm{n}}\\sum_\\mathbf{x} \\|\\mathbf{y}(\\mathbf{x}) - \\mathbf{a}\\|^2.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some Questions:\n",
    "Why isn't this a function of $\\mathbf{x}$ and $\\mathbf{y}$ too?\n",
    "\n",
    "What is the /dimensionality/ of the part of the equation inside the double lines? \n",
    "\n",
    "What do you call the operation characterized by the double lines? (See \"norm\" below).\n",
    "\n",
    "Why is adjusting weights for a multilayer network hard?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some Answers:\n",
    "1. It is, but we can't vary those, so we treat them as constants. \n",
    "\n",
    "2. the dimensionality is that of the output vector\n",
    "\n",
    "3. the norm (how big or how far) - think of Hamming distance from the Hopfield network.\n",
    "\n",
    "4. Because we don't know how to distribute the errors for the intermediate connections. How much /blame/ do we give to each one? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What do we want? How do we get it?\n",
    "\n",
    "What do we want? \n",
    "\n",
    "To make our network get better; that is to come closer to the \"right\" answer. Right is in quotes because what is right may be different in different circumstances. This is operator determined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we get it?\n",
    "\n",
    "1. What is free for us to change?\n",
    "\n",
    "2. How do we determine if our change is for the better?\n",
    "\n",
    "3. Can we determine a way to do it that will just work? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why can't we just do gradient descent?\n",
    "(Someone please explain this).\n",
    "Networks get too big. In theory you could. In practice you can't.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[**Classic Publication** pdf](http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can read this article!\n",
    "\n",
    "You have all the notation, language, and concepts. Note that the abstract makes sense to you. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Explain backpropagation as a tweet (140 characters or less)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is backpropagation biologically plausible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Some Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$w_{jk}^l$ is the weight between the $kth$ neuron in the $l-1$ layer to the $jth$ neuron in the $lth$ layer. \n",
    "Note the ordering of $j's$ and $k's$. It may be backwards from your intuition. \n",
    "\n",
    "How would you write as an equation with the \"sigma\" summation sign the value activation of a single /arbitrary/ neuron in an /arbitrary/ layer of a multi-layer network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The activation**\n",
    "   \n",
    "$$a^l_j = \\sigma \\left ( \\sum_k w_{jk}^l~a^{l-1}_k \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   \n",
    "*** Explain what this means in words.\n",
    "    Especially the $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Explain what this means with a picture.\n",
    "Use translation (code:words:pictures) to develop and test your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rewrite this equation as a matrix equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\sigma(\\mathbf{W^l}\\vec{a^{l-1}})$ \n",
    "Notice that I am \"hiding\" the bias inside this equation. You have to always have that extra weight and the fixed input activation of $1$ for the bias. \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the dimension of this \"output\" and what is the interpretation of the $\\sigma$? What is *vectorizing*? From this expression why does it make sense to put the $j$ and $k$ backwards?\n",
    "Think of the dot product, column vectors, and the way rows and columns match up. All the odd writing is just to make this consistent with compact matrix notation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Write a function that given an input vector of activity and a weight matrix computes the new output activation.\n",
    "\n",
    "2. Write a function to output the derivative of the activity.\n",
    "\n",
    "Both in Python. They should be free standing functions that you can reuse in code later, i.e. they should start with\n",
    "\n",
    "`def yourfunction(args):`\n",
    "\n",
    "  `more stuff here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([0.9,0.8])\n",
    "ab = np.append(a,1.0)\n",
    "#what is going on in the line above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "w = np.array([[0.1,-0.25,0.3],[-.4,0.2,0.2],[1.2,2.3,0.03]])\n",
    "#how many neurons are in this layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19,  0.  ,  2.95])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtinput = w @ ab\n",
    "wtinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54735762,  0.5       ,  0.95026349])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newa = sig(np.dot(w,ab))\n",
    "newa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f5f8cbaa1d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newa_alt = map(sig, np.dot(w,ab))\n",
    "newa_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.54735761814308936, 0.5, 0.95026348844144337]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(newa_alt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *alt*ernative shows you there is more than one way to do things, but that sometimes that can hide complications. Your data has a type that may change depending on how you do things. These later examples use generators and iterators that may take awhile to wrap your head around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remembering \"costs\"\n",
    "\n",
    "   $$C = \\frac{1}{2n} \\sum_x \\|y(x) - a^L(x)\\|^2$$\n",
    "\n",
    "   The $L$ became capitalized because this is the \"last\" layer of a multilayer network. Why are we summing over $x$? What are the $x's$? What does the \"[norm](https://en.wikipedia.org/wiki/Norm_(mathematics))\" mean again? Size. Think Euclidean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation: A pseudo code account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Fix the inputs of the first layer to the input pattern $x$.\n",
    "2. Compute the weighted input to each neuron of the next layer using the input, weights and biases.\n",
    "3. Compute the weighted cost function error vector for the last layer.\n",
    "4. Backpropagate the error\n",
    "5. Use the backpropagated error to update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation: A \"stages\" account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Feedforward Stage\n",
    "Backward Pass Stage\n",
    "\n",
    "Weight Updating Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Just like we had to watch the parameters carefully in the H and H model, here the trick is to track the dimensions and orientation of your data objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding why it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It all comes down to the right way to solve for the weight changes.\n",
    "What is it we want to change? What are we free to change? \n",
    "\n",
    "Write an equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First step\n",
    "\n",
    "$$\\frac{\\partial{C_x}}{\\partial{W^{L}_{ji}}}$$\n",
    "\n",
    "Translate this into words. How would we use this if we knew what it was?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Second step\n",
    "\n",
    "Substitute and keep doing the chain rule - forever. \n",
    "    \n",
    "$$\\frac{\\partial{1/2(y - \\hat{y})^2}}{\\partial{W^{L}_{ij}}}$$ \n",
    "\n",
    "What part of this is a function of $W$? Only the $\\hat{y}$ (it is common to use a \"hat\" to reflect that one variable is an estimate of the variable under the hat) depends on the weights, and we use the chain rule.\n",
    "\n",
    "$$\\frac{\\partial{1/2(y - \\hat{y})^2}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{W^{L}_{ij}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Third step\n",
    "\n",
    "To be systematic, let's only consider the last, terminal, also called output layer and denote it $L$.\n",
    "\n",
    "$$\\frac{\\partial{1/2(y - \\hat{y})^2}}{\\partial{\\hat{y}}} \\frac{\\partial{\\sigma(z^L_i)}}{\\partial{z^L_i}}\\frac{\\partial{z^L_i}}{\\partial{W^{L}_{ij}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fourth step\n",
    "\n",
    "What is $z^L$?\n",
    "\n",
    "$$\\frac{\\partial{1/2(y - \\hat{y})^2}}{\\partial{\\hat{y}}} \\frac{\\partial{\\sigma(z^L_i)}}{\\partial{z^L_i}}\\frac{\\partial{\\sum_kW^{L}_{ik}~a^{L-1}_k}}{\\partial{W^{L}_{ij}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fifth step - simplify\n",
    "\n",
    "$$\\frac{\\partial{C_x}}{\\partial{W^{L}_{ji}}} = (\\hat{y} - y)\\sigma^\\prime(z^L_i)a^{L-1}_j$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More notation\n",
    "\n",
    "What part of the above formula is common to all synapses of the output neuron and which is unique for each synapse to the output neuron? Let's denote the common part with a common label $\\delta$.\n",
    "\n",
    "$$\\frac{\\partial{C_x}}{\\partial{W^{L}_{i}}} = \\delta^L_i a^{{L-1}^T}$$\n",
    "\n",
    "And do the same for the next layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making the pieces to solve the XOR problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The key equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$   \\delta^{\\mbox{Last Layer}} = \\nabla_a C \\circ \\sigma^{\\prime}(\\mbox{weighted inputs}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\delta^{\\mbox{allOtherLayers}} = (\\mbox{weights}^{nextLayer})^T \\delta^{\\mbox{nextLayer}} \\circ \\sigma^{\\prime}(\\mbox{weighted inputs})^{\\mbox{currentLayer}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Memory Hint:    $a_{in} \\delta^{out}$ \n",
    "$$\\frac{\\partial{C}}{\\partial{W_{jk}^l}} = a^{l-1}_k\\delta^l_j$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problem Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   1. We will use two inputs, at least three hidden units, and one output unit.\n",
    "   2. Create an input that has each possible input and the proper \"class\".\n",
    "   3. Initiate a random weight matrix.\n",
    "      How do you determine the size?\n",
    "\n",
    "      How do you represent this in your code? There are matrices and arrays - each has potential advantages.\n",
    "   4. Calculate cost\n",
    "   5. Calculate the derivative of the cost.\n",
    "   6. Calculate sigmoid\n",
    "   7. Calculate derivative of the sigmoid.\n",
    "   8. Write a function for the forward pass.\n",
    "   9. Write a function for the backward pass.\n",
    "   10. Write a function to update the weights.\n",
    "   11. Write a function to go through the above for a long time or until you get a good enough approximation.\n",
    "   12. Rewrite your code to be more general and accept different size inputs and outputs, and to allow for different size hidden layers (and maybe even more hidden layers).\n",
    "   13. Solve a harder problem\n",
    "      1. (http://www.cs.utoronto.ca/~kriz/cifar.html)\n",
    "\n",
    "      2.(http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/BabyAIShapesDatasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Some Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is a very good [online textbook](http://neuralnetworksanddeeplearning.com/chap1.html ) for this material.\n",
    "\n",
    "And some [nice short videos](https://youtu.be/bxe2T-V8XRs?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU) too."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "backprop.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
