<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=0.8"/><title>Computational Modeling for Psychology</title><link rel="stylesheet" type="text/css" href="scribble.css" title="default"/><link rel="stylesheet" type="text/css" href="autobib.css" title="default"/><link rel="stylesheet" type="text/css" href="figure.css" title="default"/><link rel="stylesheet" type="text/css" href="racket.css" title="default"/><link rel="stylesheet" type="text/css" href="scribble-style.css" title="default"/><script type="text/javascript" src="scribble-common.js"></script><script type="text/javascript" src="figure.js"></script><script type="text/javascript">
(function() {document.write('<scr' + 'ipt type="text/javascript" src="MathJax/MathJax.js?config=default"></scr' + 'ipt>');})();
</script><!--[if IE 6]><style type="text/css">.SIEHidden { overflow: hidden; }</style><![endif]--></head><body id="scribble-racket-lang-org"><div class="tocset"><div class="tocview"><div class="tocviewlist tocviewlisttopspace"><div class="tocviewtitle"><table cellspacing="0" cellpadding="0"><tr><td style="width: 1em;"><a href="javascript:void(0);" title="Expand/Collapse" class="tocviewtoggle" onclick="TocviewToggle(this,&quot;tocview_0&quot;);">&#9658;</a></td><td></td><td><a href="" class="tocviewselflink" data-pltdoc="x">Computational Modeling for Psychology</a></td></tr></table></div><div class="tocviewsublistonly" style="display: none;" id="tocview_0"><table cellspacing="0" cellpadding="0"><tr><td align="right">1&nbsp;</td><td><a href="#%28part._.Introductory._.Material%29" class="tocviewlink" data-pltdoc="x">Introduction and Computing Requirements</a></td></tr><tr><td align="right">2&nbsp;</td><td><a href="#%28part._.Computation._and._.Cognition%29" class="tocviewlink" data-pltdoc="x">What is Computation and is Cognition Computable?</a></td></tr><tr><td align="right">3&nbsp;</td><td><a href="#%28part._.D.Es._and._.Spikes%29" class="tocviewlink" data-pltdoc="x">Differential Equations and Spiking Neuron Models</a></td></tr><tr><td align="right">4&nbsp;</td><td><a href="#%28part._.Neural._.Networks%29" class="tocviewlink" data-pltdoc="x">Neural Networks</a></td></tr><tr><td align="right">5&nbsp;</td><td><a href="#%28part._.Projects%29" class="tocviewlink" data-pltdoc="x">Topics for Final Projects</a></td></tr></table></div></div></div><div class="tocsub"><table class="tocsublist" cellspacing="0"><tr><td><span class="tocsublinknumber"></span><a href="#%28part._.Computational_.Modeling_for_.Psychology%29" class="tocsubseclink" data-pltdoc="x">Computational Modeling for Psychology</a></td></tr><tr><td><span class="tocsublinknumber">1<tt>&nbsp;</tt></span><a href="#%28part._.Introductory._.Material%29" class="tocsubseclink" data-pltdoc="x">Introduction and Computing Requirements</a></td></tr><tr><td><span class="tocsublinknumber">1.1<tt>&nbsp;</tt></span><a href="#%28part._.Preface%29" class="tocsubseclink" data-pltdoc="x">Preface</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3apreamble%29" class="tocsubseclink" data-pltdoc="x">Preamble Bibliography</a></td></tr><tr><td><span class="tocsublinknumber">1.2<tt>&nbsp;</tt></span><a href="#%28part._.Preliminaries%29" class="tocsubseclink" data-pltdoc="x">Preliminaries</a></td></tr><tr><td><span class="tocsublinknumber">1.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Racket%29" class="tocsubseclink" data-pltdoc="x">Racket</a></td></tr><tr><td><span class="tocsublinknumber">1.2.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Getting_.Racket%29" class="tocsubseclink" data-pltdoc="x">Getting Racket</a></td></tr><tr><td><span class="tocsublinknumber">1.2.2<tt>&nbsp;</tt></span><a href="#%28part._.Git%29" class="tocsubseclink" data-pltdoc="x">Git</a></td></tr><tr><td><span class="tocsublinknumber">1.2.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Getting_.Git%29" class="tocsubseclink" data-pltdoc="x">Getting Git</a></td></tr><tr><td><span class="tocsublinknumber">1.2.3<tt>&nbsp;</tt></span><a href="#%28part._.Scribble%29" class="tocsubseclink" data-pltdoc="x">Scribble</a></td></tr><tr><td><span class="tocsublinknumber">2<tt>&nbsp;</tt></span><a href="#%28part._.Computation._and._.Cognition%29" class="tocsubseclink" data-pltdoc="x">What is Computation and is Cognition Computable?</a></td></tr><tr><td><span class="tocsublinknumber">2.1<tt>&nbsp;</tt></span><a href="#%28part._forgetting%29" class="tocsubseclink" data-pltdoc="x">What Is Cognition?</a></td></tr><tr><td><span class="tocsublinknumber">2.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Cognition_is____%29" class="tocsubseclink" data-pltdoc="x">Cognition is ...</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3acog%29" class="tocsubseclink" data-pltdoc="x">Cognition Section Bibliography</a></td></tr><tr><td><span class="tocsublinknumber">2.2<tt>&nbsp;</tt></span><a href="#%28part._.Is_.Cognition_.Computational_%29" class="tocsubseclink" data-pltdoc="x">Is Cognition Computational?</a></td></tr><tr><td><span class="tocsublinknumber">2.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Motivating_questions_%29" class="tocsubseclink" data-pltdoc="x">Motivating questions:</a></td></tr><tr><td><span class="tocsublinknumber">2.2.2<tt>&nbsp;</tt></span><a href="#%28part._.Introduction%29" class="tocsubseclink" data-pltdoc="x">Introduction</a></td></tr><tr><td><span class="tocsublinknumber">2.2.3<tt>&nbsp;</tt></span><a href="#%28part._.Computing_in_.Minds_and_.Computers%29" class="tocsubseclink" data-pltdoc="x">Computing in Minds and Computers</a></td></tr><tr><td><span class="tocsublinknumber">2.2.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Classical_.Computational_.Theory_of_.Mind%29" class="tocsubseclink" data-pltdoc="x">Classical Computational Theory of Mind</a></td></tr><tr><td><span class="tocsublinknumber">2.2.3.1.1<tt>&nbsp;</tt></span><a href="#%28part._.What_is_a_.Turing_machine__.Some_.Background_and_.Details%29" class="tocsubseclink" data-pltdoc="x">What is a Turing machine? Some Background and Details</a></td></tr><tr><td><span class="tocsublinknumber">2.2.4<tt>&nbsp;</tt></span><a href="#%28part._.Programming_a_.Turing_.Machine__the_.Busy_.Beaver%29" class="tocsubseclink" data-pltdoc="x">Programming a Turing Machine:<span class="mywbr"> &nbsp;</span> the Busy Beaver</a></td></tr><tr><td><span class="tocsublinknumber">2.2.4.1<tt>&nbsp;</tt></span><a href="#%28part._.Why_.Use_the_.Busy_.Beaver_.Problem_.As_an_.Example_%29" class="tocsubseclink" data-pltdoc="x">Why Use the Busy Beaver Problem As an Example?</a></td></tr><tr><td><span class="tocsublinknumber">2.2.4.2<tt>&nbsp;</tt></span><a href="#%28part._.A_.Busy_.Beaver_.Warm-.Up%29" class="tocsubseclink" data-pltdoc="x">A Busy Beaver Warm-<wbr></wbr>Up</a></td></tr><tr><td><span class="tocsublinknumber">2.2.5<tt>&nbsp;</tt></span><a href="#%28part._.Busy_.Beaver_.Problem_.Demo_.Code%29" class="tocsubseclink" data-pltdoc="x">Busy Beaver Problem Demo Code</a></td></tr><tr><td><span class="tocsublinknumber">2.2.5.1<tt>&nbsp;</tt></span><a href="#%28part._.Testing_the_.Busy_.Beaver_.Code%29" class="tocsubseclink" data-pltdoc="x">Testing the Busy Beaver Code</a></td></tr><tr><td><span class="tocsublinknumber">2.2.5.2<tt>&nbsp;</tt></span><a href="#%28part._.Busy_.Beaver_.Homework%29" class="tocsubseclink" data-pltdoc="x">Busy Beaver Homework</a></td></tr><tr><td><span class="tocsublinknumber">2.2.5.3<tt>&nbsp;</tt></span><a href="#%28part._.Resources%29" class="tocsubseclink" data-pltdoc="x">Resources</a></td></tr><tr><td><span class="tocsublinknumber">2.2.6<tt>&nbsp;</tt></span><a href="#%28part._.Optional_.Material%29" class="tocsubseclink" data-pltdoc="x">Optional Material</a></td></tr><tr><td><span class="tocsublinknumber">2.2.6.1<tt>&nbsp;</tt></span><a href="#%28part._.Functionalism%29" class="tocsubseclink" data-pltdoc="x">Functionalism</a></td></tr><tr><td><span class="tocsublinknumber">2.2.6.2<tt>&nbsp;</tt></span><a href="#%28part._.Is_the_.Computational_.Account_of_.Mind_.Trivial_%29" class="tocsubseclink" data-pltdoc="x">Is the Computational Account of Mind Trivial?</a></td></tr><tr><td><span class="tocsublinknumber">2.2.6.3<tt>&nbsp;</tt></span><a href="#%28part._.What_.Would_a_.Non-computational_.Theory_of_.Mind_.Be_%29" class="tocsubseclink" data-pltdoc="x">What Would a Non-<wbr></wbr>computational Theory of Mind Be?</a></td></tr><tr><td><span class="tocsublinknumber">2.2.6.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Logical_.Behaviorism%29" class="tocsubseclink" data-pltdoc="x">Logical Behaviorism</a></td></tr><tr><td><span class="tocsublinknumber">2.2.6.3.2<tt>&nbsp;</tt></span><a href="#%28part._.Type-.Identity_.Theory%29" class="tocsubseclink" data-pltdoc="x">Type-<wbr></wbr>Identity Theory</a></td></tr><tr><td><span class="tocsublinknumber">2.2.6.4<tt>&nbsp;</tt></span><a href="#%28part._.Alternatives_to_the_.Turing_.Machine_.Model_of_.Computation%29" class="tocsubseclink" data-pltdoc="x">Alternatives to the Turing Machine Model of Computation</a></td></tr><tr><td><span class="tocsublinknumber">2.2.6.4.1<tt>&nbsp;</tt></span><a href="#%28part._.Lambda_.Calculus%29" class="tocsubseclink" data-pltdoc="x">Lambda Calculus</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3acomp-cog%29" class="tocsubseclink" data-pltdoc="x">Computational Cognition References</a></td></tr><tr><td><span class="tocsublinknumber">3<tt>&nbsp;</tt></span><a href="#%28part._.D.Es._and._.Spikes%29" class="tocsubseclink" data-pltdoc="x">Differential Equations and Spiking Neuron Models</a></td></tr><tr><td><span class="tocsublinknumber">3.1<tt>&nbsp;</tt></span><a href="#%28part._.Differential_.Equations_and_.Spiking_.Neuron_.Models%29" class="tocsubseclink" data-pltdoc="x">Differential Equations and Spiking Neuron Models</a></td></tr><tr><td><span class="tocsublinknumber">3.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Spiking._.Neuron._.Models%29" class="tocsubseclink" data-pltdoc="x">Goals</a></td></tr><tr><td><span class="tocsublinknumber">3.1.2<tt>&nbsp;</tt></span><a href="#%28part._.The_.Action_.Potential_-_a_very_short_review%29" class="tocsubseclink" data-pltdoc="x">The Action Potential -<wbr></wbr> a very short review</a></td></tr><tr><td><span class="tocsublinknumber">3.1.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Multiple_.Ways_to_.Say_the_.Same_.Thing%29" class="tocsubseclink" data-pltdoc="x">Multiple Ways to Say the Same Thing</a></td></tr><tr><td><span class="tocsublinknumber">3.1.3<tt>&nbsp;</tt></span><a href="#%28part._.Derivatives_are_.Slopes%29" class="tocsubseclink" data-pltdoc="x">Derivatives are Slopes</a></td></tr><tr><td><span class="tocsublinknumber">3.1.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Use_your_computer_as_a_tool_for_exploration%29" class="tocsubseclink" data-pltdoc="x">Use your computer as a tool for exploration</a></td></tr><tr><td><span class="tocsublinknumber">3.1.4<tt>&nbsp;</tt></span><a href="#%28part._use-deriv-to-solve%29" class="tocsubseclink" data-pltdoc="x">Using Derivatives to Solve Problems With a Computer</a></td></tr><tr><td><span class="tocsublinknumber">3.1.4.1<tt>&nbsp;</tt></span><a href="#%28part._.What_is_the_square_root_of_128_%29" class="tocsubseclink" data-pltdoc="x">What is the square root of 128?</a></td></tr><tr><td><span class="tocsublinknumber">3.1.4.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Working_.Through_an_.Example%29" class="tocsubseclink" data-pltdoc="x">Working Through an Example</a></td></tr><tr><td><span class="tocsublinknumber">3.1.4.2<tt>&nbsp;</tt></span><a href="#%28part._.Practice_.Simulating_.With_.D.Es%29" class="tocsubseclink" data-pltdoc="x">Practice Simulating With DEs</a></td></tr><tr><td><span class="tocsublinknumber">3.1.4.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Frictionless_.Springs%29" class="tocsubseclink" data-pltdoc="x">Frictionless Springs</a></td></tr><tr><td><span class="tocsublinknumber">3.1.4.3<tt>&nbsp;</tt></span><a href="#%28part._.Damped_.Oscillators%29" class="tocsubseclink" data-pltdoc="x">Damped Oscillators</a></td></tr><tr><td><span class="tocsublinknumber">3.2<tt>&nbsp;</tt></span><a href="#%28part._.Integrate_and_.Fire_.Neuron%29" class="tocsubseclink" data-pltdoc="x">Integrate and Fire Neuron</a></td></tr><tr><td><span class="tocsublinknumber">3.2.1<tt>&nbsp;</tt></span><a href="#%28part._.History_of_the_.Integrate_and_.Fire_.Model%29" class="tocsubseclink" data-pltdoc="x">History of the Integrate and Fire Model</a></td></tr><tr><td><span class="tocsublinknumber">3.2.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Louis_.Lapicque_-_.Earlier_.Computational_.Neuroscientist%29" class="tocsubseclink" data-pltdoc="x">Louis Lapicque -<wbr></wbr> Earlier Computational Neuroscientist</a></td></tr><tr><td><span class="tocsublinknumber">3.2.1.2<tt>&nbsp;</tt></span><a href="#%28part._.Lord_.Adrian_and_the_.All-or-.None_.Action_.Potential%29" class="tocsubseclink" data-pltdoc="x">Lord Adrian and the All-<wbr></wbr>or-<wbr></wbr>None Action Potential</a></td></tr><tr><td><span class="tocsublinknumber">3.2.2<tt>&nbsp;</tt></span><a href="#%28part._sec~3aiandf%29" class="tocsubseclink" data-pltdoc="x">The Integrate and Fire Equation</a></td></tr><tr><td><span class="tocsublinknumber">3.2.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Electronics_.Background%29" class="tocsubseclink" data-pltdoc="x">Electronics Background</a></td></tr><tr><td><span class="tocsublinknumber">3.2.2.2<tt>&nbsp;</tt></span><a href="#%28part._.Formula_.Discussion_.Questions%29" class="tocsubseclink" data-pltdoc="x">Formula Discussion Questions</a></td></tr><tr><td><span class="tocsublinknumber">3.2.2.3<tt>&nbsp;</tt></span><a href="#%28part._.Coding_up_the_.Integrate_and_.Fire_.Neuron%29" class="tocsubseclink" data-pltdoc="x">Coding up the Integrate and Fire Neuron</a></td></tr><tr><td><span class="tocsublinknumber">3.2.2.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Class_.Exercise__.Adding_a_refractory_period_to_the_.Integrate_and_.Fire_model%29" class="tocsubseclink" data-pltdoc="x">Class Exercise:<span class="mywbr"> &nbsp;</span> Adding a refractory period to the Integrate and Fire model</a></td></tr><tr><td><span class="tocsublinknumber">3.2.3<tt>&nbsp;</tt></span><a href="#%28part._.Integrate_and_.Fire_.Homework%29" class="tocsubseclink" data-pltdoc="x">Integrate and Fire Homework</a></td></tr><tr><td><span class="tocsublinknumber">3.3<tt>&nbsp;</tt></span><a href="#%28part._.Hodgkin._and._.Huxley%29" class="tocsubseclink" data-pltdoc="x">Hodgkin and Huxley Model of the Action Potential</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Background_and_.Motivation%29" class="tocsubseclink" data-pltdoc="x">Background and Motivation</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Biographical_.Sources%29" class="tocsubseclink" data-pltdoc="x">Biographical Sources</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.2<tt>&nbsp;</tt></span><a href="#%28part._.Model_.Description__detailed_%29" class="tocsubseclink" data-pltdoc="x">Model Description (detailed)</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.3<tt>&nbsp;</tt></span><a href="#%28part._.Comments_and_.Steps_in_.Coding_the_.Hodgkin_.Huxley_.Model%29" class="tocsubseclink" data-pltdoc="x">Comments and Steps in Coding the Hodgkin Huxley Model</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Test_your_understanding%29" class="tocsubseclink" data-pltdoc="x">Test your understanding</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.4<tt>&nbsp;</tt></span><a href="#%28part._.It_s_.Differential_.Equations_.All_the_.Way_.Down%29" class="tocsubseclink" data-pltdoc="x">It&rsquo;s Differential Equations All the Way Down</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.5<tt>&nbsp;</tt></span><a href="#%28part._.Getting_.Started%29" class="tocsubseclink" data-pltdoc="x">Getting Started</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.5.1<tt>&nbsp;</tt></span><a href="#%28part._.Constants%29" class="tocsubseclink" data-pltdoc="x">Constants</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.6<tt>&nbsp;</tt></span><a href="#%28part._.Alpha_and_.Beta_.Formulas%29" class="tocsubseclink" data-pltdoc="x">Alpha and Beta Formulas</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.7<tt>&nbsp;</tt></span><a href="#%28part._.Updating_the_.Voltage%29" class="tocsubseclink" data-pltdoc="x">Updating the Voltage</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.7.1<tt>&nbsp;</tt></span><a href="#%28part._.Demonstrating_the_.Hodgkin-.Huxley_.Model%29" class="tocsubseclink" data-pltdoc="x">Demonstrating the Hodgkin-<wbr></wbr>Huxley Model</a></td></tr><tr><td><span class="tocsublinknumber">3.3.1.8<tt>&nbsp;</tt></span><a href="#%28part._.Hodgkin_and_.Huxley_.Homework%29" class="tocsubseclink" data-pltdoc="x">Hodgkin and Huxley Homework</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3ahandh%29" class="tocsubseclink" data-pltdoc="x">Hodgkin-<wbr></wbr>Huxley References</a></td></tr><tr><td><span class="tocsublinknumber">3.4<tt>&nbsp;</tt></span><a href="#%28part._.A_.Digression_.Into_.Dynamics%29" class="tocsubseclink" data-pltdoc="x">A Digression Into Dynamics</a></td></tr><tr><td><span class="tocsublinknumber">3.4.1<tt>&nbsp;</tt></span><a href="#%28part._.Introduction%29" class="tocsubseclink" data-pltdoc="x">Introduction</a></td></tr><tr><td><span class="tocsublinknumber">3.4.2<tt>&nbsp;</tt></span><a href="#%28part._.Beginning_to_.Think_.Dynamically%29" class="tocsubseclink" data-pltdoc="x">Beginning to Think Dynamically</a></td></tr><tr><td><span class="tocsublinknumber">3.4.3<tt>&nbsp;</tt></span><a href="#%28part._.Fixed_.Points%29" class="tocsubseclink" data-pltdoc="x">Fixed Points</a></td></tr><tr><td><span class="tocsublinknumber">3.4.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Class_.Exercise%29" class="tocsubseclink" data-pltdoc="x">Class Exercise</a></td></tr><tr><td><span class="tocsublinknumber">3.4.3.2<tt>&nbsp;</tt></span><a href="#%28part._.Extended_.Class_.Activity%29" class="tocsubseclink" data-pltdoc="x">Extended Class Activity</a></td></tr><tr><td><span class="tocsublinknumber">3.4.3.3<tt>&nbsp;</tt></span><a href="#%28part._.From_.Lines_to_.Planes%29" class="tocsubseclink" data-pltdoc="x">From Lines to Planes</a></td></tr><tr><td><span class="tocsublinknumber">3.4.3.4<tt>&nbsp;</tt></span><a href="#%28part._.Dealing_with_the_.Non-linear%29" class="tocsubseclink" data-pltdoc="x">Dealing with the Non-<wbr></wbr>linear</a></td></tr><tr><td><span class="tocsublinknumber">3.4.4<tt>&nbsp;</tt></span><a href="#%28part._.Neuronal_.Dyanmics_.Applied_to_.Spiking_.Neuron_.Models%29" class="tocsubseclink" data-pltdoc="x">Neuronal Dyanmics Applied to Spiking Neuron Models</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3adyn%29" class="tocsubseclink" data-pltdoc="x">Dynamics Bibliography</a></td></tr><tr><td><span class="tocsublinknumber">3.5<tt>&nbsp;</tt></span><a href="#%28part._.Morris_.Lecar_.Model%29" class="tocsubseclink" data-pltdoc="x">Morris Lecar Model</a></td></tr><tr><td><span class="tocsublinknumber">3.5.1<tt>&nbsp;</tt></span><a href="#%28part._.Work_in_.Progress%29" class="tocsubseclink" data-pltdoc="x">Work in Progress</a></td></tr><tr><td><span class="tocsublinknumber">4<tt>&nbsp;</tt></span><a href="#%28part._.Neural._.Networks%29" class="tocsubseclink" data-pltdoc="x">Neural Networks</a></td></tr><tr><td><span class="tocsublinknumber">4.1<tt>&nbsp;</tt></span><a href="#%28part._.Introduction_to_.Linear_.Algebra_and_.Neural_.Networks%29" class="tocsubseclink" data-pltdoc="x">Introduction to Linear Algebra and Neural Networks</a></td></tr><tr><td><span class="tocsublinknumber">4.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Linear_.Algebra_.Goals%29" class="tocsubseclink" data-pltdoc="x">Linear Algebra Goals</a></td></tr><tr><td><span class="tocsublinknumber">4.1.2<tt>&nbsp;</tt></span><a href="#%28part._.Drawing_.Cellular_.Automata%29" class="tocsubseclink" data-pltdoc="x">Drawing Cellular Automata</a></td></tr><tr><td><span class="tocsublinknumber">4.1.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Comments_on_the_programmatic_implementation%29" class="tocsubseclink" data-pltdoc="x">Comments on the programmatic implementation</a></td></tr><tr><td><span class="tocsublinknumber">4.1.3<tt>&nbsp;</tt></span><a href="#%28part._.More_.Lessons_from_.Cellular_.Automata%29" class="tocsubseclink" data-pltdoc="x">More Lessons from Cellular Automata</a></td></tr><tr><td><span class="tocsublinknumber">4.2<tt>&nbsp;</tt></span><a href="#%28part._.The_.Math_.That_.Underlies_.Neural_.Networks_%29" class="tocsubseclink" data-pltdoc="x">The Math That Underlies Neural Networks?</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Linear_.Algebra%29" class="tocsubseclink" data-pltdoc="x">Linear Algebra</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Important_.Objects_and_.Operations%29" class="tocsubseclink" data-pltdoc="x">Important Objects and Operations</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Adding_.Matrices%29" class="tocsubseclink" data-pltdoc="x">Adding Matrices</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.1.2<tt>&nbsp;</tt></span><a href="#%28part._.Activity%29" class="tocsubseclink" data-pltdoc="x">Activity</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.2<tt>&nbsp;</tt></span><a href="#%28part._.Common_.Notational_.Conventions_for_.Vectors_and_.Matrices%29" class="tocsubseclink" data-pltdoc="x">Common Notational Conventions for Vectors and Matrices</a></td></tr><tr><td><span class="tocsublinknumber">4.2.2<tt>&nbsp;</tt></span><a href="#%28part._.What_is_a_.Neural_.Network_%29" class="tocsubseclink" data-pltdoc="x">What is a Neural Network?</a></td></tr><tr><td><span class="tocsublinknumber">4.2.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Non-linearities%29" class="tocsubseclink" data-pltdoc="x">Non-<wbr></wbr>linearities</a></td></tr><tr><td><span class="tocsublinknumber">4.2.2.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Exercise_.X.O.R%29" class="tocsubseclink" data-pltdoc="x">Exercise XOR</a></td></tr><tr><td><span class="tocsublinknumber">4.2.2.2<tt>&nbsp;</tt></span><a href="#%28part._.Connections%29" class="tocsubseclink" data-pltdoc="x">Connections</a></td></tr><tr><td><span class="tocsublinknumber">4.2.2.3<tt>&nbsp;</tt></span><a href="#%28part._.Boolean_.Logic%29" class="tocsubseclink" data-pltdoc="x">Boolean Logic</a></td></tr><tr><td><span class="tocsublinknumber">4.2.2.4<tt>&nbsp;</tt></span><a href="#%28part.__.First_.Order_.Logic_-_.Truth_.Tables%29" class="tocsubseclink" data-pltdoc="x"> First Order Logic -<wbr></wbr> Truth Tables</a></td></tr><tr><td><span class="tocsublinknumber">4.3<tt>&nbsp;</tt></span><a href="#%28part._.Perceptrons%29" class="tocsubseclink" data-pltdoc="x">Perceptrons</a></td></tr><tr><td><span class="tocsublinknumber">4.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Goals%29" class="tocsubseclink" data-pltdoc="x">Goals</a></td></tr><tr><td><span class="tocsublinknumber">4.3.2<tt>&nbsp;</tt></span><a href="#%28part._.Perceptron_.History_and_.Implementation%29" class="tocsubseclink" data-pltdoc="x">Perceptron History and Implementation</a></td></tr><tr><td><span class="tocsublinknumber">4.3.3<tt>&nbsp;</tt></span><a href="#%28part._.The_.Perceptron_.Rules%29" class="tocsubseclink" data-pltdoc="x">The Perceptron Rules</a></td></tr><tr><td><span class="tocsublinknumber">4.3.4<tt>&nbsp;</tt></span><a href="#%28part._.You_.Are_.The_.Perceptron%29" class="tocsubseclink" data-pltdoc="x">You Are The Perceptron</a></td></tr><tr><td><span class="tocsublinknumber">4.3.4.1<tt>&nbsp;</tt></span><a href="#%28part._.A_simple_data_set%29" class="tocsubseclink" data-pltdoc="x">A simple data set</a></td></tr><tr><td><span class="tocsublinknumber">4.3.4.2<tt>&nbsp;</tt></span><a href="#%28part._.What_does_it_all_mean__.How_is_the_.Perceptron_.Learning_%29" class="tocsubseclink" data-pltdoc="x">What does it all mean? How is the Perceptron Learning?</a></td></tr><tr><td><span class="tocsublinknumber">4.3.4.3<tt>&nbsp;</tt></span><a href="#%28part._.Bias%29" class="tocsubseclink" data-pltdoc="x">Bias</a></td></tr><tr><td><span class="tocsublinknumber">4.3.4.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Geometrical_.Thinking%29" class="tocsubseclink" data-pltdoc="x">Geometrical Thinking</a></td></tr><tr><td><span class="tocsublinknumber">4.3.5<tt>&nbsp;</tt></span><a href="#%28part._.The_.Delta_.Rule_-_.Homework%29" class="tocsubseclink" data-pltdoc="x">The Delta Rule -<wbr></wbr> Homework</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3aperceptron%29" class="tocsubseclink" data-pltdoc="x">Perceptron Bibliography</a></td></tr><tr><td><span class="tocsublinknumber">4.4<tt>&nbsp;</tt></span><a href="#%28part._.Hopfield_.Networks%29" class="tocsubseclink" data-pltdoc="x">Hopfield Networks</a></td></tr><tr><td><span class="tocsublinknumber">4.4.1<tt>&nbsp;</tt></span><a href="#%28part._.Not_all_.Networks_are_the_.Same%29" class="tocsubseclink" data-pltdoc="x">Not all Networks are the Same</a></td></tr><tr><td><span class="tocsublinknumber">4.4.1.1<tt>&nbsp;</tt></span><a href="#%28part._.How_does_a_network_like_this_work_%29" class="tocsubseclink" data-pltdoc="x">How does a network like this work?</a></td></tr><tr><td><span class="tocsublinknumber">4.4.1.2<tt>&nbsp;</tt></span><a href="#%28part._.Test_your_understanding_%29" class="tocsubseclink" data-pltdoc="x">Test your understanding:</a></td></tr><tr><td><span class="tocsublinknumber">4.4.1.3<tt>&nbsp;</tt></span><a href="#%28part._.A_.Worked_.Example%29" class="tocsubseclink" data-pltdoc="x">A Worked Example</a></td></tr><tr><td><span class="tocsublinknumber">4.4.1.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Distance_.Metrics%29" class="tocsubseclink" data-pltdoc="x">Distance Metrics</a></td></tr><tr><td><span class="tocsublinknumber">4.4.1.4<tt>&nbsp;</tt></span><a href="#%28part._.Hebb_s_.Outer_.Product_.Rule%29" class="tocsubseclink" data-pltdoc="x">Hebb&rsquo;s Outer Product Rule</a></td></tr><tr><td><span class="tocsublinknumber">4.4.2<tt>&nbsp;</tt></span><a href="#%28part._.Hopfield_.Homework_.Description__.Robustness_to_.Noise%29" class="tocsubseclink" data-pltdoc="x">Hopfield Homework Description:<span class="mywbr"> &nbsp;</span> Robustness to Noise</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3ahopfield%29" class="tocsubseclink" data-pltdoc="x">Hopfield Bibliography</a></td></tr><tr><td><span class="tocsublinknumber">4.5<tt>&nbsp;</tt></span><a href="#%28part._.Backpropagation%29" class="tocsubseclink" data-pltdoc="x">Backpropagation</a></td></tr><tr><td><span class="tocsublinknumber">4.5.1<tt>&nbsp;</tt></span><a href="#%28part._.Warm_up_questions%29" class="tocsubseclink" data-pltdoc="x">Warm up questions</a></td></tr><tr><td><span class="tocsublinknumber">4.5.2<tt>&nbsp;</tt></span><a href="#%28part._.Sigmoid_.Functions%29" class="tocsubseclink" data-pltdoc="x">Sigmoid Functions</a></td></tr><tr><td><span class="tocsublinknumber">4.5.2.1<tt>&nbsp;</tt></span><a href="#%28part._.A_few_questions_about_sigmoid_functions%29" class="tocsubseclink" data-pltdoc="x">A few questions about sigmoid functions</a></td></tr><tr><td><span class="tocsublinknumber">4.5.2.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Mean_.Squared_.Error%29" class="tocsubseclink" data-pltdoc="x">Mean Squared Error</a></td></tr><tr><td><span class="tocsublinknumber">4.5.3<tt>&nbsp;</tt></span><a href="#%28part._.Backpropagation_1%29" class="tocsubseclink" data-pltdoc="x">Backpropagation 1</a></td></tr><tr><td><span class="tocsublinknumber">4.5.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Some_.Details%29" class="tocsubseclink" data-pltdoc="x">Some Details</a></td></tr><tr><td><span class="tocsublinknumber">4.5.3.2<tt>&nbsp;</tt></span><a href="#%28part._.Learning_.About_.Backpropagation%29" class="tocsubseclink" data-pltdoc="x">Learning About Backpropagation</a></td></tr><tr><td><span class="tocsublinknumber">4.5.3.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Bread_.Crumbs%29" class="tocsubseclink" data-pltdoc="x">Bread Crumbs</a></td></tr><tr><td><span class="tocsublinknumber">4.5.4<tt>&nbsp;</tt></span><a href="#%28part._.Homework%29" class="tocsubseclink" data-pltdoc="x">Homework</a></td></tr><tr><td><span class="tocsublinknumber">4.5.4.1<tt>&nbsp;</tt></span><a href="#%28part._.Additional_.Readings%29" class="tocsubseclink" data-pltdoc="x">Additional Readings</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3abackprop%29" class="tocsubseclink" data-pltdoc="x">Backpropagation Bibliography</a></td></tr><tr><td><span class="tocsublinknumber">5<tt>&nbsp;</tt></span><a href="#%28part._.Projects%29" class="tocsubseclink" data-pltdoc="x">Topics for Final Projects</a></td></tr><tr><td><span class="tocsublinknumber">5.1<tt>&nbsp;</tt></span><a href="#%28part._.Agent_.Based_.Modeling%29" class="tocsubseclink" data-pltdoc="x">Agent Based Modeling</a></td></tr><tr><td><span class="tocsublinknumber">5.2<tt>&nbsp;</tt></span><a href="#%28part._.Genetic_.Algorithms%29" class="tocsubseclink" data-pltdoc="x">Genetic Algorithms</a></td></tr><tr><td><span class="tocsublinknumber">5.3<tt>&nbsp;</tt></span><a href="#%28part._.Quantum_.Probability%29" class="tocsubseclink" data-pltdoc="x">Quantum Probability</a></td></tr><tr><td><span class="tocsublinknumber">5.4<tt>&nbsp;</tt></span><a href="#%28part._.Vector_.Symbolic_.Architectures%29" class="tocsubseclink" data-pltdoc="x">Vector Symbolic Architectures</a></td></tr><tr><td><span class="tocsublinknumber">5.5<tt>&nbsp;</tt></span><a href="#%28part._.Linear_.Ballistic_.Accumulators%29" class="tocsubseclink" data-pltdoc="x">Linear Ballistic Accumulators</a></td></tr><tr><td><span class="tocsublinknumber">5.6<tt>&nbsp;</tt></span><a href="#%28part._.Fitzhugh-.Nagamo_.Neuron_.Model%29" class="tocsubseclink" data-pltdoc="x">Fitzhugh-<wbr></wbr>Nagamo Neuron Model</a></td></tr><tr><td><span class="tocsublinknumber"></span><a href="#%28part._ref~3aprojects%29" class="tocsubseclink" data-pltdoc="x">Project References</a></td></tr></table></div></div><div class="maincolumn"><div class="main"><h2><a name="(part._.Computational_.Modeling_for_.Psychology)"></a>Computational Modeling for Psychology</h2><div class="SAuthorListBox"><span class="SAuthorList"><p class="author">Britt Anderson PhD &amp; MD &lt;<a href="mailto:britt@uwaterloo.ca">britt@uwaterloo.ca</a>&gt;</p></span></div><h3>1<tt>&nbsp;</tt><a name="(part._.Introductory._.Material)"></a>Introduction and Computing Requirements</h3><h4>1.1<tt>&nbsp;</tt><a name="(part._.Preface)"></a>Preface</h4><p>Experimental psychology was invented as a counterweight to the physical sciences. It is the difference between a science of mass and of weight, luminance and brightness. A pound of feathers has the same mass as a pound of pennies, but clearly the latter weighs more. Just try it.</p><p>To make a science of such subjective experience as to whether one thing is heavier or brighter than another there needed to be methods for human experimentation that were scientific. That is, they combined a subject matter of subjective experience with the standard procedures of empirical sciences: repeat measurements, control conditions, and systematic variation. By convention Wilhelm Wundt is taken as Empirical Psychology&rsquo;s founder and 1879, the year he established his independent experimental laboratory, as the date for the founding. It is only in the 1800s that we see the emergence of scientific experiments that look like modern psychology: Weber&rsquo;s weights, Helmholtz&rsquo;s mercury lamp flash experiments on attention, and Wundt&rsquo;s own experiments on attention.</p><p><iframe src="https://www.youtube.com/embed/Zr7O41r8uEI" width="560" height="315" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="true">Loading&#8230;</iframe></p><p>While Wundt was merging the experimental methods of physics and physiology with the content of human awareness, it was a generation before Wundt that Weber collected the data that led Gustav Fechner, a physicist, to express mathematically a procedure for measuring psychological magnitudes as functions of physical intensities: <span class="emph">psychophysics</span><span class="Autobibref">&nbsp;(<a href="#%28autobib._.David._.K..._.Robinson.Gustav._.Theodor._.Fechner~3a._.Psychophysics._and._.Natural._.Science.Oxford._.Research._.Encylopedia._of._.Psychology2020https~3a%2F%2Foxfordre..com%2Fpsychology%2Fpsychology%2Fview%2F10..1093%2Facrefore%2F9780190236557..001..0001%2Facrefore-9780190236557-e-487%29" class="AutobibLink" data-pltdoc="x">Robinson</a> <a href="#%28autobib._.David._.K..._.Robinson.Gustav._.Theodor._.Fechner~3a._.Psychophysics._and._.Natural._.Science.Oxford._.Research._.Encylopedia._of._.Psychology2020https~3a%2F%2Foxfordre..com%2Fpsychology%2Fpsychology%2Fview%2F10..1093%2Facrefore%2F9780190236557..001..0001%2Facrefore-9780190236557-e-487%29" class="AutobibLink" data-pltdoc="x">2020</a>)</span>.</p><p>Despite this early and potent demonstration of the power of using math for achieving insight into human subjective experience, quantitative models were not frequent in psychology for the next hundred years, and even now, despite notable and influential exceptions (the Rescorla-Wagner model, developed in the context of conditioning and the source of  modern reinforcement learning, Rosenblatt&rsquo;s perceptrons: the font from which neural networks flowed), mathematical models form only a small portion of published psychological research.  While the contemporary content of scientific psychology has greatly expanded, the predominant use of quantitative methods in psychology is still <span style="font-style: italic">statistical inference</span>. That reliance on statistics may be both cause and consequence for why mathematics, such as calculus and linear algebra, are not curricular requirements for many psychology undergraduate programs though statistics courses are. We are much quicker to deploy complicated statistical methodologies than to use math as the language for expressing concretely, concisely, and unambiguously our psychological theories. Nor do we use computer programs based on psychological theories to explore model implications via simulations as much as we should.</p><p><span style="font-weight: bold">This course is intended as a corrective. It endeavors to give undergraduates who may not have had any post-secondary math courses to speak of an exposure to some of the terminology and notation for the areas of mathematics most used in psychological and neuroscience models. The course combines this exposure with a heavy dose of programming exercises to practice concrete use. The goal is to build familiarity with terms and to desensitize some of the math and computing anxiety that formula and code excerpts can induce. In addition, and perhaps most importantly, the course wants to give students practice in seeing how formal mathematical ideas can be a potent source for focusing our discussion of what key psychological concepts <span style="font-style: italic">are</span></span>.</p><p>Of course, one cannot explore computational and mathematical ideas without having some familiarity with computing basics: writing code, markup syntax for reports and documentation, and ancillary tools such as <span class="emph">git</span> for sharing. In years past I combined all these content areas into this single course. The heterogeneity of student backgrounds made that tough, but as there were no alternatives it was necessary. Now, however, I have split off the computing tools part from this content part. Students can and should come to this course with some basic familiarity with using their computer as a research tool. If they do not have that knowledge they can gain it from a variety of on-line sources. I outline my approach <a href="https://brittanderson.github.io/Intro2Computing4Psychology/book/index.html">here</a>.</p><p><iframe src="https://player.vimeo.com/video/448900968?h=eff8e7355a&amp;amp;badge=0&amp;amp;autopause=0&amp;amp;player_id=0&amp;amp;app_id=58479" width="560" height="315" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="true">Loading&#8230;</iframe></p><p>Freeing this course from the constraints of teaching computing basics provides the space for including new content and teaching the older material differently. I would like both novice programmers and those with more experience programming to be able to get something from the exercises. I have explicitly decided not to use more common programming languages, such as python, so that everyone can focus on what it is we are trying to do, and not just what library can we import or what code we can find online to cut and paste? With the freedom to select any computing language I had the chance to hearken back to the early days of artificial intelligence (AI); an era when AI was about thinking and reasoning and not about how to import a model pretrained on billions of examples. By choosing a LISP I can also engage in a discussion of how programming languages differ, and how the design choices and features of a programming language may influence the expression of our theoretical ideas. Can a particular programming language lead us to new ways of thinking and conceiving of the problem space we wish to explore theoretically and via simulation?</p><p>All that is grand, but the course is still intended for undergraduates, many of whom may only possess programming rudiments. How to get them all, the Mac Users (both Intel and M1/2), as well as Windows and *Nix users, to have a common environment so that I can teach the same thing to all and so that they can get the tools installed on their computers in less than a month? Common-Lisp (CL) would be ideal, and I wrote some of the code for an <a href="https://github.com/brittAnderson/compNeuroIntro420/tree/lisp">earlier offering</a> in CL, but installing CL and getting a sane working environment can be challenging. Thus, I decided to try <a href="https://racket-lang.org/">Racket</a>. It is a language designed to support teaching, and has the DrRacket IDE. This works pretty much out of the box on Linux, Windows, and OSX systems. It even has a documentation system, <a href="https://docs.racket-lang.org/scribble/index.html">scribble</a>, built-in, and which I am using to write this document.</p><p>The remaining question is what new content to include? So far, I plan to expand the section on neuron modeling with an additional example, the Morris-Lecar model, that gives us a chance to explore how the differential equation formulation gives us additional information about our model via visualizing the phase space.</p><p>I also can now include something more traditional in the history of computational models of mind. We can code a simple Turing machine solving the busy beaver problem. We gain familiarity with this oft cited entity, and some concrete experience with the idea of computability and halting. How much more space is left for additional models? I hope to get to the Kohonen neural network for a week too. We will see from this fresh offering if there is time.</p><p>In summary, the goals for this course and this document are to give
students a familiarity with the mathematical terminology and domains
that form the backdrop to modeling in psychology. I still feel some
basic understanding of what certain mathematical gadgets are is
important, e.g. what a differential equation is is something
psychologists modeling memory should know about, but that most of them
do not. They do not, most of them, need to know how to analytically
solve the equations, but they should be able to use their own
programmed implementation to explore the implications of their ideas.
The basic constructs of linear algebra, matrices and vectors, are also
critical. It is essential for implementing many common neural
networks, but vector spaces also comprise a theoretical account of
representation. How much I can move beyond these fundamentals now that
I am not also trying to combine it with an introduction to programming
is a continuing experiment as we prepare to launch the Spring 2023 offering.</p><h5><a name="(part._ref~3apreamble)"></a>Preamble Bibliography</h5><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td><p><span class="Autobibtarget"><a name="(autobib._.David._.K..._.Robinson.Gustav._.Theodor._.Fechner~3a._.Psychophysics._and._.Natural._.Science.Oxford._.Research._.Encylopedia._of._.Psychology2020https~3a//oxfordre..com/psychology/psychology/view/10..1093/acrefore/9780190236557..001..0001/acrefore-9780190236557-e-487)"></a><span class="Autobibentry">David K. Robinson. <span style="font-style: italic">Gustav Theodor Fechner: Psychophysics and Natural Science</span>. Oxford Research Encylopedia of Psychology, 2020. <a href="https://oxfordre.com/psychology/psychology/view/10.1093/acrefore/9780190236557.001.0001/acrefore-9780190236557-e-487"><span class="url">https://oxfordre.com/psychology/psychology/view/10.1093/acrefore/9780190236557.001.0001/acrefore-9780190236557-e-487</span></a></span></span></p></td></tr></table></p><h4>1.2<tt>&nbsp;</tt><a name="(part._.Preliminaries)"></a>Preliminaries</h4><h5>1.2.1<tt>&nbsp;</tt><a name="(part._.Racket)"></a>Racket</h5><p>For this course we will be writing our code in <a href="https://racket-lang.org/">Racket</a>. Racket is in the category of <span class="emph">LISP</span>s, and is a descendant of <span class="emph">Scheme</span>. These languages are in the tradition of good old fashioned AI (GOFAI). Their heritage is in symbolic computation, and connects programming with formal models of computation such as the &#955; calculus. Knowing an example of this language family is good for developing programming knowledge and helping to see the big picture. Programming is more than a particular language or syntax, it is a medium for expressing ideas. Learning more than one way to express oneself programmatically helps to abstract the message out of the medium. Racket offers the practical advantage that it comes with good support for all popular operating systems and most hardware. Racket incorporates a <a href="https://docs.racket-lang.org/quick/index.html">picture aware</a> integrated development environment with syntax highlighting and has long been a staple of programming education with extensive tutorial material. At the same time Racket is a modern programming language in which one can write "production" code (though admittedly few do). It is touted as a <a href="https://cacm.acm.org/magazines/2018/3/225475-a-programmable-programming-language/fulltext">programming language for writing programming languages</a>, thus a student fully familiar with Racket can do more than most when it comes to coding. Though it is not the language of choice if one wants to get a job, it&rsquo;s selection here emphasizes our perspective on trying to understand the nature of the methods more than how to scale them or optimize run-time efficiencies.</p><h5>1.2.1.1<tt>&nbsp;</tt><a name="(part._.Getting_.Racket)"></a>Getting Racket</h5><ol><li><p>Go to <a href="https://www.racket-lang.org">racket-lang.org</a> and download and install the proper version for you operating system</p></li><li><p>Verify you can open Dr. Racket</p></li><li><p>Verify that Dr. Racket works by entering a simple instruction in the top window and seeing it executed in the bottom window</p></li></ol><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="drrack.svg" alt="" width="480.0" height="300.0"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3adrracket))" x-target-lift="Figure"></a>Figure&nbsp;1: </span>The Dr Racket IDE with a <span class="stt">#lang</span> line and some simple code</span></p></blockquote><p><span style="font-weight: bold">Submit</span> your screen shot of your Dr Racket IDE with your name to the appropriate Dropbox on Learn.</p><h5>1.2.2<tt>&nbsp;</tt><a name="(part._.Git)"></a>Git</h5><p>This book and the code it uses is in a git repository. Currently the git repository for this book is hosted on <a href="https:github.com/brittAnderson/compNeuroIntro420">github</a>. Make sure you have the <span class="stt">racket-book</span> branch selected. You are free to look at other branches, and you may find some interesting code or examples there from earlier offerings of the course, but the branch that we will be using for the Fall 2022 term is the <span class="stt">racket-book</span> branch.</p><p><a href="https://git-scm.com/">Git</a> is a  program for version control, and is very useful. <a href="https://github.com">Github</a> is one of a few different hosting hubs where many developers host their code to make it visible to others. You can <span class="stt">fork</span> and <span class="stt">clone</span> the code of others to try out their software or make your own changes to it.</p><p>I tried to explain all this once in a video. If you are already very confused it will not make things worse, and if you are only mildly confused it might help.</p><p><iframe src="https://player.vimeo.com/video/456349595?h=4b78ed9d6" width="560" height="315" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="true">Loading&#8230;</iframe></p><h5>1.2.2.1<tt>&nbsp;</tt><a name="(part._.Getting_.Git)"></a>Getting Git</h5><p>To make sure you have, and will be able to update, the code and material for this course you will need to clone this repository. If you wish to be able to make contributions to this course via a <span class="emph">pull request</span> you will first need to fork this repo. To that you should,</p><ol><li><p>Get a <a href="https://git-scm.com/downloads">program</a> for using git on your operating system installed,</p></li><li><p>Clone (and possibly fork) this repository</p></li><li><p>Demonstrate that you are tracking the correct branch.</p></li></ol><p>One way to do this is to run the <span class="stt">git branch </span><span class="stt">&ndash;</span><span class="stt">verbose</span> and <span class="stt">git remote </span><span class="stt">&ndash;</span><span class="stt">verbose</span> commands from the terminal, take a screenshot (on my linux system I use the terminal screen shot library <a href="https://wiki.archlinux.org/title/Screen_capture">scrot</a>).</p><p><span style="font-weight: bold">Submit</span> the screenshot of the output to the Dropbox on Learn.</p><h5>1.2.3<tt>&nbsp;</tt><a name="(part._.Scribble)"></a>Scribble</h5><p><a href="https://docs.racket-lang.org/scribble/index.html">Scribble</a> is the Racket documentation tool. The course material is being written in it. It is quite powerful right out of the box without too much tweaking, but to get more advanced features functioning well you may have to do some searching and import a number of additional racket packages. It is a version of a markup language (<span class="emph">markdown</span> is another common markup language and <span class="emph">jupyter</span> notebooks are yet another version of the same idea).</p><p>The advantage of these tools is that they allow you to blend both code and text in a single document. You can explain what you are doing as text with links and images like you would in a document. You can also include code like you would in a program. The code can be both written out like a quotation or actually run with the results of the code actually input into a document. This allows the possibility of <span class="emph">literate programming</span> or <span class="emph">reproducible research reports</span> or what we are doing here: treating the code as another type of data (like our words and references and images and videos) that we use to express our ideas as clearly as we can. I find the <span class="emph"><span style="font-weight: bold"><span class="stt">babel system of org-mode</span></span></span> to be the best of the bunch, but unfortunately it does not support Racket well.</p><p>Some of the course assignments will require you to submit a scribble document. You can start out writing a simple scribble document in the Dr Racket IDE and using the examples on the Racket language website. I have also included in this repository a <a href="./template-explained.scrbl">template scribble file</a> that includes a number of imports and has also been exported to html so that you can better understand what the scribble commands look like and what they are supposed to look like when you compile them (<a href="./template-explained.html">template file explained and compiled</a>).</p><h3>2<tt>&nbsp;</tt><a name="(part._.Computation._and._.Cognition)"></a>What is Computation and is Cognition Computable?</h3><h4>2.1<tt>&nbsp;</tt><a name="(part._forgetting)"></a>What Is Cognition?</h4><p>The technical challenges for modeling in psychology are mastering the mathematics and programming necessary to formally express an idea and to implement a simulation to examine the consequences. The conceptual challenge is to decide what it is you are modeling. <span class="refelem"><span class="refcolumn"><span class="refcontent">Is a mathematical formula that reproduces an observed behavioral pattern a model?</span></span></span> In the late 1800s Ebbinghaus, using creative, and for their time, <a href="https://youtu.be/TGGr5Uc8_Bw">innovative, Herculean methodologies</a>, demonstrated that the proportion of items retained in memory declines predictably over time. The form of the forgetting curve is exponential.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="Forgetting_curve_decline.svg" alt="" width="230.0444175" height="182.7995775"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3aforgetting-curve))" x-target-lift="Figure"></a>Figure&nbsp;2: </span><a href="https://en.wikipedia.org/wiki/Forgetting_curve">Forgetting Curve - Wikipedia</a></span></p></blockquote><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict.png" alt="image" width="406" height="406"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3aexponential-curve))" x-target-lift="Figure"></a>Figure&nbsp;3: </span>Plot of <span class="math">\exp ( \frac{1}{x} )</span></span></p></blockquote><h5>2.1.1<tt>&nbsp;</tt><a name="(part._.Cognition_is____)"></a>Cognition is ...</h5><p><div class="SIntrapara">As revealed in this recent article<span class="Autobibref">&nbsp;(<a href="#%28autobib._.Tim._.Bayne%2C._.David._.Brainard%2C._.Richard._.Byrne%2C._.Lars._.Chittka%2C._.Nicky._.Clayton%2C._.Cecilia._.Heyes%2C._.Jennifer._.Mather%2C._.Bence._~c3~96lveczky%2C._.Michael._.Shadlen%2C._.Thomas._.Suddendorf%2C._and._.Barbara._.Webb.What._.Is._.Cognition~3f.Current._.Biology._292019https~3a%2F%2Fdoi..org%2F10..1016%2Fj..cub..2019..05..044%29" class="AutobibLink" data-pltdoc="x">Bayne et al<span class="Sendabbrev">.</span></a> <a href="#%28autobib._.Tim._.Bayne%2C._.David._.Brainard%2C._.Richard._.Byrne%2C._.Lars._.Chittka%2C._.Nicky._.Clayton%2C._.Cecilia._.Heyes%2C._.Jennifer._.Mather%2C._.Bence._~c3~96lveczky%2C._.Michael._.Shadlen%2C._.Thomas._.Suddendorf%2C._and._.Barbara._.Webb.What._.Is._.Cognition~3f.Current._.Biology._292019https~3a%2F%2Fdoi..org%2F10..1016%2Fj..cub..2019..05..044%29" class="AutobibLink" data-pltdoc="x">2019</a>)</span> there are a wide array of views on what is cognition. </div><div class="SIntrapara"><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Which of these opinions, if any, do you agree with and why?</p></blockquote></blockquote></blockquote></div></p><h5><a name="(part._ref~3acog)"></a>Cognition Section Bibliography</h5><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Tim._.Bayne,._.David._.Brainard,._.Richard._.Byrne,._.Lars._.Chittka,._.Nicky._.Clayton,._.Cecilia._.Heyes,._.Jennifer._.Mather,._.Bence._~c3~96lveczky,._.Michael._.Shadlen,._.Thomas._.Suddendorf,._and._.Barbara._.Webb.What._.Is._.Cognition~3f.Current._.Biology._292019https~3a//doi..org/10..1016/j..cub..2019..05..044)"></a><span class="Autobibentry">Tim Bayne, David Brainard, Richard Byrne, Lars Chittka, Nicky Clayton, Cecilia Heyes, Jennifer Mather, Bence &#214;lveczky, Michael Shadlen, Thomas Suddendorf, and Barbara Webb. What Is Cognition? <span style="font-style: italic">Current Biology</span> 29, 2019. <a href="https://doi.org/10.1016/j.cub.2019.05.044"><span class="url">https://doi.org/10.1016/j.cub.2019.05.044</span></a></span></span></p></td></tr></table></p><h4>2.2<tt>&nbsp;</tt><a name="(part._.Is_.Cognition_.Computational_)"></a>Is Cognition Computational?</h4><h5>2.2.1<tt>&nbsp;</tt><a name="(part._.Motivating_questions_)"></a>Motivating questions:</h5><ol><li><p>Is there anything a human can think that a computer cannot compute?</p></li><li><p>What does it mean for a function to be "computable"?</p></li><li><p>What does it mean to say that cognition is computational?</p></li><li><p>What implications does the answer have for modeling cognition?</p></li></ol><h5>2.2.2<tt>&nbsp;</tt><a name="(part._.Introduction)"></a>Introduction</h5><p>Is there a difference between computational cognitive neuroscience and cognitive computational neuroscience<span class="Autobibref">&nbsp;(<a href="#%28autobib._.Thomas._.Naselaris%2C._.Danielle._.Bassett%2C._.Alyson._.Fletcher%2C._.Konrad._.Kording%2C._.Nikolaus._.Kriegeskorte%2C._.Hendrikje._.Nienborg%2C._.Russell._.A._.Poldrak%2C._.Daphna._.Shohamy%2C._and._.Kendrick._.Kay.Cognitive._.Computational._.Neuroscience~3a._.A._.New._.Conference._for._an._.Emerging._.Discipline.Trends._in._.Cognitive._.Sciences._22%2C._pp..._365--3672018https~3a%2F%2Fdx..doi..org%2F10..1016%2Fj..tics..2018..02..008%29" class="AutobibLink" data-pltdoc="x">Naselaris et al<span class="Sendabbrev">.</span></a> <a href="#%28autobib._.Thomas._.Naselaris%2C._.Danielle._.Bassett%2C._.Alyson._.Fletcher%2C._.Konrad._.Kording%2C._.Nikolaus._.Kriegeskorte%2C._.Hendrikje._.Nienborg%2C._.Russell._.A._.Poldrak%2C._.Daphna._.Shohamy%2C._and._.Kendrick._.Kay.Cognitive._.Computational._.Neuroscience~3a._.A._.New._.Conference._for._an._.Emerging._.Discipline.Trends._in._.Cognitive._.Sciences._22%2C._pp..._365--3672018https~3a%2F%2Fdx..doi..org%2F10..1016%2Fj..tics..2018..02..008%29" class="AutobibLink" data-pltdoc="x">2018</a>)</span>? The former seems to suggest that we are interested in explaining cognition directly from the actions of neurons and then using computational tools as adjuncts to aid this attempt. On the other hand, when you reverse the order it seems as if you are trying to explain thinking via the computational accounts of neuronal function. The latter seems to require a clear link between notions of computation and models of thinking. To use Marr&rsquo;s three levels as a scaffold<span class="Autobibref">&nbsp;(<a href="#%28autobib._.R..._.Mc.Clamrock.Marr%27s._three._levels~3a._.A._re-evaluation.Minds._and._.Machines._1%2C._pp..._185--1961991https~3a%2F%2Fdoi..org%2F10..1007%2F.B.F00361036%29" class="AutobibLink" data-pltdoc="x">McClamrock</a> <a href="#%28autobib._.R..._.Mc.Clamrock.Marr%27s._three._levels~3a._.A._re-evaluation.Minds._and._.Machines._1%2C._pp..._185--1961991https~3a%2F%2Fdoi..org%2F10..1007%2F.B.F00361036%29" class="AutobibLink" data-pltdoc="x">1991</a>)</span> <span class="refelem"><span class="refcolumn"><span class="refcontent">This reference is an older one, and not the original, but it does use LISP as its example, so it seems to relate nicely to the programming language we are using for our exercise.</span></span></span>, we are taking neurons as our implementation, positing algorithms for them, and assuming that computational principles populate the level of our cognitive abstractions. Measures of success depend on the meanings of terms.</p><p>In all of this is the basic idea that if you want to understand <span class="emph">mind</span> you need to develop a <span class="emph">theory</span> (and understand what that word means), express your theory clearly, for which the best language is <span class="emph">math</span>, and then explore the implications of your construction via simulation, which requires the writing of <span class="emph">code</span>. Mind &#8594; Theory &#8594; Math  &#8594; Code. Let&rsquo;s explore a little more what is implied by this pathway. What it means to use computing as a model for mind.</p><h5>2.2.3<tt>&nbsp;</tt><a name="(part._.Computing_in_.Minds_and_.Computers)"></a>Computing in Minds and Computers</h5><p>One definition of whether something is computable is whether there exists a Turing Machine that computes it. That Turing machine must halt. Since one cannot decide in advance for all machines whether they will halt or not it turns out that whether or not something is computable is, in general, undecidable.</p><p>Although most of us learned the name "Turing" in the context of whether a computer has human intelligence, the so-called <span class="emph">Turing test</span>, the model of Turing computability emerged from thinking about humans computing <span class="Autobibref">&nbsp;(<a href="#%28autobib._.A..._.M..._.Turing.On._computable._numbers._with._an._application._to._the._.Entscheidungsproblem.Proc..._.London._.Math..._.Soc..._42%2C._pp..._230--2651936%29" class="AutobibLink" data-pltdoc="x">Turing</a> <a href="#%28autobib._.A..._.M..._.Turing.On._computable._numbers._with._an._application._to._the._.Entscheidungsproblem.Proc..._.London._.Math..._.Soc..._42%2C._pp..._230--2651936%29" class="AutobibLink" data-pltdoc="x">1936</a>)</span>. <span class="refelem"><span class="refcolumn"><span class="refcontent">A nice short version of this history can be found in this <a href="https://link.springer.com/content/pdf/10.1007/978-3-642-31933-4.pdf">pdf</a>.</span></span></span></p><h5>2.2.3.1<tt>&nbsp;</tt><a name="(part._.Classical_.Computational_.Theory_of_.Mind)"></a>Classical Computational Theory of Mind</h5><p>The classical computational theory of mind says that in all important ways the mind is like a Turing machine. If we want to  model (or emulate) functions of mind one way would be to build a Turing machine. However this can be a practical challenge, and one can question the insight gained from this approach. Still given the theoretical and practical prominence given to Turing computability it behooves us to know what a Turing machine truly is.</p><h5>2.2.3.1.1<tt>&nbsp;</tt><a name="(part._.What_is_a_.Turing_machine__.Some_.Background_and_.Details)"></a>What is a Turing machine? Some Background and Details</h5><p><div class="SIntrapara"><a href="https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-12">Turing machines</a> are quite simple implements. While one can build a physical Turing machine, the more usual sense of the term is for a hypothetical computer that is comprised of
</div><div class="SIntrapara"><ul><li><p>a finite alphabet,</p></li><li><p>a finite set of states,</p></li><li><p>the capacity to read and write to a single location in memory, and the ability to adjust the memory location immediately left or right or to make no move at all, and</p></li><li><p>a set of instructions (or "machine table") that translates the combination of the current state and the current symbol to a new state and one of the acceptable actions.</p></li></ul></div></p><p>Other models of computation are the <span class="emph">lambda calculus</span>  and the <span class="emph">theory of recursive functions</span>. Those alternative accounts of computability are interesting, and may offer more insight or be more practical in some situations, but it appears to be the case that they are equivalent. Anything designated "computable" by one of these formal accounts is computable by the others as well. Does this mean that if you accept the computational mind hypothesis you must accept that the mind is able to be simulated by a Turing Machine? As a consequence does this mean that minds are <span class="emph"><a href="https://plato.stanford.edu/entries/multiple-realizability">multiple realizable</a></span>?</p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><ul><li><p>Are Turing machines <span class="emph">digital</span>?</p></li><li><p>Is this an important distinction?</p></li><li><p>Does this mean that analog computations are omitted?</p></li><li><p>Would an analog paradigm be a better match to modeling mental activity?</p></li></ul></blockquote></blockquote></blockquote><h5>2.2.4<tt>&nbsp;</tt><a name="(part._.Programming_a_.Turing_.Machine__the_.Busy_.Beaver)"></a>Programming a Turing Machine: the Busy Beaver</h5><p>The following is a slightly formatted version of the Wikipedia description of a Turing machine.</p><ol><li><p>A Turing machine has n "operational" states plus a Halt state, where n is a positive integer, and one of the n states is distinguished as the starting state.</p></li><li><p>The machine uses a single two-way infinite (or unbounded) tape.</p></li><li><p>The tape alphabet is {0, 1}, with 0 serving as the blank symbol.</p></li><li><p><div class="SIntrapara">The machine&rsquo;s transition function takes two inputs:
    </div><div class="SIntrapara"><ul><li><p>the current non-Halt state,</p></li><li><p>the symbol in the current tape cell,</p></li></ul></div></p></li><li><p><div class="SIntrapara">and produces three outputs:
    </div><div class="SIntrapara"><ul><li><p>a symbol to write over the symbol in the current tape cell (it may be the same symbol as the symbol overwritten),</p></li><li><p>a direction to move (left or right; that is, shift to the tape cell one place to the left or right of the current cell), and</p></li><li><p>a state to transition into (which may be the Halt state).</p></li></ul></div></p></li></ol><p>We will use it to guide us to write a simple instance that computes a solution to the Busy Beaver problem. A n-state Turing machine has <span class="math">(4n + 4)2n</span> states. The formula is (symbols &#215; directions &#215; (states + 1))(symbols &#215; states). The transition function (how to figure out where to go next may be seen as a finite look-up table. Each row of the table is a 5-tuple: (current state, current symbol, symbol to write, direction of shift, next state). Our goal with the Busy Beaver problem is to run our machine to produce as long a series of uninterrupted ones as we can <span class="emph">and</span> <span style="font-weight: bold">halt</span>.</p><p>What it means to "run" a Turing machine is to start in the starting state with the current tape cell being any cell of a blank (all-0) "tape", and then iterate the transition function. If the Halt state is entered then the number of 1s remaining on the tape is called the machine&rsquo;s score. Different transition rules will give us different outputs, so we can score our machine based on its performance.</p><p>To restate in a more general way, the n-state busy beaver (BB-n) game is a contest to find an n-state Turing machine having the largest possible score &#8212; the largest number of 1s on its tape after halting. A machine that attains the largest possible score among all n-state Turing machines is called an n-state busy beaver, and a machine whose score is merely the highest so far attained (perhaps not the largest possible) is called a champion n-state machine (This ends the lightly edited Wikipedia quote).</p><h5>2.2.4.1<tt>&nbsp;</tt><a name="(part._.Why_.Use_the_.Busy_.Beaver_.Problem_.As_an_.Example_)"></a>Why Use the Busy Beaver Problem As an Example?</h5><p>The Busy Beaver Problem is non-computable</p><p>The busy beaver problem is to compute the maximum number of 1&rsquo;s that a Turing machine can write before halting with the number of states equal to n. This <a href="https://jeremykun.com/2012/02/08/busy-beavers-and-the-quest-for-big-numbers/">webpage</a> includes the proof of the non-computability of the busy beaver problem. It uses contradiction, and like most proofs relying on contradiction I find it head warping, but there it is.</p><h5>2.2.4.2<tt>&nbsp;</tt><a name="(part._.A_.Busy_.Beaver_.Warm-.Up)"></a>A Busy Beaver Warm-Up</h5><p>A simple version of the Busy Beaver problem, and one you can do by hand with pencil and paper, is the n=2 version. Create a Turing Machine with the following transition rules:</p><ul><li><p>a0 &#8594; b1r</p></li><li><p>a1  &#8594; b1l</p></li><li><p>b0  &#8594; a1l</p></li><li><p>b1  &#8594; h1r</p></li></ul><h5>2.2.5<tt>&nbsp;</tt><a name="(part._.Busy_.Beaver_.Problem_.Demo_.Code)"></a>Busy Beaver Problem Demo Code</h5><p><div class="SIntrapara">Making a Structure for Our Turing Machine</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">struct</span><span class="hspace">&nbsp;</span><span class="RktSym">turing-machine</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">state</span><span class="hspace">&nbsp;</span><span class="RktSym">tape</span><span class="hspace">&nbsp;</span><span class="RktSym">head-location</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">#:transparent</span><span class="hspace">&nbsp;</span><span class="RktPn">#:mutable</span><span class="RktPn">)</span></td></tr></table></blockquote></div></p><p><div class="SIntrapara"><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>There is some inelegance here to handle the fact that we are not, in fact, starting with an infinitely long tape and want to build it on the fly.</p></blockquote></blockquote></blockquote></div><div class="SIntrapara"><p><div class="SIntrapara">Moving our TM Along the Tape</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move-left</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">let</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">loc</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-head-location</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">lst</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-tape</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">if</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">=</span><span class="hspace">&nbsp;</span><span class="RktSym">loc</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-tape!</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">cons</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="hspace">&nbsp;</span><span class="RktSym">lst</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-head-location!</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">loc</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move-right</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">let</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">loc</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-head-location</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">lst</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-tape</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">when</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">=</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktSym">loc</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">length</span><span class="hspace">&nbsp;</span><span class="RktSym">lst</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-tape!</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">append</span><span class="hspace">&nbsp;</span><span class="RktSym">lst</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-head-location!</span><span class="hspace">&nbsp;</span><span class="RktSym">temp-tm</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktSym">loc</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktSym">dir</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">cond</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktPn">(</span><span class="RktSym">equal?</span><span class="hspace">&nbsp;</span><span class="RktSym">dir</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">left</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move-left</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktPn">(</span><span class="RktSym">equal?</span><span class="hspace">&nbsp;</span><span class="RktSym">dir</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">right</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move-right</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">else</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">error</span><span class="hspace">&nbsp;</span><span class="RktVal">"illegal direction"</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p></div></p><p><div class="SIntrapara">Helper Functions</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">tm-equal-state-value</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktSym">state</span><span class="hspace">&nbsp;</span><span class="RktSym">value</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">and</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">equal?</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-state</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">state</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">=</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list-ref</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-tape</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-head-location</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">value</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">upd-tape-location</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktSym">value</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-tape!</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list-set</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-tape</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-head-location</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">value</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">pretty-print-tm</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">display</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">format</span><span class="hspace">&nbsp;</span><span class="RktVal">"state:~a, tape: ~a, head: ~a\n"</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-state</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-tape</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-head-location</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><p><div class="SIntrapara">Rules are the Critical Part of This Machine</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">rule</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">cond</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktPn">(</span><span class="RktSym">tm-equal-state-value</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">a</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-state!</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">b</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">upd-tape-location</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">right</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktPn">(</span><span class="RktSym">tm-equal-state-value</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">a</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-state!</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">b</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">upd-tape-location</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">left</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktPn">(</span><span class="RktSym">tm-equal-state-value</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">b</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-state!</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">a</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">upd-tape-location</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">left</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktPn">(</span><span class="RktSym">tm-equal-state-value</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">b</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">set-turing-machine-state!</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">h</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">upd-tape-location</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">move</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">right</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><p><div class="SIntrapara">A Run Through the N=2 Busy Beaver Problem</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">busy-beaver-2-do</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">pretty-print-tm</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">do</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktPn">(</span><span class="RktSym">equal?</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine-state</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">h</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">display</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">format</span><span class="hspace">&nbsp;</span><span class="RktVal">"Loops equaled ~a\n"</span><span class="hspace">&nbsp;</span><span class="RktSym">i</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">rule</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">pretty-print-tm</span><span class="hspace">&nbsp;</span><span class="RktSym">tm</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktSym">tm</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><p><div class="SIntrapara">test</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">require</span><span class="hspace">&nbsp;</span><span class="RktVal">"./code/tm.rkt"</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">initial-turing-machine</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">turing-machine</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">a</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">working-tm</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">struct-copy</span><span class="hspace">&nbsp;</span><span class="RktSym">turing-machine</span><span class="hspace">&nbsp;</span><span class="RktSym">initial-turing-machine</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">busy-beaver-2-do</span><span class="hspace">&nbsp;</span><span class="RktSym">initial-turing-machine</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0"><tr><td><p><span class="RktOut">state:a, tape: (0), head: 0</span></p></td></tr><tr><td><p><span class="RktOut">state:b, tape: (1 0), head: 1</span></p></td></tr><tr><td><p><span class="RktOut">state:a, tape: (1 1), head: 0</span></p></td></tr><tr><td><p><span class="RktOut">state:b, tape: (0 1 1), head: 0</span></p></td></tr><tr><td><p><span class="RktOut">state:a, tape: (0 1 1 1), head: 0</span></p></td></tr><tr><td><p><span class="RktOut">state:b, tape: (1 1 1 1), head: 1</span></p></td></tr><tr><td><p><span class="RktOut">state:h, tape: (1 1 1 1), head: 2</span></p></td></tr><tr><td><p><span class="RktOut">Loops equaled 6</span></p></td></tr></table></td></tr><tr><td><p><span class="RktRes">(turing-machine 'h '(1 1 1 1) 2)</span></p></td></tr></table></blockquote></div></p><h5>2.2.5.1<tt>&nbsp;</tt><a name="(part._.Testing_the_.Busy_.Beaver_.Code)"></a>Testing the Busy Beaver Code</h5><p>The code typed above can be found at <a href="../code/tm.rkt">../code/tm.rkt</a>. Open DrRacket. Open this file (you can use the navigate to it with DrRacket&rsquo;s built-in file manager if the link does not work for you). Click <span style="font-weight: bold"><span class="stt">run</span></span>. You should seen no error messages (or much of anything else). Now add the code  <span class="RktVal">"(busy-beaver-2-do initial-turing-machine)"</span> to the bottom of the file, and re-run the file. You should see the output typed above in the lower, execution, window.</p><p>In your editor make a copy of this file with a new name. Test that you can edit the code by changing the line of what to do with <span class="stt"></span><span class="stt">&rsquo;</span><span class="stt">a 0</span> to transition to <span class="stt"></span><span class="stt">&rsquo;</span><span class="stt">h</span> instead of <span class="stt"></span><span class="stt">&rsquo;</span><span class="stt">b</span>. Save and re-run. You should halt after the first state. If that is working, you have the basics functioning and now can tackle the homework.</p><h5>2.2.5.2<tt>&nbsp;</tt><a name="(part._.Busy_.Beaver_.Homework)"></a>Busy Beaver Homework</h5><p>Come up with a version of rules for n=4. All you will need to edit is the rules section. Everything else should just work. After you are satisfied create a new function for <span class="stt">busy-beaver-4-do</span> that uses your new rule. Add the line invoking it to your file. That will allow me to run your racket program from the command line and see the results of your rule in my terminal. You can test this yourself by running <span class="RktVal">"racket &lt;your-name&gt;-busy-beaver-4-do.rkt"</span> when you are in the home directory of your program.</p><p>I will run your programs against each other in class (hopefully). Don&rsquo;t try and break any records, but do spend sometime trying to get the numbers up. We are learning about Turing machines and how to write code the implements mathematical and theoretical ideas for the elucidation of cognition. Spending too much time perfecting your Busy Beaver implementation misses the point, but just getting something that works without crashing doesn&rsquo;t give you enough chance to think about this model of computation. This will also give you a chance to start to learn the racket language a little bit.</p><h5>2.2.5.3<tt>&nbsp;</tt><a name="(part._.Resources)"></a>Resources</h5><p>This tutorial article with examples has a nice visualization. If you are
having a little trouble getting started <a href="https://catonmat.net/busy-beaver">busy-beaver</a> might help.</p><p>Why the <a href="https://en.wikipedia.org/wiki/Busy_beaver">Busy Beaver</a>?
Because the solution to this problem is noncomputable. <span class="refelem"><span class="refcolumn"><span class="refcontent">What does it mean that we are
solving this with our computers and our own reasoning, but that the
problem itself is not computable? Does that present any hurdle at
all for using the Turing Machine as a model of mind?</span></span></span></p><h5>2.2.6<tt>&nbsp;</tt><a name="(part._.Optional_.Material)"></a>Optional Material</h5><p>Someday some of this material may make it into the regular portion of the course. For now, it is simply left here as a guide to additional topics that one may want to think about when engaged in cognitive computational (neuroscience) modeling.</p><h5>2.2.6.1<tt>&nbsp;</tt><a name="(part._.Functionalism)"></a>Functionalism</h5><p>One of the schools of thought in the domain of Philosophy of Mind, <a href="https://plato.stanford.edu/entries/functionalism/">functionalism</a> comes up a lot in computational modeling. It suggests that if you know what something does, what its function is, you know the important stuff. You don&rsquo;t need to think about mental states in terms of what they are as "things". You might think about them in terms of what they "do".  Mental states serve a functional role in a cognitive system. The important thing is how they relate to the sensory input, motor output and to <span class="emph">each other.</span></p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Food for Thought Question: If you accept that the mind is a computational machine, and that for all computable problems an equivalent Turing machine exits, does that require you accept that any functionally equivalent computational system, regardless of its hardware (i.e. it could be vacuum tubes or the population of China) would be a mind?</p></blockquote></blockquote></blockquote><p>The variety of  functionalism closest to our Turing machine is probably <span class="emph">machine state functionalism</span>.</p><p>After you familiarize yourself with functionalism, you might return to the question above and revisit your answer.</p><h5>2.2.6.2<tt>&nbsp;</tt><a name="(part._.Is_the_.Computational_.Account_of_.Mind_.Trivial_)"></a>Is the Computational Account of Mind Trivial?</h5><ol><li><p>Any sufficiently complex physical system (such as the molecules comprising the wall behind me or the brain) can be shown to be <span class="emph">isomorphic</span> to the formal structure of <span class="emph">any</span> program. If you view the mind as a program than you might as well say that you and the wall behind you share the same thoughts.</p></li><li><p>There is no room for the time scale to matter and there is an intuition that it should. We could implement a Turing machine with water wheels, levers &amp; pulleys, vacuum tubes, or transistors. The speed with which the resulting machine computes will be very different, but they will all perform the same computation. Do we think that a model of mind that is blind to time scale can possibly be right?</p></li><li><p>Discrete or continuous. Turing machines are <span style="font-weight: bold">discrete, finite</span> state machines. Time and thought operate in continuous time (don&rsquo;t they?). Are discrete models that move forward in time in discontinuous steps capable of modeling us who live in and think in the world of continuous time?</p></li><li><p>Computations might model something without explaining it. Weather simulators predict rain, but they don&rsquo;t themselves actually rain. Flight simulators do not fly. Even if a computer program simulated a mind it does not mean it would be thinking. Does the simulations, explanation or demonstration distinction bother you?</p></li></ol><h5>2.2.6.3<tt>&nbsp;</tt><a name="(part._.What_.Would_a_.Non-computational_.Theory_of_.Mind_.Be_)"></a>What Would a Non-computational Theory of Mind Be?</h5><h5>2.2.6.3.1<tt>&nbsp;</tt><a name="(part._.Logical_.Behaviorism)"></a>Logical Behaviorism</h5><p>Mental states are predispositions to behave. There is no internal state corresponding to belief that is mental. Belief is only a predisposition to behave in a certain way in a certain context.  We characterize people by what they are likely to do without ascribing to them associated mental states. The person who first developed this idea, Gilbert Ryle, asserts that being a mentalist is incompatible with being a realist (that is it makes you a dualist). Logical behaviorism  does not seem to be much in vogue now, but it is another take on the important issues <span class="Autobibref">&nbsp;(<a href="#%28autobib._.Michael._.Sch~c3~bctte.Logical._.Behaviorism.Encyclopedia._of._.Neuroscience%2C._pp..._372--3752008http~3a%2F%2Fdx..doi..org%2F10..1007%2F978-3-540-29678-2_596%29" class="AutobibLink" data-pltdoc="x">Sch&#252;tte</a> <a href="#%28autobib._.Michael._.Sch~c3~bctte.Logical._.Behaviorism.Encyclopedia._of._.Neuroscience%2C._pp..._372--3752008http~3a%2F%2Fdx..doi..org%2F10..1007%2F978-3-540-29678-2_596%29" class="AutobibLink" data-pltdoc="x">2008</a>)</span>.</p><h5>2.2.6.3.2<tt>&nbsp;</tt><a name="(part._.Type-.Identity_.Theory)"></a>Type-Identity Theory</h5><p>Mental states just <span class="emph">are</span> brain states. <span class="refelem"><span class="refcolumn"><span class="refcontent">Since our brains are different from time to time (synaptic weights change; cells die (and a few born)) does that mean we never have the same mental state twice? Since no two people have the same brain does that mean no two people ever have the same mental states?</span></span></span></p><h5>2.2.6.4<tt>&nbsp;</tt><a name="(part._.Alternatives_to_the_.Turing_.Machine_.Model_of_.Computation)"></a>Alternatives to the Turing Machine Model of Computation</h5><p>Although the Turing Machine account of computation seems to dominate examples in psychology and neuroscience this may be an artifact of the example of the Turing Test as a prominent example of evaluating machine intelligence. There are other formal theories of computation, and as stated above they are all conjectured to be equivalent. The one I describe below has had a large influence on programming language design. It was the mathematical idea that motivated the development of LISP, which via its descendent Scheme, became the Racket language we are using in this course.</p><h5>2.2.6.4.1<tt>&nbsp;</tt><a name="(part._.Lambda_.Calculus)"></a>Lambda Calculus</h5><p>The lambda calculus was developed as a theory of functions. <a href="https://news.stanford.edu/news/2011/october/john-mccarthy-obit-102511.html">John McCarthy</a> invented Lisp as a theoretical exercise for working on the theory of computable functions. He felt the Turing machine to be too mechanical and too awkward for this work, and wanted a better tool, a better metaphor. He adopted the lambda of the lambda calculus and the ~eval~ function to take in lisp programs and execute them. Later one of his <a href="https://en.wikipedia.org/wiki/Steve_Russell_(computer_scientist)">collaborators</a> observed that it was relatively straightforward to implement this as a real programming language. A bit more of the history is <a href="https://lwn.net/Articles/778550/">here</a>. But in order to try and learn a bit of the lambda calculus you might look at <span class="Autobibref">&nbsp;(<a href="#%28autobib._.Greg._.Michaelson.An._introduction._to._functional._programming._through._.Lambda._calculus.Dover._.Publications2011https~3a%2F%2Fwww..cs..rochester..edu%2F~7ebrown%2F173%2Freadings%2F.L.C.Book..pdf%29" class="AutobibLink" data-pltdoc="x">Michaelson</a> <a href="#%28autobib._.Greg._.Michaelson.An._introduction._to._functional._programming._through._.Lambda._calculus.Dover._.Publications2011https~3a%2F%2Fwww..cs..rochester..edu%2F~7ebrown%2F173%2Freadings%2F.L.C.Book..pdf%29" class="AutobibLink" data-pltdoc="x">2011</a>)</span></p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Why a "lambda" (&#955;)? The &#955; of lambda calculus doesn&rsquo;t really mean anything. It just signals that you have a lambda expression. Its creation as the symbol for the function calculus was an accident of notation and the limits of older typewriters.</p></blockquote></blockquote></blockquote><p><span style="font-weight: bold">Some Details and Interesting Facts about the Lambda Calculus</span></p><p>There is not just one lambda calculus.</p><p>To have the lambda calculus you need to specify your <span class="emph">algebra</span>. What are the rules for reducing things (i.e. your computations), what are the allowed symbols, and what do things mean? A "lambda calculus" is a way of handling "lambda expressions."</p><p><span style="font-weight: bold">Into the Weeds</span></p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Want to implement the lambda calculus yourself on a computer (using Common Lisp)? Check out this <a href="https://flownet.com/ron/lambda-calculus.html">blog post</a>.</p></blockquote></blockquote></blockquote><p><span class="emph">Terms</span> are either simple variables <span class="math">x</span> or <span class="math">y</span> or composite terms <span class="math">\lambda~v~t_1</span>. Having two terms next to each other <span class="math">(~t_1~t_2)</span> means "apply" <span class="math">t_1</span> to <span class="math">t_2</span>. The meaning of a term like <span class="math">\lambda~v~.~ t_1</span> is the value returned by the lambda abstraction. The meaning part is sometimes designated in writing by formulas using arrows, such as <span class="math">t_1~\rightarrow~t_2</span>.</p><p><div class="SIntrapara">Some "axioms" you supply (or that <a href="https://plato.stanford.edu/entries/church/">Alonzo Church</a> did).
</div><div class="SIntrapara"><ul><li><p>Beta reduction</p></li><li><p>Beta abstraction</p></li><li><p>Alpha conversion</p></li><li><p>Eta reduction</p></li><li><p>Normal Form</p></li></ul></div></p><p>The equivalent of the halting problem for the Turing machine is the reaching of a <span class="emph">normal form</span> in the  lambda calculus.</p><p>Functions have the form &#955; &lt;name&gt; . &lt;body&gt;</p><p>Note the "dot". This separates the name from the body of expressions that it names.</p><p>&lt;body&gt; is also an expression (note the recursion that is built in).</p><p><div class="SIntrapara"><span style="font-weight: bold">Some Opportunities for Practice</span>
</div><div class="SIntrapara"><ol><li><p>Write the lambda expression for the identity function? What is the identity function?</p></li><li><p>Apply the identity function to itself.</p></li><li><p>What is the identity function in Racket?</p></li><li><p><div class="SIntrapara">An interesting lambda expression is the "self-application" expression: <div class="math">\lambda~s . (s~s).</div>
With pencil and paper try to </div><div class="SIntrapara"><ol><li><p>apply this to the identity,</p></li><li><p>apply the identity to the self-application,</p></li><li><p>apply the self-application to itself. What is its termination status?</p></li></ol></div></p></li></ol></div></p><h5><a name="(part._ref~3acomp-cog)"></a>Computational Cognition References</h5><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td><p><span class="Autobibtarget"><a name="(autobib._.R..._.Mc.Clamrock.Marr's._three._levels~3a._.A._re-evaluation.Minds._and._.Machines._1,._pp..._185--1961991https~3a//doi..org/10..1007/.B.F00361036)"></a><span class="Autobibentry">R. McClamrock. Marr&rsquo;s three levels: A re-evaluation. <span style="font-style: italic">Minds and Machines</span> 1, pp. 185&ndash;196, 1991. <a href="https://doi.org/10.1007/BF00361036"><span class="url">https://doi.org/10.1007/BF00361036</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Greg._.Michaelson.An._introduction._to._functional._programming._through._.Lambda._calculus.Dover._.Publications2011https~3a//www..cs..rochester..edu/~7ebrown/173/readings/.L.C.Book..pdf)"></a><span class="Autobibentry">Greg Michaelson. <span style="font-style: italic">An introduction to functional programming through Lambda calculus</span>. Dover Publications, 2011. <a href="https://www.cs.rochester.edu/~brown/173/readings/LCBook.pdf"><span class="url">https://www.cs.rochester.edu/~brown/173/readings/LCBook.pdf</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Thomas._.Naselaris,._.Danielle._.Bassett,._.Alyson._.Fletcher,._.Konrad._.Kording,._.Nikolaus._.Kriegeskorte,._.Hendrikje._.Nienborg,._.Russell._.A._.Poldrak,._.Daphna._.Shohamy,._and._.Kendrick._.Kay.Cognitive._.Computational._.Neuroscience~3a._.A._.New._.Conference._for._an._.Emerging._.Discipline.Trends._in._.Cognitive._.Sciences._22,._pp..._365--3672018https~3a//dx..doi..org/10..1016/j..tics..2018..02..008)"></a><span class="Autobibentry">Thomas Naselaris, Danielle Bassett, Alyson Fletcher, Konrad Kording, Nikolaus Kriegeskorte, Hendrikje Nienborg, Russell A Poldrak, Daphna Shohamy, and Kendrick Kay. Cognitive Computational Neuroscience: A New Conference for an Emerging Discipline. <span style="font-style: italic">Trends in Cognitive Sciences</span> 22, pp. 365&ndash;367, 2018. <a href="https://dx.doi.org/10.1016/j.tics.2018.02.008"><span class="url">https://dx.doi.org/10.1016/j.tics.2018.02.008</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Michael._.Sch~c3~bctte.Logical._.Behaviorism.Encyclopedia._of._.Neuroscience,._pp..._372--3752008http~3a//dx..doi..org/10..1007/978-3-540-29678-2_596)"></a><span class="Autobibentry">Michael Sch&#252;tte. Logical Behaviorism. <span style="font-style: italic">Encyclopedia of Neuroscience</span>, pp. 372&ndash;375, 2008. <a href="http://dx.doi.org/10.1007/978-3-540-29678-2_596"><span class="url">http://dx.doi.org/10.1007/978-3-540-29678-2_596</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.A..._.M..._.Turing.On._computable._numbers._with._an._application._to._the._.Entscheidungsproblem.Proc..._.London._.Math..._.Soc..._42,._pp..._230--2651936)"></a><span class="Autobibentry">A. M. Turing. On computable numbers with an application to the Entscheidungsproblem. <span style="font-style: italic">Proc. London Math. Soc.</span> 42, pp. 230&ndash;265, 1936.</span></span></p></td></tr></table></p><h3>3<tt>&nbsp;</tt><a name="(part._.D.Es._and._.Spikes)"></a>Differential Equations and Spiking Neuron Models</h3><h4>3.1<tt>&nbsp;</tt><a name="(part._.Differential_.Equations_and_.Spiking_.Neuron_.Models)"></a>Differential Equations and Spiking Neuron Models</h4><h5>3.1.1<tt>&nbsp;</tt><a name="(part._.Spiking._.Neuron._.Models)"></a>Goals</h5><p>Why are Differential Equations an important technique for computational modelling in psychology and neuroscience?</p><p>Work on the modeling of the action potential eventually resulted in <a href="https://www.nobelprize.org/prizes/medicine/1963/summary/">Nobel Prizes</a>. The Hodgkin-Huxley equations that resulted from this work are differential equations. Subsequent models, even very marked simplifications such as the Integrate and Fire model, are also differential equations. When you rely on a simulation software that allows you to create populations of such neurons you are using, at least indirectly, a differential equation. It is worth knowing what they are.</p><p><div class="SIntrapara">Beyond that, the idea of a differential equation is a very intuitive and useful notion. You can recast the example of the <a href="#%28part._forgetting%29" data-pltdoc="x">What Is Cognition?</a> forgetting curve as a differential equation in which the change in the strength of a memory over time is proportional to the current strength of the memory  as a function of time. </div><div class="SIntrapara"><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Exponentials show up a lot in neuroscience and psychology. When you see a rate of change in a quantity that is proportional to the magnitude of that quantity there is an exponential hidden in there somewhere.</p></blockquote></blockquote></blockquote></div></p><p>Further, modern computers and their powers mean that we can often use differential equations in our models by naively implementing their effects as a series of very tiny steps. We might gain insight if we knew more about how to solve differential equations analytically, but often, if our goals our practical, that is running a simulation to see the result, we can be ignorant of differential equations at that level, and just deploy them as another practical tool. Just like you can now use software to implement Monte Carlo simulations in statistics without knowing the full details of the theory mathematically.</p><p>This gives us the following goals for this section:</p><ol><li><p>Learn what a Differential Equation is as a mathematical entity,</p></li><li><p>Get an intuition for differential equations by thinking of them as slopes,</p></li><li><p>Learn how they emerge as a natural effort to account for changing quantities in neuroscience and psychology,</p></li><li><p>Put this altogether by writing programs to implement the integrate and fire point neuron model and a version of the Hodgkin-Huxley neuron model.</p></li></ol><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>In preparation for things to come you might try to remember (or look up) what is the integral of one over x? In symbols, what is <span class="math">\int \frac{1}{x}~ dx</span>?</p></blockquote></blockquote></blockquote><h5>3.1.2<tt>&nbsp;</tt><a name="(part._.The_.Action_.Potential_-_a_very_short_review)"></a>The Action Potential - a very short review</h5><p>Our goal is to use differential equations in code written to simulate spiking neurons. Therefore, we ought to remind ourselves about the basics of what is a neuronal action potential.</p><p><div class="SIntrapara">Give yourself 10 minutes to brush up on what an action potential is. First, try and draw an action potential:
</div><div class="SIntrapara"><ul><li><p>What are the axes?</p></li><li><p>What ion causes the upward deflection?</p></li><li><p>What causes the repolarization?</p></li><li><p>Who discovered the action potential?</p></li><li><p>Who won the Nobel Prize for characterizing the ionic events of the action potential experimentally and building a mathematical model?</p></li></ul></div></p><p>Did you draw     <a href="https://commons.wikimedia.org/w/index.php?curid=44114666">this</a>?</p><p><span style="font-weight: bold">An aside: Notation</span></p><p>Mathematics is full of notation. And one person&rsquo;s notation is another person&rsquo;s jargon. The key thing is not to let yourself be scared off. Often the motivation for mathematical notation is just convenience: condensing something that would take a long time to say or write into an abbreviated form. It is the equivalent of saying "meh" or "lol".  Mathematical notation is just a technical emoji. You probably know the mathematical idea that is being represented; you just don&rsquo;t know the abbreviation that is being used. So, just like you can teach grandma the meaning of TMI you can with a little bit of practice get used to the Greek symbols that appear so often in mathematical presentations.</p><p>As a first exercise, write out in long hand what is meant by the
following:</p><p><div class="math">\sum_{\forall x \in \left\{ 1 , 2 , 3 \right \}} x ~=~ 6</div>
Did you write: <a href="../sections/notation-answer.txt">this?</a></p><h5>3.1.2.1<tt>&nbsp;</tt><a name="(part._.Multiple_.Ways_to_.Say_the_.Same_.Thing)"></a>Multiple Ways to Say the Same Thing</h5><p>Another thing to note about mathematical notation is that it often provides more than one way to say the same thing. Which notation is used depends on context and the technical community the work is intended for. Computer scientists frequently use <span class="stt">i</span> as a variable for indexing a loop. To the mathematician it is the complex part of an imaginary number <span class="math">i~=~\sqrt{-1}</span>, but engineers use <span class="stt">j</span> instead. Here are some of the many different ways you may see the derivative depicted.</p><p>Leibniz notation: <span class="math">\frac{dx}{dt}</span></p><p>Physicists often use this for derivatives with respect to time (<span class="stt">t</span>): <span class="math">\dot{x}</span></p><p>Mathematicians often use the variable itself as a representation for the function and use the number of "primes" to indicate how many derivatives to take: <span class="math">x&rsquo;</span></p><p>Or they may make the variable representing the function explicit if they think that will make their reasoning clearer in the present context: <span class="math">f&rsquo;(x)</span></p><p>This is called operator notation. You won&rsquo;t see it as much, but when doing certain kinds of proofs or reasoning more abstractly it can be much more convenient: <span class="math">D~f</span></p><h5>3.1.3<tt>&nbsp;</tt><a name="(part._.Derivatives_are_.Slopes)"></a>Derivatives are Slopes</h5><p>There may be many ways to write out the notation for a derivative, but the uniting concept behind them is as "rates of change." They are essentially just the slopes you learned about in secondary school. The old "rise over run" where the length of the run is made very, very small.</p><p>You might want to pause here and make sure you remember what a slope is.</p><p><div class="SIntrapara"><ul><li><p>Can you write the equation to find the slope of a line?</p></li><li><p>How would you apply this to a curve and not a line?</p></li><li><p>When in doubt return to definition. What is the definition of a slope of a function?</p></li></ul></div><div class="SIntrapara">We will see momentarily how to go from our basic understanding of the slope of a line to <span style="font-weight: bold">generalize</span> it to also include curves. This notion of generalizing is often a key step in developing an idea for modeling.</div></p><h5>3.1.3.1<tt>&nbsp;</tt><a name="(part._.Use_your_computer_as_a_tool_for_exploration)"></a>Use your computer as a tool for exploration</h5><p>Demonstrating something mathematically can give a great deal of satisfaction and ultimately is the guarantor of whether something is correct. Often we want to know more than whether something is correct in the abstract, we want to see specific examples. Sometimes pencil and paper are the best approach, but often we can do the same thing more quickly and more extensively by using our computer. Let&rsquo;s digress to use our computer for visualizing ideas about slopes. You should try to get these to work in Dr. Racket.</p><p><div class="SIntrapara">Example:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">xs</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktVal">2</span><span class="hspace">&nbsp;</span><span class="RktVal">3</span><span class="hspace">&nbsp;</span><span class="RktVal">4</span><span class="hspace">&nbsp;</span><span class="RktVal">5</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">ys</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktVal">2</span><span class="hspace">&nbsp;</span><span class="RktVal">4</span><span class="hspace">&nbsp;</span><span class="RktVal">6</span><span class="hspace">&nbsp;</span><span class="RktVal">8</span><span class="hspace">&nbsp;</span><span class="RktVal">10</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lines</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">vector</span><span class="hspace">&nbsp;</span><span class="RktSym">xs</span><span class="hspace">&nbsp;</span><span class="RktSym">ys</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">#:title</span><span class="hspace">&nbsp;</span><span class="RktVal">"A Line: What is it's slope?"</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_2.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><p><div class="SIntrapara">Example:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">function</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lambda</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">expt</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktVal">3</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">3</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">3</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">function</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lambda</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">12</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">16</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktVal">3</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">#:title</span><span class="hspace">&nbsp;</span><span class="RktVal">"A curve (of what?) showing the slope at a point."</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_3.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_4.png" alt="image" width="406" height="406"/></p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Derivatives are Instantaneous Slopes</p></blockquote></blockquote></blockquote><p>These plots are intended to demonstrate the idea that locally everything is linear. If you calculate the slope for your curve exactly like you do for a line you will get something that starts to look more and more like a line the smaller your "run" gets. The idea is that you pick two points that are "close enough" and your derivative becomes "close enough." At least with a computer. Mathematically, you just keep going to the limit.</p><p><a name="(elem._derivative)"></a>
<span style="font-weight: bold">Definition of the Derivative</span>
<div class="math">\frac{df}{dx} = \lim_{h \to 0}\frac{f(x + h) - f(x)}{(x + h) - x}\tag{D}</div>
<br/></p><h5>3.1.4<tt>&nbsp;</tt><a name="(part._use-deriv-to-solve)"></a>Using Derivatives to Solve Problems With a Computer</h5><h5>3.1.4.1<tt>&nbsp;</tt><a name="(part._.What_is_the_square_root_of_128_)"></a>What is the square root of 128?</h5><p>We want to know the value of <span class="math">x</span> that makes <span class="math">128 =x^2</span> true?</p><p><span class="refelem"><span class="refcolumn"><span class="refcontent">Always use the computer for the busy work when you can. Your computer can solve many mathematical problems for you. For example, requiring <span class="stt">symalg</span> we can programmatically find that the derivative of <span class="math">x^2</span> is
<span class="math">2 x</span>. Look at the code for this <span class="stt">margin-note</span> and you will see how I computed that with racket (and then typeset it).</span></span></span></p><ul><li><p>Come up with a guess.</p></li><li><p>Calculate the error.</p></li><li><p>Adjust your guess based on the error.</p></li><li><p>This adjustment will use the derivative.</p></li></ul><h5>3.1.4.1.1<tt>&nbsp;</tt><a name="(part._.Working_.Through_an_.Example)"></a>Working Through an Example</h5><p>Let&rsquo;s say we want to solve for <span class="math">x</span> when <span class="math">x^2 = 128</span>. How might we start? When in doubt, guess!</p><p>How much is your guess off?</p><p><div class="math">\mbox{Error} = \mbox{(my guess)}^2 - \mbox{128}</div></p><p>What we want to do now is adjust our guess. Since we know how much our function changes its output for each adjustment in the input, <span class="refelem"><span class="refcolumn"><span class="refcontent">How do we know this? Our derivative is a <span style="font-style: italic">rate of change</span>.</span></span></span> we can revise our guess based on this necessary adjustment. If we are still wrong, we just repeat the process.</p><p>To get there let us consider representing the ratio of how our function&rsquo;s output changes for changes in input. We can just make things concrete.</p><p><div class="math">\frac{\Delta~\mbox{output}}{\Delta~\mbox{input}} = \frac{\mbox{function(input_1)} - \mbox{function(input_0)}}{\mbox{input_1} - \mbox{input_0}}</div></p><p>If you take a look at the definition of the derivative <a href="#%28elem._derivative%29" data-pltdoc="x">(equation D)</a> above you will see the resemblance, except for the absence of the limit. When trying to solve this problem we don&rsquo;t initially know both inputs, but we do know that when we put in the solution to our problem we will get 128. And we also know that we can compute the derivative. A bit of rearranging and renaming give us.<span class="refelem"><span class="refcolumn"><span class="refcontent">Can you map the steps I took to get this equation from the one above?</span></span></span></p><p><div class="SIntrapara">Examples:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">df</span><span class="hspace">&nbsp;</span><span class="RktSym">g</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">2.0</span><span class="hspace">&nbsp;</span><span class="RktSym">g</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">update-guess</span><span class="hspace">&nbsp;</span><span class="RktSym">g</span><span class="hspace">&nbsp;</span><span class="RktSym">target</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">target</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">expt</span><span class="hspace">&nbsp;</span><span class="RktSym">g</span><span class="hspace">&nbsp;</span><span class="RktVal">2.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">df</span><span class="hspace">&nbsp;</span><span class="RktSym">g</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">my-sqrt</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">target</span><span class="hspace">&nbsp;</span><span class="RktVal">128.0</span><span class="RktPn">]</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">guess</span><span class="hspace">&nbsp;</span><span class="RktVal">7.0</span><span class="RktPn">]</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">tol</span><span class="hspace">&nbsp;</span><span class="RktVal">1e-6</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">let*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">udg</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">update-guess</span><span class="hspace">&nbsp;</span><span class="RktSym">guess</span><span class="hspace">&nbsp;</span><span class="RktSym">target</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">current-guess</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktSym">guess</span><span class="hspace">&nbsp;</span><span class="RktSym">udg</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">if</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&lt;</span><span class="hspace">&nbsp;</span><span class="RktSym">udg</span><span class="hspace">&nbsp;</span><span class="RktSym">tol</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktSym">current-guess</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">my-sqrt</span><span class="hspace">&nbsp;</span><span class="RktSym">target</span><span class="hspace">&nbsp;</span><span class="RktSym">current-guess</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><p>(my-sqrt 55.0 4.0)]</p><ul><li><p>What is a <span class="stt">cube root</span>?</p></li><li><p>What is the derivative of <span class="math">x^3</span>?</p></li><li><p>Write a Racket program to computer the cube root of a give number.</p></li></ul><h5>3.1.4.2<tt>&nbsp;</tt><a name="(part._.Practice_.Simulating_.With_.D.Es)"></a>Practice Simulating With DEs</h5><h5>3.1.4.2.1<tt>&nbsp;</tt><a name="(part._.Frictionless_.Springs)"></a>Frictionless Springs</h5><p>We want to code neurons, but to get there we should feel comfortable with the underlying tool or we won&rsquo;t be able to adapt it or re-use it for some new purpose. I don&rsquo;t want to give you a fish. I want to teach you how to fish.</p><p>By working with an example simpler than a neuron, and one for which you might have more intuition, such as a simple spring or "slinky" I hope you will get a better <span style="font-style: italic">feel</span> for how the numbers, equations, and code all relate. Then we can move on to the neuronal application.</p><p>The equation of a frictionless spring is:</p><p><a name="(elem._spring)"></a>
<div class="math">\frac{d^2 s}{dt^2} = -P~s\tag{S}</div>
<br/></p><p>where &rsquo;s&rsquo; refers to space, &rsquo;t&rsquo; refers to time, and &rsquo;P&rsquo; is a constant, often called the spring constant, that indicates how stiff or springy the spring is.</p><p>Imagine that we knew this derivative. It would tell us how much space the spring head would move for a given, very small, increment of time. We could then just add this to our current position to get the new position and repeat. This method of using a derivative to iterate forward is sometimes called the Euler method.</p><p>Returning to our definition of the derivative:</p><p><div class="math">\frac{s(t + \Delta t) - s(t)}{\Delta t} = velocity \approx \frac{d s}{d t}</div></p><p>But our spring equation is not given in terms of the velocity it is given in terms of the acceleration which is the second derivative. Therefore, to find our new position we need the velocity, but we only have the acceleration. However, if we knew the acceleration and the velocity we could use that to calculate the new velocity. Unfortunately we don&rsquo;t know the velocity, unless ... , maybe we could just assume something. Let&rsquo;s say it is zero because we have started our process where we have stretched the spring, and are holding it, just before letting it go.</p><p>How will our velocity change with time?</p><p><div class="math">\frac{v(t + \Delta t) - v(t)}{\Delta t} = acceleration \approx \frac{d v}{d t} = \frac{d^2 s}{d t^2}</div></p><p>And we have a formula for this. We can now bootstrap our simulation.</p><p>Note the similarity of the two functions. You could write a helper function that was generic to this pattern of old value + rate of change times the  time step, and just used the pertinent values.</p><p>How do we know the formula for acceleration? We were given it in <a href="#%28elem._spring%29" data-pltdoc="x">Equation S</a> above.</p><p><div class="SIntrapara">Examples:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">require</span><span class="hspace">&nbsp;</span><span class="RktVal">"./code/spring.rkt"</span><span class="RktPn">)</span></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">spring-results</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">release-spring</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lines</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">vector</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">fourth</span><span class="hspace">&nbsp;</span><span class="RktSym">spring-results</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">third</span><span class="hspace">&nbsp;</span><span class="RktSym">spring-results</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_5.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><h5>3.1.4.3<tt>&nbsp;</tt><a name="(part._.Damped_.Oscillators)"></a>Damped Oscillators</h5><p>Provide the code for the damped oscillator. It has the formula of</p><p><div class="math">\frac{d^2 s}{dt^2} = -P~s(t) - k~v(t)</div></p><p>This should really only need to change a couple of lines to update the model to be able to handle the damped version as well. You might want to edit <a href="./../code/spring.rkt">spring.rkt</a>.</p><h4>3.2<tt>&nbsp;</tt><a name="(part._.Integrate_and_.Fire_.Neuron)"></a>Integrate and Fire Neuron</h4><p>In this section we take a look at the history and math of the computational model of neuron firing called "Integrate and Fire" (I&amp;F).
 The I&amp;F model uses math essentially the same as the spring example.</p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Is the integrate and fire model used much in modeling in the present time.? <a href="https://scholar.google.com/scholar?as_ylo=2020&amp;q=%22integrate+and+fire%22+neuron&amp;hl=en&amp;as_sdt=7,39">Answer</a>.</p></blockquote></blockquote></blockquote><h5>3.2.1<tt>&nbsp;</tt><a name="(part._.History_of_the_.Integrate_and_.Fire_.Model)"></a>History of the Integrate and Fire Model</h5><h5>3.2.1.1<tt>&nbsp;</tt><a name="(part._.Louis_.Lapicque_-_.Earlier_.Computational_.Neuroscientist)"></a>Louis Lapicque - Earlier Computational Neuroscientist</h5><p><a href="https://link.springer.com/content/pdf/10.1007/s00422-007-0190-0.pdf">Modern Commentary on Lapique&rsquo;s Neuron Model</a></p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="Lapicque_laboratoire.png" alt="" width="321" height="500"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3alapique-lab))" x-target-lift="Figure"></a>Figure&nbsp;4: </span>The Lapique Lab at the Sorbonne early 1900s</span></p></blockquote><p><a href="http://www.snv.jussieu.fr/brette/papers/Lap07.pdf">Original Lapique Paper (scanned; pdf)</a></p><p><a href="https://fr.wikipedia.org/wiki/Louis_Lapicque">Brief Biographical Details of Lapicque</a>.</p><h5>3.2.1.2<tt>&nbsp;</tt><a name="(part._.Lord_.Adrian_and_the_.All-or-.None_.Action_.Potential)"></a>Lord Adrian and the All-or-None Action Potential</h5><p>One of the first demonstrations of the all-or-none nature of the neuronal action potential was made by Lord Adrian. Lord Adrian was an interesting scientific figure. He asserted that some of his prowess at electrophysiology stemmed from his interest in and training in fencing.</p><p><div class="SIntrapara">To answer the questions:
</div><div class="SIntrapara"><ul><li><p>When was the action potential demonstrated?</p></li><li><p>What was the experimental animal used by Adrian?</p></li></ul></div><div class="SIntrapara">Consult this <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1420429/pdf/jphysiol01990-0084.pdf">this pdf for answers.</a></div></p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Want more details? There is an excellent free book available <a href="https://lcnwww.epfl.ch/gerstner/SPNM/SPNM.html"><span style="font-weight: bold">Spiking Neurons</span>.</a> They also have another more modern book out too <a href="http://neuronaldynamics.epfl.ch/online/index.html"><span style="font-weight: bold">Neuronal Dynamics</span>.</a></p></blockquote></blockquote></blockquote><h5>3.2.2<tt>&nbsp;</tt><a name="(part._sec~3aiandf)"></a>The Integrate and Fire Equation</h5><p>While Hodgkin and Huxley provided the first robust computational model of the neuronal action potential their model is quite complex, as we will soon see. Is all that complexity necessary? That of course depends on the nature of the scientific question, and if we are primarily interested in whether a spike has or has not occurred, and not the ionic events that produced the spike, we may find our experimental questions dealt with much more concisely by a much simpler model of neuronal spiking: the leaky integrate and fire model.</p><p>The formula for the leaky integrate and fire neuron is:</p><p><a name="(elem._iandf-eq)"></a>
<div class="math">\tau \frac{dV(t)}{dt} = -V(t) + R~I(t) .\tag{I}</div>
<br/></p><p>In the next sections we will describe how this simplification came to be, and use it as the basis for learning some of the elementary electrical laws and relations upon which it is based.</p><h5>3.2.2.1<tt>&nbsp;</tt><a name="(part._.Electronics_.Background)"></a>Electronics Background</h5><p>The following questions are the ones we need answers to to derive our integrate and fire model.</p><ol><li><p>What is Ohm&rsquo;s Law?</p></li><li><p>What is Kirchoff&rsquo;s Point Rule?</p></li><li><p>What is Capacitance?</p></li><li><p>What is the relation between current and charge?</p></li></ol><h5>3.2.2.2<tt>&nbsp;</tt><a name="(part._.Formula_.Discussion_.Questions)"></a>Formula Discussion Questions</h5><p>To understand our formula clearly we should review the meaning of the key symbols and notation.</p><ol><li><p>What does <span class="math">\frac{dV}{dt}</span> mean?</p></li><li><p>What does <span class="math">\frac{1}{\tau}</span> mean?</p></li><li><p>Why does the voltage term on the right have a negative sign?</p></li><li><p>What is <span class="math">I(t)</span>?</p></li></ol><p>To derive our equation we need to put all these fact together.</p><p>We recall that just like we used the derivative to help us figure out where the spring would be some small increment of time into the future, we use the same approach to compute our future voltage. That future voltage will also include a term that reflects an additional current that we have "experimentally" injected.</p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Can you tell why, looking at the integrate and fire equation, if we don&rsquo;t reach the firing threshold, we see an exponential decay?</p></blockquote></blockquote></blockquote><p>Deriving the IandF Equation
<div class="math">\begin{align*}
     I &amp;= I_R + I_C &amp; (a) \\
       &amp;= I_R + C\frac{dV}{dt} &amp; (b)\\
       &amp;= \frac{V}{R} + C\frac{dV}{dt} &amp; (c)\\
     RI  &amp;= V + RC\frac{dV}{dt} &amp; (d) \\
     \frac{1}{\tau} (RI-V)  &amp;= \frac{dV}{dt} &amp; (e)\\
     \end{align*}</div></p><ul><li><p>a: Kirchoff&rsquo;s point rule,</p></li><li><p>b: the relationship between current, charge, and their derivatives</p></li><li><p>c: Ohm&rsquo;s law</p></li><li><p>d:multiply through by R</p></li><li><p>e: rearrange and define <span class="math">\tau</span></p></li></ul><h5>3.2.2.3<tt>&nbsp;</tt><a name="(part._.Coding_up_the_.Integrate_and_.Fire_.Neuron)"></a>Coding up the Integrate and Fire Neuron</h5><p>Most of the integrate and fire implementation is conceptually and practically identical to the spring example. You assume a starting voltage (initial state) and then you update that voltage using the differential equation for how voltage changes with time (<span class="math">\frac{dV}{dt}</span>).</p><p>There is one critical difference though. Unlike real neurons the Integrate and Fire neuron model does not have a natural threshold and spiking behavior. You pick a threshold and everyone time your voltage reaches that threshold you designate a spike and reset the voltage.</p><p>What I added below is a strictly cosmetic amendment that changes the first value after the threshold to a number much higher than the threshold so that when plotted it creates the visual appearance of a spike.</p><h5>3.2.2.3.1<tt>&nbsp;</tt><a name="(part._.Class_.Exercise__.Adding_a_refractory_period_to_the_.Integrate_and_.Fire_model)"></a>Class Exercise: Adding a refractory period to the Integrate and Fire model</h5><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>What is the refractory period for a neuron?</p></blockquote></blockquote></blockquote><p>In class make sure you can get the <a href="./../code/iandf.rkt">integrate and fire model</a> working in Dr. Racket. After you get the basic model working trying altering the input current to see how that affects the number of spikes and the regularity of their spiking.</p><p>Next, change the form of the input current to be something other than a constant. I suggest trying a sine wave. This will give you a chance to sample some of racket&rsquo;s potential.</p><p>Find out how to take the sin of a number. Then learn how to <span class="stt">map</span> the sin function over a list of numbers. If you use <span class="stt">in-range</span> you can create a stream of numbers from a minimum to a maximum for a given step size. Then you may want to shift up or scale all the numbers to make them non-zero. This could be done by mapping again. The <span class="stt">map</span> function is very powerful and allows you to avoid writing a lot of lengthy looping code.</p><p>After you have done that edit the code to include a refractory period. First, decide on the logic of how to do this and only after that start editing the code to implement it.</p><p>The next examples walk through the code and describe some of the ideas.</p><p><div class="SIntrapara"><span style="font-weight: bold"><span style="font-style: italic">Defining our parameters:</span></span></div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">dt</span><span class="hspace">&nbsp;</span><span class="RktVal">0.05</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">max-t</span><span class="hspace">&nbsp;</span><span class="RktVal">10</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">init-t</span><span class="hspace">&nbsp;</span><span class="RktVal">0.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">start-time</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">stop-time</span><span class="hspace">&nbsp;</span><span class="RktVal">6.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">cap</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">res</span><span class="hspace">&nbsp;</span><span class="RktVal">2</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">threshold</span><span class="hspace">&nbsp;</span><span class="RktVal">3.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">spike-display</span><span class="hspace">&nbsp;</span><span class="RktVal">8.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">init-v</span><span class="hspace">&nbsp;</span><span class="RktVal">0.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">voltage</span><span class="hspace">&nbsp;</span><span class="RktSym">init-v</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">injection-current</span><span class="hspace">&nbsp;</span><span class="RktVal">4.3</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">injection-time</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">cons</span><span class="hspace">&nbsp;</span><span class="RktSym">start-time</span><span class="hspace">&nbsp;</span><span class="RktSym">stop-time</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">tau</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktSym">res</span><span class="hspace">&nbsp;</span><span class="RktSym">cap</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></blockquote></div></p><p>This is a good habit to develop with your code.
Do not "hard code" in values for variables that you will have to write in multiple locations in a file.
It makes it hard to update and debug your code.
Give sensible and short names to things you will use in your code.
Then define values for those at the top of your code.
This gives you one place to look for explanations and reminders, and also gives you a place where when you make a single change it will propagate through your code.</p><p><div class="SIntrapara"><span style="font-weight: bold"><span style="font-style: italic">Euler&rsquo;s Method (again):</span></span></div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">update</span><span class="hspace">&nbsp;</span><span class="RktSym">old-value</span><span class="hspace">&nbsp;</span><span class="RktSym">rate-of-change</span><span class="hspace">&nbsp;</span><span class="RktSym">time-step</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktSym">rate-of-change</span><span class="hspace">&nbsp;</span><span class="RktSym">time-step</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">old-value</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><p>This is the same updating rule that we used in the spring example.
It is a rewriting of the definition of the derivative.
This is sometimes referred to as <a href="https://en.wikipedia.org/wiki/Euler_method">Euler&rsquo;s method</a>.</p><p><div class="SIntrapara"><span style="font-weight: bold"><span style="font-style: italic">Helper functions:</span></span></div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">dv-dt</span><span class="hspace">&nbsp;</span><span class="RktSym">localres</span><span class="hspace">&nbsp;</span><span class="RktSym">locali</span><span class="hspace">&nbsp;</span><span class="RktSym">localv</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktSym">tau</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktSym">localres</span><span class="hspace">&nbsp;</span><span class="RktSym">locali</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">localv</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">between</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktPn">#:lower</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">lower</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">car</span><span class="hspace">&nbsp;</span><span class="RktSym">injection-time</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:upper</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">upper</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">cdr</span><span class="hspace">&nbsp;</span><span class="RktSym">injection-time</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:if-true</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">if-true</span><span class="hspace">&nbsp;</span><span class="RktSym">injection-current</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:if-false</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">if-false</span><span class="hspace">&nbsp;</span><span class="RktVal">0.0</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">if</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">and</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&gt;=</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">lower</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&lt;=</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">upper</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">if-true</span><span class="hspace">&nbsp;</span><span class="RktSym">if-false</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">voltage-choice</span><span class="hspace">&nbsp;</span><span class="RktSym">curr-volt</span><span class="hspace">&nbsp;</span><span class="RktSym">spike-status</span><span class="hspace">&nbsp;</span><span class="RktPn">#:thr</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">thr</span><span class="hspace">&nbsp;</span><span class="RktSym">threshold</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:sd</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">sd</span><span class="hspace">&nbsp;</span><span class="RktSym">spike-display</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">cond</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktPn">(</span><span class="RktSym">and</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&gt;</span><span class="hspace">&nbsp;</span><span class="RktSym">curr-volt</span><span class="hspace">&nbsp;</span><span class="RktSym">thr</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">not</span><span class="hspace">&nbsp;</span><span class="RktSym">spike-status</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">sd</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">spike-status</span><span class="hspace">&nbsp;</span><span class="RktVal">#i0.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktVal">#t</span><span class="hspace">&nbsp;</span><span class="RktSym">curr-volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><p>Just as we were given the equation for a spring, here we are given the equation for the I&amp;F neuron, which we translate from math to code. In addition, I created some smaller "helper" functions. I like a style that gives my functions default values. Then I don&rsquo;t have to enter so many arguments when I call the function. This, of course, only makes sense if there are values which you input to your function and that rarely change. I also find it convenient to use a style where I have keywords for my functions. Then I can change the order that I enter things. It does make my code longer, because I have to type the keywords when specifying the input to my functions. This is what I am doing with the lines that look like: <span class="RktMeta"></span><span class="RktPn">#:if-false</span><span class="RktMeta"></span><span class="hspace">&nbsp;</span><span class="RktMeta"></span><span class="RktPn">[</span><span class="RktSym">if-false</span><span class="RktMeta"></span><span class="hspace">&nbsp;</span><span class="RktMeta"></span><span class="RktVal">0.0</span><span class="RktPn">]</span><span class="RktMeta"></span></p><p>It would be possible to collapse all this into one big function, but that would be harder for me to understand, and harder for you to understand.
In general, try to write short little functions that do one thing.
Then you can chain those small functions together to accomplish the larger task.</p><p><div class="SIntrapara"><span style="font-weight: bold"><span style="font-style: italic">Running our model:</span></span></div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">run-iandf-sim</span><span class="hspace">&nbsp;</span><span class="RktPn">#:tolerance</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">tolerance</span><span class="hspace">&nbsp;</span><span class="RktVal">0.1</span><span class="RktPn">]</span><span class="hspace">&nbsp;</span><span class="RktPn">#:max-time</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">max-time</span><span class="hspace">&nbsp;</span><span class="RktVal">10</span><span class="RktPn">]</span><span class="hspace">&nbsp;</span><span class="RktPn">#:max-iter</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktSym">max-iter</span><span class="hspace">&nbsp;</span><span class="RktVal">10000</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">for*/fold</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">t</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">v</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">accum</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">(</span><span class="RktVal">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:result</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">reverse</span><span class="hspace">&nbsp;</span><span class="RktSym">accum</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">n</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">in-range</span><span class="hspace">&nbsp;</span><span class="RktSym">max-iter</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:break</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&gt;</span><span class="hspace">&nbsp;</span><span class="RktSym">t</span><span class="hspace">&nbsp;</span><span class="RktSym">max-time</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">let</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">spike</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&lt;</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">abs</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">v</span><span class="hspace">&nbsp;</span><span class="RktSym">spike-display</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">tolerance</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">values</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktSym">dt</span><span class="hspace">&nbsp;</span><span class="RktSym">t</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">between</span><span class="hspace">&nbsp;</span><span class="RktSym">t</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">voltage-choice</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">update</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">dv-dt</span><span class="hspace">&nbsp;</span><span class="RktSym">res</span><span class="hspace">&nbsp;</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">dt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">spike</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">cons</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktSym">t</span><span class="hspace">&nbsp;</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">accum</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><p><div class="SIntrapara">Examples:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">require</span><span class="hspace">&nbsp;</span><span class="RktVal">"./code/iandf.rkt"</span><span class="RktPn">)</span></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">iandf-results</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">run-iandf-sim</span><span class="hspace">&nbsp;</span><span class="RktPn">#:max-time</span><span class="hspace">&nbsp;</span><span class="RktVal">10.0</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lines</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">vector</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">first</span><span class="hspace">&nbsp;</span><span class="RktSym">iandf-results</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">third</span><span class="hspace">&nbsp;</span><span class="RktSym">iandf-results</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_6.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><p>Though things may look more complex than the spring example, it is because I have so many more <span style="font-style: italic">local</span> variables to define.
The basic flow is still just a <span style="font-weight: bold">loop</span>.</p><p>Visualizations can be essential in helping you to see and understand the function of your computational program. Thus, while it is only cosmetic, I find the addition of the apparent spike helps me to see what the output of my simulation is. In another context, e.g. if were only counting spikes this decorative element would be un-needed complexity.</p><h5>3.2.3<tt>&nbsp;</tt><a name="(part._.Integrate_and_.Fire_.Homework)"></a>Integrate and Fire Homework</h5><p>The Integrate and Fire homework has two components. One practical and one theoretical.</p><p>Practically, submit an integrate and fire racket program that alters mine in some meaningful way. You might change the plot or the type of current input. You might examine how the results depends on the size of the time step used. Just something to show that you can edit code and keep it working.</p><p>Theoretically, look at <a href="https://redwood.berkeley.edu/wp-content/uploads/2018/08/mainen-sejnowski.pdf">this article (pdf)</a> and tell me how you feel our integrate and fire model compares to these actual real world spiking data when both are give constant input. What are the implications for using the integrate and fire model as a model of neuronal function?</p><h4>3.3<tt>&nbsp;</tt><a name="(part._.Hodgkin._and._.Huxley)"></a>Hodgkin and Huxley Model of the Action Potential</h4><h5>3.3.1<tt>&nbsp;</tt><a name="(part._.Background_and_.Motivation)"></a>Background and Motivation</h5><p>Hodgkin and Huxley, the people as well as their model, provide a nice example for how to structure one&rsquo;s education to enable one to do work that combines mathematics, models, and empirical data. Each was a scientist from one side of the aisle who sought training from the other.</p><p>Another lesson taught by the Hodgkin and Huxley model is a meta lesson: you may not understand in the beginning what your true problem even is. You need to be prepared for it to appear, and when it does to be able to attack it with the methods appropriate to its nature. Rather than being the man with a hammer and seeing everything as a nail, you need to carry a Swiss Army knife.</p><h5>3.3.1.1<tt>&nbsp;</tt><a name="(part._.Biographical_.Sources)"></a>Biographical Sources</h5><p>To learn more about these remarkable individuals and their careers you can consult the biographies of the Nobel Foundation. The Nobel Prize organization keeps biographies of all recipients <a href="https://www.nobelprize.org/prizes/medicine/1963/hodgkin/biographical/">Hodgkin</a>,  <a href="https://www.nobelprize.org/prizes/medicine/1963/huxley/biographical/">Huxley</a>.</p><p>This <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3424716/pdf/tjp0590-2571.pdf">article (pdf)</a> is a nice summary of the work done by Hodgkin and Huxley. You might look for how long it took Huxley to calculate his simulation of one action potential numerically using essentially the same method we will be using. Compare how long it takes you to how long it took him.</p><h5>3.3.1.2<tt>&nbsp;</tt><a name="(part._.Model_.Description__detailed_)"></a>Model Description (detailed)</h5><p>I will not be describing the Hodgkin and Huxley model in detail as there are many other sources that do an excellent job and are online and freely available. One recommended source is Gersnter&rsquo;s <a href="https://lcnwww.epfl.ch/gerstner/SPNM/node14.html#table-HH1">book&rsquo;s chapter</a>. Gerstner goes into more detail than I do. If you have problems getting things to work, or just want a more detailed mathematical explanation this is an excellent resource.</p><h5>3.3.1.3<tt>&nbsp;</tt><a name="(part._.Comments_and_.Steps_in_.Coding_the_.Hodgkin_.Huxley_.Model)"></a>Comments and Steps in Coding the Hodgkin Huxley Model</h5><p>Some introductory reminders and admonitions:</p><p>The current going in to the cell is intended to represent what an electrophysiologist would inject in their laboratory setting, or what might be changed by the input from other neurons.
The total current coming out of the neuron is the sum of the capacitance (due to the lipid bilayer), and the resistance (due to the ion channels).
This is <span style="font-weight: bold">Kirchoff&rsquo;s</span> rule implemented in the Hodgkin and Huxley model.</p><p>Recall that in the Integrate and Fire model we lumped all our ionic
events together into one term:</p><p><div class="math">\tau \frac{dV(t)}{dt} = -V(t) + R~I(t)</div></p><p>The Hodgkin and Huxley model is basically the same as the Integrate and Fire model. What differs is that total conductance is decomposed into three parts where we have a resistance <span style="font-style: italic">for each ion channel</span>.
The rule for currents in parallel is to apply Kirchoff&rsquo;s and Ohm&rsquo;s laws realizing that they all experience the same voltage, thus the currents sum. The Hodgkin and Huxley model has components for Sodium (Na), Potassium (K), and negative anions (still lumped as "leak").</p><p><div class="math">\sum_i I_R(t) = \bar{g}_{Na} m^3 h l(V(t) - E_{Na}) + \bar{g}_{K} n^4 (V(t) - E_{K}) + \bar{g}_{L} (V(t) - E_{L})</div></p><p><div class="math">I_{tot} = I_r + I_C</div></p><p>By the same logic as for the integrate land fire <span class="math">I_C = c~\frac{dV}{dt}</span>.</p><p><div class="math">I_{tot} = \bar{g}_{Na} m^3 h (V(t) l- E_{Na}) + \bar{g}_{K} n^4 (V(t) - E_{K}) + \bar{g}_{L} (V(t) - E_{L}) + c~\frac{dV}{dt}</div></p><p>If you rearrange terms you can get the <span class="math">\frac{dV}{dt}</span> on one side of the equation by itself.</p><p><div class="math">c~\frac{dV}{dt} = I_{tot} - (\bar{g}_{Na} m^3 h (V(t) - E_{Na}) + \bar{g}_{K} n^4 (V(t) - E_{K}) + \bar{g}_{L} (V(t) - E_{L}))\tag 1</div></p><h5>3.3.1.3.1<tt>&nbsp;</tt><a name="(part._.Test_your_understanding)"></a>Test your understanding</h5><p>You cannot program what you don&rsquo;t understand. A major headache in any programming task comes from starting to write your code too soon. Your time to completion will often be shorter if you delay starting the writing of your code until you can confirm a solid understanding of the intent of your code and the flow of the algorithm you are implementing. It is a mistake to think that programming will bring understanding. Programming may bring you new insights or help you extend your understanding, but it cannot turn a confused implementation into a working one. Sometimes you think you understand something, and in the act of coding you find that you really do not. Or else that there are elements of the original problem that were under specified. At this point you should stop writing code, and go back to the blackboard to work through what it is you are trying to do. Make your coding about implementing an idea. Do not expect it to deliver the idea.</p><p>So, in that light, and before you start coding, ask yourself,</p><ol><li><p>What are the <span class="math">\bar{g}_*</span> terms?</p></li><li><p>What are the <span class="math">E_{*}</span> terms?</p></li><li><p>What do m,n, and h represent?</p></li><li><p>Where did these equations come from?</p></li></ol><h5>3.3.1.4<tt>&nbsp;</tt><a name="(part._.It_s_.Differential_.Equations_.All_the_.Way_.Down)"></a>It&rsquo;s Differential Equations All the Way Down</h5><p>Although the Hodgkin and Huxley model uses the same mathematics as the Integrate and Fire model, and we will use the same Euler&rsquo;s method to step forward and calculate model terms that evolve over time, this model is more complex in two ways that make the coding more intricate. First, it has multiple derivatives and derivatives at multiple levels. Each of the <span style="font-style: italic">m</span>, <span style="font-style: italic">n</span>, and <span style="font-style: italic">h</span> terms are also changing and regulated by a differential equation. They are dependent on voltage. For example, <div class="math">\dot{m} = \alpha_m (V)(1 - m) - \beta_m (V) m</div>.</p><p><div class="SIntrapara"><span style="font-weight: bold">Test Your Understanding</span>
</div><div class="SIntrapara"><ol><li><p>Each of the m,n, and h terms have their own equation of exactly the same form, but with their unique alphas and betas (that is what the subscript means).</p></li><li><p>What does the V in parentheses mean?</p></li><li><p>When they were finally sequenced (decades later), what do you think was the number of sub-units that the sodium and potassium channels were found to have?</p></li></ol></div></p><h5>3.3.1.5<tt>&nbsp;</tt><a name="(part._.Getting_.Started)"></a>Getting Started</h5><p>You will need to make some assumptions to get your initial conditions.</p><ul><li><p>If you allow <span class="math">t \rightarrow \infty \mbox{, then } \frac{dV}{dt}=?</span></p></li><li><p>You assume that it goes to zero; that is, you reach steady state. Then you can solve for some of the constants.</p></li><li><p>Where do the constants come from?</p></li><li><p>They come from experiments, and you use what you are given.</p></li><li><p>Assume the following constants - they are set to assume a resting potential of zero (instead of what and why doesn&rsquo;t this matter)?</p></li><li><p>These constants also work out to enforce a capacitance of 1</p></li></ul><h5>3.3.1.5.1<tt>&nbsp;</tt><a name="(part._.Constants)"></a>Constants</h5><p><table cellspacing="0" cellpadding="0" class="boxed"><tr><td><p><span style="font-weight: bold">Constant</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Value</span></p></td></tr><tr><td><p>ena</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>115</p></td></tr><tr><td><p>gna</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>120</p></td></tr><tr><td><p>ek</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>-12</p></td></tr><tr><td><p>gk</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>36</p></td></tr><tr><td><p>el</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>10.6</p></td></tr><tr><td><p>gl</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0.3</p></td></tr></table></p><p><span style="font-weight: bold">WARNING</span> These constants are adjusted to make the resting potential 0 and the capacitance 1.0. If you want your model to have a biological resting potential you will need to adjust these values, but when you think about it the scale is rather arbitrary. What does water freeze at 0 or -32? Well it depends on the scale: centigrade or fahrenheit. Same for neurons. Why not use a scale that makes the math simpler. Focus on the relative behavior not some absolute, and rather arbitrary, numbers.</p><h5>3.3.1.6<tt>&nbsp;</tt><a name="(part._.Alpha_and_.Beta_.Formulas)"></a>Alpha and Beta Formulas</h5><p><span class="math">\alpha_{n}(V_{m})={\frac {0.01(10-V_l{m})}{\exp {\big (}{\frac{10-V_{m}}{10}}{\big )}-1}}</span></p><p><span class="math">\alpha_{m}(V_{m})={\frac {0.1(25-V_{lm})}{\exp {\big (}{\frac {25-V_{m}}{10}}{\big )}-1}}</span></p><p><span class="math">\alpha _{h}(V_{m})=0.07\exp {\bigg (l}{\frac {-V_{m}}{20}}{\bigg )}</span></p><p><span class="math">\beta _{n}(V_{m})=0.125\exp {\bigg (l}{\frac {-V_{m}}{80}}{\bigg )}</span></p><p><span class="math">\beta _{m}(V_{m})=4\exp {\bigg (}{\frac {-V_{m}}{18}}{\bigg )}</span></p><p><div class="SIntrapara"><span class="math">\beta_{h}(V_{m})={\frac {1}{\exp {\big (}{\frac {30-V_{m}}{10}}{\big)}+1}}</span>
</div><div class="SIntrapara"><blockquote class="SubFlow" style="background-color:linen;"><p><span style="font-weight: bold">Programming Concept: Hash Tables</span>. Often when writing a more complex program you will have collections of values that go together conceptually. If you declare each as its own variable your functions that need the entire collection can require very long strings of arguments. It is often convenient to group such variables into a collection type recognized by your programming language. Python dictionaries are one approach. R and other languages may make it easier to use <span style="font-style: italic">objects</span>. In this instance I am using a Racket hash table. I provide a name and a value and then an overall name for the table of name-value pairs.</p></blockquote></div><div class="SIntrapara"><p><div class="SIntrapara">Defining the Basic Neuron Parameters</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-details</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">dt</span><span class="hspace">&nbsp;</span><span class="RktVal">0.05</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">init-t</span><span class="hspace">&nbsp;</span><span class="RktVal">0.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">start-time</span><span class="hspace">&nbsp;</span><span class="RktVal">10.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">stop-time</span><span class="hspace">&nbsp;</span><span class="RktVal">34.05</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">cap</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">init-v</span><span class="hspace">&nbsp;</span><span class="RktVal">0.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">injection-current</span><span class="hspace">&nbsp;</span><span class="RktVal">20.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">ena</span><span class="hspace">&nbsp;</span><span class="RktVal">115.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">gna</span><span class="hspace">&nbsp;</span><span class="RktVal">120.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">ek</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span>2.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">gk</span><span class="hspace">&nbsp;</span><span class="RktVal">36.0</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">el</span><span class="hspace">&nbsp;</span><span class="RktVal">10.6</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">'</span><span class="RktVal">gl</span><span class="hspace">&nbsp;</span><span class="RktVal">0.3</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p></div></p><p><div class="SIntrapara">All the Helper Functions:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-n</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">0.1</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">0.01</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">exp</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">0.1</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-m</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">2.5</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">0.1</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">exp</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">2.5</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">0.1</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-h</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">0.07</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">exp</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span>.0</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">20.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-n</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">0.125</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">exp</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span>.0</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">80.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-m</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">4.0</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">exp</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span>.0</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">18.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-h</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">exp</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">3.0</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">0.1</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">m-dot</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="hspace">&nbsp;</span><span class="RktSym">m</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-m</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktSym">m</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-m</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">m</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">n-dot</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="hspace">&nbsp;</span><span class="RktSym">n</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-n</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktSym">n</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-n</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">n</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">h-dot</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="hspace">&nbsp;</span><span class="RktSym">h</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-h</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktSym">h</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-h</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">h</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">m-infinity</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-m</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-m</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-m</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">n-infinity</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-n</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-n</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-n</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">h-infinity</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-h</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">alpha-h</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">beta-h</span><span class="hspace">&nbsp;</span><span class="RktSym">volt</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">between</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">let</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">lower</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">start-time</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">upper</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">stop-time</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">if-true</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">injection-current</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">if-false</span><span class="hspace">&nbsp;</span><span class="RktVal">0.0</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">if</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">and</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&gt;=</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">lower</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&lt;=</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">upper</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">if-true</span><span class="hspace">&nbsp;</span><span class="RktSym">if-false</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><h5>3.3.1.7<tt>&nbsp;</tt><a name="(part._.Updating_the_.Voltage)"></a>Updating the Voltage</h5><p>Look back at the <span class="math">\frac{dv}{dt}</span> formula for the <a href="#%28elem._iandf-eq%29" data-pltdoc="x">Integrate and Fire equation</a> and try to see the similarities. Although this function looks more complex it is still the basic Euler Method we used from the Integrate and Fire model. In fact, if you look at the source code for the <span class="RktSym">update</span><span class="RktMeta"></span> function you will see it is literally the one from the Integrate and Fire model.</p><p><div class="SIntrapara">Computing the Change of Voltage</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">dvdt</span><span class="hspace">&nbsp;</span><span class="RktSym">voltage-now</span><span class="hspace">&nbsp;</span><span class="RktSym">curr-in</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-m</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-n</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-h</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-parameters</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">let</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">ena</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-parameters</span><span class="RktVal">'</span><span class="RktVal">ena</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">gna</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-parameters</span><span class="RktVal">'</span><span class="RktVal">gna</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">ek</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-parameters</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">ek</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">gk</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-parameters</span><span class="RktVal">'</span><span class="RktVal">gk</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">el</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-parameters</span><span class="RktVal">'</span><span class="RktVal">el</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">gl</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-parameters</span><span class="RktVal">'</span><span class="RktVal">gl</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">curr-in</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktSym">gna</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">expt</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-m</span><span class="hspace">&nbsp;</span><span class="RktVal">3.0</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-h</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">voltage-now</span><span class="hspace">&nbsp;</span><span class="RktSym">ena</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktSym">gk</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">expt</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-n</span><span class="hspace">&nbsp;</span><span class="RktVal">4.0</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">voltage-now</span><span class="hspace">&nbsp;</span><span class="RktSym">ek</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktSym">gl</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">voltage-now</span><span class="hspace">&nbsp;</span><span class="RktSym">el</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Note that the looping construct we have been frequently using, the <span class="RktSym">for*/fold</span><span class="RktMeta"></span>, has an * in it. This means that later accumulator values can be declared dependent on ones that come before. This is not the case for <span class="RktSym">for/fold</span><span class="RktMeta"></span>, which declare the accumulators in parallel.</p></blockquote></blockquote></blockquote><p><div class="SIntrapara">Running the Model</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">run-hh-sim</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:max-time</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">max-time</span><span class="hspace">&nbsp;</span><span class="RktVal">60.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:max-iter</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">max-iter</span><span class="hspace">&nbsp;</span><span class="RktVal">50000</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">let</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">dt</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">dt</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">init-v</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">init-v</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">for*/fold</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">t</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hash-ref</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">init-t</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">hh-m</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">m-infinity</span><span class="hspace">&nbsp;</span><span class="RktSym">init-v</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">hh-n</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">n-infinity</span><span class="hspace">&nbsp;</span><span class="RktSym">init-v</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">hh-h</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">h-infinity</span><span class="hspace">&nbsp;</span><span class="RktSym">init-v</span><span class="RktPn">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktVal">0.0</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">v</span><span class="hspace">&nbsp;</span><span class="RktSym">init-v</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktSym">accum</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">(</span><span class="RktVal">)</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:result</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">reverse</span><span class="hspace">&nbsp;</span><span class="RktSym">accum</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">n</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">in-range</span><span class="hspace">&nbsp;</span><span class="RktSym">max-iter</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">#:break</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&gt;</span><span class="hspace">&nbsp;</span><span class="RktSym">t</span><span class="hspace">&nbsp;</span><span class="RktSym">max-time</span><span class="RktPn">)</span></td></tr><tr><td><span class="RktPn">(</span><span class="RktSym">values</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktSym">t</span><span class="hspace">&nbsp;</span><span class="RktSym">dt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">update</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-m</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">m-dot</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-m</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">dt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">update</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-n</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">n-dot</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-n</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">dt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">update</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-h</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">h-dot</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-h</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">dt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">between</span><span class="hspace">&nbsp;</span><span class="RktSym">t</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">update</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">dvdt</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span><span class="hspace">&nbsp;</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-m</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-n</span><span class="hspace">&nbsp;</span><span class="RktSym">hh-h</span><span class="hspace">&nbsp;</span><span class="RktSym">nps</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">dt</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">cons</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktSym">t</span><span class="hspace">&nbsp;</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktSym">v</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">accum</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><h5>3.3.1.7.1<tt>&nbsp;</tt><a name="(part._.Demonstrating_the_.Hodgkin-.Huxley_.Model)"></a>Demonstrating the Hodgkin-Huxley Model</h5><p><div class="SIntrapara">Running the HH Model</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">require</span><span class="hspace">&nbsp;</span><span class="RktVal">"./code/handh.rkt"</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">run</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">run-hh-sim</span><span class="hspace">&nbsp;</span><span class="RktSym">neuron-details</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lines</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">vector</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">first</span><span class="hspace">&nbsp;</span><span class="RktSym">run</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">second</span><span class="hspace">&nbsp;</span><span class="RktSym">run</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lines</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">vector</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">first</span><span class="hspace">&nbsp;</span><span class="RktSym">run</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">third</span><span class="hspace">&nbsp;</span><span class="RktSym">run</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_7.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><h5>3.3.1.8<tt>&nbsp;</tt><a name="(part._.Hodgkin_and_.Huxley_.Homework)"></a>Hodgkin and Huxley Homework</h5><p>I have given you code for this neuron in <a href="./../code/handh.rkt">handh.rkt</a>. Your homework will require you to modify this code and generate example plots of the output. You may find this harder than prior homeworks. Start sooner to see how it goes.</p><p>You will need to submit a scribble file. It should have a title, your name, and text describing each plot. You should provide at least two plots. You will need to adapt the model to fit this paper <span class="Autobibref">&nbsp;(<a href="#%28autobib._.Omar._.A..._.Hafez._and._.Allan._.Gottschalk.Altered._.Neuronal._.Excitability._in._a._.Hodgkin-.Huxley._.Model._.Incorporating._.Channelopathies._of._the._.Delayed._.Rectifier._.Potassium._.Channel.Journal._of._.Computational._.Neuroscience._48%2C._pp..._377--3862020http~3a%2F%2Fdx..doi..org%2F10..1007%2Fs10827-020-00766-1%29" class="AutobibLink" data-pltdoc="x">Hafez and Gottschalk</a> <a href="#%28autobib._.Omar._.A..._.Hafez._and._.Allan._.Gottschalk.Altered._.Neuronal._.Excitability._in._a._.Hodgkin-.Huxley._.Model._.Incorporating._.Channelopathies._of._the._.Delayed._.Rectifier._.Potassium._.Channel.Journal._of._.Computational._.Neuroscience._48%2C._pp..._377--3862020http~3a%2F%2Fdx..doi..org%2F10..1007%2Fs10827-020-00766-1%29" class="AutobibLink" data-pltdoc="x">2020</a>)</span>. These authors analyzed a series of inherited channelopathies (where ion channels are altered due to mutation) via simulation. In these conditions empirical measurements had been made on actual patients. The authors adapted the Hodgkin and Huxley model to permit them to explore the effects of such altered conductance on neuronal excitability. For this home work you will need to adapt the <span style="font-style: italic">n</span> equation to be as follows:</p><p><div class="math">\frac{dn}{dt} = \gamma_\tau (\gamma_\alpha \alpha_n (v&#8722;\Delta V)(1&#8722;n)&#8722; \gamma_\beta \beta_n (v&#8722; \Delta V)n)</div></p><p>Then you must select one of the sets of <span class="math">\gamma</span> from their <a href="https://link.springer.com/article/10.1007/s10827-020-00766-1/tables/1">Table 1</a> and plot the neuronal activity.</p><p>To start implement the new <span style="font-style: italic">n</span> channel and set all the <span class="math">\gamma</span>s to 1. This should work exactly like the old model. Generate a plot showing that it does. For the second plot generate the same plot with your altered <span class="math">\gamma</span> values.</p><p>To ease your transition to scribble I will accept it if you generate the plots as pngs in Dr Racket and then load them as images in your scribble document. For the submission give me the scrbl file, the image files, and the html output you generated (the .html file).</p><h5><a name="(part._ref~3ahandh)"></a>Hodgkin-Huxley References</h5><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Omar._.A..._.Hafez._and._.Allan._.Gottschalk.Altered._.Neuronal._.Excitability._in._a._.Hodgkin-.Huxley._.Model._.Incorporating._.Channelopathies._of._the._.Delayed._.Rectifier._.Potassium._.Channel.Journal._of._.Computational._.Neuroscience._48,._pp..._377--3862020http~3a//dx..doi..org/10..1007/s10827-020-00766-1)"></a><span class="Autobibentry">Omar A. Hafez and Allan Gottschalk. Altered Neuronal Excitability in a Hodgkin-Huxley Model Incorporating Channelopathies of the Delayed Rectifier Potassium Channel. <span style="font-style: italic">Journal of Computational Neuroscience</span> 48, pp. 377&ndash;386, 2020. <a href="http://dx.doi.org/10.1007/s10827-020-00766-1"><span class="url">http://dx.doi.org/10.1007/s10827-020-00766-1</span></a></span></span></p></td></tr></table></p><h4>3.4<tt>&nbsp;</tt><a name="(part._.A_.Digression_.Into_.Dynamics)"></a>A Digression Into Dynamics</h4><h5>3.4.1<tt>&nbsp;</tt><a name="(part._.Introduction)"></a>Introduction</h5><p>In the material on differential equations and their use in spiking neuron models we have been relying on the use of a differential equation that specifies the evolution of voltage (or some important parameter) as a function of time. So far, we have focused on using such models to create a simulation of the graph of an action potential, but those functions of time are statements about <span style="font-weight: bold">dynamics</span>. And the study of neural dynamics in its own right can give us important insights into brain and neuronal activity normally and as a consequence of disease. An interest in dynamical systems has been a part of computational neuroscience since the days of Hodgkin and Huxley, but it has become much more popular now. With the development of cheaper and more powerful computing capacities it is not feasible to simulate more complex models and models in higher dimensions. The latter is apt as there has been a contemporaneous development in multiple electrode recordings that yield high dimensional data. We can study the dynamics of one neuron, but we can also study the dynamics of a population of neurons. We can look at the time evolution of a vector of voltages that move across a high dimensional space.</p><p>In order to take advantage of our recent experiences with the nature and use of simple differential equations for producing simple, single neuron simulations of the integrate and fire variety I will introduce some of the basic terminology and ideas at play in neuronal dynamics. An excellent and concise summary of these ideas, one that I drew upon heavily for this treatment, is a set of notes by <span class="Autobibref">&nbsp;(<a href="#%28autobib._.David._.Terman.An._.Introduction._to._.Dynamical._.Systems._and._.Neuronal._.Dynamics.In._.Tutorials._in._.Mathematical._.Biosciences._.I~3a._.Mathematical._.Neuroscience%2C._pp..._21--68._.Springer._.Berlin._.Heidelberg2005https~3a%2F%2Fdoi..org%2F10..1007%2F978-3-540-31544-5_2%29" class="AutobibLink" data-pltdoc="x">Terman</a> <a href="#%28autobib._.David._.Terman.An._.Introduction._to._.Dynamical._.Systems._and._.Neuronal._.Dynamics.In._.Tutorials._in._.Mathematical._.Biosciences._.I~3a._.Mathematical._.Neuroscience%2C._pp..._21--68._.Springer._.Berlin._.Heidelberg2005https~3a%2F%2Fdoi..org%2F10..1007%2F978-3-540-31544-5_2%29" class="AutobibLink" data-pltdoc="x">2005</a>)</span>.</p><h5>3.4.2<tt>&nbsp;</tt><a name="(part._.Beginning_to_.Think_.Dynamically)"></a>Beginning to Think Dynamically</h5><p><div class="SIntrapara">Assume you have a derivative that is equal to <span class="math">f(x) = x - x^3</span>.</div><div class="SIntrapara"><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>This is a derivative by declaration, but not by the usual notation.</p></blockquote></blockquote></blockquote></div></p><p><div class="SIntrapara">Example:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">dx/dt=x-x^3</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">y</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">vector</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">expt</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktVal">3.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr></table></blockquote></div></p><p>What you want is some function of the variable <span style="font-style: italic">x</span> that will equal what you get when you take its derivative. For this function you could find the analytical solution by first separating the variables (<span style="font-style: italic">x</span>&rsquo;s on one side and <span style="font-style: italic">t</span>&rsquo;s on the other) and then using partial fractions before integrating. Should you do this by hand (or using a computer algebra system like <a href="https://www.wolframalpha.com/input?i=integral+of+1%2Fx">Wolfram Alpha</a>) you will find a complicated formula where it is not easy to intuit how the value of the function changes as you evolve <span style="font-style: italic">t</span>.</p><p>The dynamical systems approach is the same idea as we used to implement our spiking neuron models. We think of <span class="math">x</span> as itself a function: <span class="math">x(t)</span>. Then as <span style="font-style: italic">t</span> changes we will also change <span class="math">x</span>.</p><h5>3.4.3<tt>&nbsp;</tt><a name="(part._.Fixed_.Points)"></a>Fixed Points</h5><p>Give me a place to stand, and I shall move the world. &#8212;<wbr></wbr>Archimedes</p><p>Fixed points are the points in a function where it no longer changes. It becomes <span style="font-style: italic">fixed</span>. We have already used fixed points, at least informally, to determine certain starting parameters for our Hodgkin &amp; Huxley model. We assumed that the system evolved to some point in time where the derivative of a function with time was zero. Then we could infer or compute the form that our parameter (e.g. the &#593; or &#946;) took. This is the determinant of a fixed point: a point where the derivative is zero. If the derivative is describing how your function changes over time, then when the derivative is zero the function does not change and so that point is fixed.</p><p>You can learn a lot about a system by looking at its <span style="font-weight: bold">fixed points</span>.</p><h5>3.4.3.1<tt>&nbsp;</tt><a name="(part._.Class_.Exercise)"></a>Class Exercise</h5><p>For the function above what are the fixed points and are they <span style="font-weight: bold">stable</span> or <span style="font-weight: bold">unstable</span>? Solve your equation for all the values of <span class="math">x</span> that make the derivative zero. That will give you the fixed points. What do you think is meant by the term <span style="font-style: italic">stability</span>?</p><p>Informally, you can assess stability by looking nearby the fixed points to see if they are pushed toward or away from the fixed point. Which of these three are stable or unstable?</p><p><div class="SIntrapara">Stability From Vector Fields:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">dx/dt=x-x^3</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">y</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">vector</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">expt</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktVal">3.0</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">vector-field</span><span class="hspace">&nbsp;</span><span class="RktSym">dx/dt=x-x^3</span><span class="hspace">&nbsp;&nbsp;&nbsp;</span><span class="RktVal"><span class="nobreak">-0</span>.2</span><span class="hspace">&nbsp;</span><span class="RktVal">0.2</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-0</span>.2</span><span class="hspace">&nbsp;</span><span class="RktVal">0.2</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_8.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><p><div class="SIntrapara">A more realistic 2D example:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">vector-field</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&#955;</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">y</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">vector</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">y</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="hspace">&nbsp;</span><span class="RktSym">y</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-2</span></span><span class="hspace">&nbsp;</span><span class="RktVal">2</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-2</span></span><span class="hspace">&nbsp;</span><span class="RktVal">2</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_9.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><h5>3.4.3.2<tt>&nbsp;</tt><a name="(part._.Extended_.Class_.Activity)"></a>Extended Class Activity</h5><p>Another example taken from <span class="Autobibref">&nbsp;(<a href="#%28autobib._.David._.Terman.An._.Introduction._to._.Dynamical._.Systems._and._.Neuronal._.Dynamics.In._.Tutorials._in._.Mathematical._.Biosciences._.I~3a._.Mathematical._.Neuroscience%2C._pp..._21--68._.Springer._.Berlin._.Heidelberg2005https~3a%2F%2Fdoi..org%2F10..1007%2F978-3-540-31544-5_2%29" class="AutobibLink" data-pltdoc="x">Terman</a> <a href="#%28autobib._.David._.Terman.An._.Introduction._to._.Dynamical._.Systems._and._.Neuronal._.Dynamics.In._.Tutorials._in._.Mathematical._.Biosciences._.I~3a._.Mathematical._.Neuroscience%2C._pp..._21--68._.Springer._.Berlin._.Heidelberg2005https~3a%2F%2Fdoi..org%2F10..1007%2F978-3-540-31544-5_2%29" class="AutobibLink" data-pltdoc="x">2005</a>)</span> is <div class="math">x&rsquo; = \lambda + x^2.</div> We can use this equation to demonstrate the idea of a <span style="font-weight: bold"><span style="font-style: italic">bifurcation</span></span>.</p><p>A bifurcation is the description of a point in the plot of a function where some aspect of function behavior changes in a way we regard as important. The trick here is to change one&rsquo;s perspective. We have been considering our functions as functions of time, which they are, but in the case of this function we also have a parameter expressed by the &#955;. This too is free to change, just like our time variable. Consider your integrate and fire neuron. For some injections of currents nothing happens. It just reaches and holds a steady value. But as we increase the level we reach a point where we get repetitive firing. It /oscillates/. Or recall how the behavior of your model changed when you adjusted the membrane time constant: &#964;. You could now think about how the entire <span class="math">v(t)</span> function changes as you change &#964;. If there was some abrupt shift in function behavior you would have a bifurcation. To make this concrete consider the fixed points and stability of this equation.</p><p><span style="font-style: italic">What are the fixed points of this equation?</span></p><p><span style="font-style: italic">Are they stable?</span></p><p>If you have had some calculus you may remember that you could find the maxima or minima of a function by the location of the points where the derivative was zero. You were either on top of a mountain or in the depths of a valley.</p><p><div class="SIntrapara">Example:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">hc-append</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">plot-pict</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">function</span><span class="hspace">&nbsp;</span><span class="RktSym">sqr</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-2</span></span><span class="hspace">&nbsp;</span><span class="RktVal">2</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">plot-pict</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">function</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lambda</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">sqr</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-2</span></span><span class="hspace">&nbsp;</span><span class="RktVal">2</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_10.png" alt="image" width="806" height="406"/></p></td></tr></table></blockquote></div></p><p>Imagine that these two plots are plots of the derivative. To see how the derivative is changing, you can visually take the derivative of this derivative by imagining the tangent lines, that is the approximations to the slope that we used before. Trace these imaginary lines around the graph from left to right and observe that in one case they go from minus to positive and in the other from positive to minus. Which is which?</p><p>More precisely you can calculate the derivative of your derivative and find its value at the fixed point. If it is greater than zero you are unstable. Less than zero and you are stable. If it exactly equals zero the behavior is not clear.</p><p>For this classroom activity find the fixed points for this equation as a function of lambda. Note the values for lambda might equal 0 or lambda could be greater or less than zero. Often it is more helpful to consider regions of values than just some odd assortment of numbers you pull from a hat. Consider your fixed points as functions of lambda.</p><p>Plot the values of x for all its fixed points as a function of lambda. What does  the look like. What goes on the x axis? Y axis?</p><p><div class="SIntrapara">Example:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hopf-fixed-example</span><span class="hspace">&nbsp;</span><span class="RktSym">l</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktSym">l</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">sqrt</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span></span><span class="hspace">&nbsp;</span><span class="RktSym">l</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span><span class="hspace">&nbsp;</span><span class="RktSym">l</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">sqrt</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span></span><span class="hspace">&nbsp;</span><span class="RktSym">l</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hopf-fixed-example-deriv</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal">2</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">ls-to-plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">range</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-5</span></span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-0</span>.05</span><span class="hspace">&nbsp;</span><span class="RktVal">0.05</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">fixed-points</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">append*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">hopf-fixed-example</span><span class="hspace">&nbsp;</span><span class="RktSym">ls-to-plot</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define-values</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">r</span><span class="hspace">&nbsp;</span><span class="RktSym">g</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">partition</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">lambda</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">positive?</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">hopf-fixed-example-deriv</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">second</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">fixed-points</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">list</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">point-label</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">vector</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">"bifurcation point"</span><span class="hspace">&nbsp;</span><span class="RktPn">#:anchor</span><span class="hspace">&nbsp;</span><span class="RktVal">'</span><span class="RktVal">right</span><span class="RktPn">)</span><span class="RktPn">(</span><span class="RktSym">points</span><span class="hspace">&nbsp;</span><span class="RktSym">r</span><span class="hspace">&nbsp;</span><span class="RktPn">#:color</span><span class="hspace">&nbsp;</span><span class="RktVal">"red"</span><span class="hspace">&nbsp;</span><span class="RktPn">#:label</span><span class="hspace">&nbsp;</span><span class="RktVal">"unstable"</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">points</span><span class="hspace">&nbsp;</span><span class="RktSym">g</span><span class="hspace">&nbsp;</span><span class="RktPn">#:color</span><span class="hspace">&nbsp;</span><span class="RktVal">"green"</span><span class="hspace">&nbsp;</span><span class="RktPn">#:label</span><span class="hspace">&nbsp;</span><span class="RktVal">"stable"</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">#:x-label</span><span class="hspace">&nbsp;</span><span class="RktVal">"lambda"</span><span class="hspace">&nbsp;</span><span class="RktPn">#:y-label</span><span class="hspace">&nbsp;</span><span class="RktVal">"fixed-points"</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_11.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><p>This is a so-called <span style="font-weight: bold"><span style="font-style: italic">saddle node</span></span>.</p><p>Now try to do the same for the function: <span class="math">x&rsquo; = \lambda~x - x^2.</span>
Find the fixed points for the <span style="font-weight: bold"><span style="font-style: italic">transcritical</span></span> bifurcation and plot the stability. It is probably easier if just use pen and paper (or ipad). Use your calculus to find the fixed points and the derivatives for stability.</p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>It is important to recognize that the &#955; here is not the same as for the &#955; calculus. There are only so many symbols and the mathematicians tend to recycle and re-use.</p></blockquote></blockquote></blockquote><p>Then are many more types of bifurcations that are classified based on these types of graphs.
Some, such as the "Hopf" can only be observed in higher dimensions where the ideas of a <span style="font-weight: bold"><span style="font-style: italic">phase</span></span> space and plot become common terms.</p><h5>3.4.3.3<tt>&nbsp;</tt><a name="(part._.From_.Lines_to_.Planes)"></a>From Lines to Planes</h5><p><div class="SIntrapara">The extrapolation we want to make from the 1D case is that the geometry of our "particle" is  moving in a plane. It turns out for all ordinary differential equations </div><div class="SIntrapara"><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Ordinary differential equations have only a single independent variable.</p></blockquote></blockquote></blockquote></div><div class="SIntrapara"> no matter how high their dimensionality this spatial metaphor will be useful.</div></p><p>For two dimension (dependent variables <span class="math">x</span> and <span class="math">y</span>) and two functions of those two variables, we need to consider the derivatives of each. How x changes with time (<span class="math">x&rsquo;</span>) is a function of both x and y. The same for how y changes (<span class="math">y&rsquo;</span>). For this two dimensional case the phase *plane* is the x - y plane.</p><p>At the initial time (t_0) the "particle" is somewhere (x(t_0), y(t_0)). As t grows the position of the particle changes. It move in the x - y plane. How do we know where it goes next? We have the derivatives that describe how it changes over time, and we use those just as we did to propragate velocity in the Hodgkin-Huxley model.</p><p><div class="SIntrapara">One approach:
</div><div class="SIntrapara"><ol><li><p>Determine the fixed points.</p></li><li><p>Draw the <span style="font-style: italic">nullclines</span>.</p></li></ol></div></p><p>A nullcline is the line in your phase space when one of the derivatives of your dependent variables is zero. For example, if you had a system of equations like:</p><p><div class="math">\begin{align*}
   x&rsquo; &amp;= y - x^2 + x \\
   y&rsquo; &amp;= x - y
   \end{align*}</div></p><p>Consider when <span class="math">x</span> is zero. That would mean when <span class="math">y = x^2 - x</span>. Now you determine what the y nullcline is for this system of equations. Then plot them with Racket&rsquo;s plot functions. The look at the graph and determine where the fixed points.</p><h5>3.4.3.4<tt>&nbsp;</tt><a name="(part._.Dealing_with_the_.Non-linear)"></a>Dealing with the Non-linear</h5><p>Make it linear. Locally everything is linear. To find out the stability of a complex system where the graphical depiction doesn&rsquo;t give you all the answers you take your non-linear system and linearize in the neighborhood of the fixed points. This is very similar to what we have already done, but disguises this process with new terminology and a more complex notation for tracking the multiple variables we need to reference. All I will do here is introduce some of the words, but their application will have to wait.</p><p><div class="SIntrapara">Continuous functions (those without breaks and jumps) can be decomposed into an expansion of terms that multiply the powers of their independent variable by coefficients. When you progress through the algebra you end up with an expansion that involves derivatives of increasing order and factorials. For most of the functions that are of biological or psychological relevance the magnitude of the terms decreases quickly enough that all the orders beyond x or <span class="math">x^2</span> can usually be ignored. Such expansions are called a <span style="font-weight: bold"><span style="font-style: italic">Taylor</span></span> series. When applied to a system of equations you get a matrix, which we will look at more shortly in the section on neural networks. The <span style="font-weight: bold"><span style="font-style: italic">Jacobian</span></span> is really just a bunch of second derivatives that we use like above to determine the stability of regions in our phase space via looking at the <span style="font-weight: bold"><span style="font-style: italic">eigenvalues</span></span> of this phase space. </div><div class="SIntrapara"><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>The terms here are so you know what to look up if you want to learn more on your own. The book on Neuronal Dyanmics referenced earlier and the Tutorial book with Terman&rsquo;s chapter are both useful sources.</p></blockquote></blockquote></blockquote></div></p><h5>3.4.4<tt>&nbsp;</tt><a name="(part._.Neuronal_.Dyanmics_.Applied_to_.Spiking_.Neuron_.Models)"></a>Neuronal Dyanmics Applied to Spiking Neuron Models</h5><p>The Hodgkin and Huxley model is quite complex. There are simpler versions of spiking neuron models that still yield much of the important features of the H-H model. Having fewer variables and fewer equations they are more tractable mathematically and more understandable via inspection. Having a spike generation process they are more applicable to real neurons than the even simpler integrate and fire model.</p><p>A popular reduced model is the <a href="https://en.wikipedia.org/wiki/Morris%E2%80%93Lecar_model">Morris Lecar</a>. It is an idealized version of a neuron with two ionic conductances and one leak current. Only one of the ionic currents changes slowly enough to matter (the other is instantaneous). There can also be an applied current. By varying the, in this version of the terminology, &#966; value one can change the behavior of the neuron and use the above methodologies for <a href="https://web.archive.org/web/20120402093059/http://pegasus.medsci.tokushima-u.ac.jp/~tsumoto/work/nolta2002_4181.pdf">studying how that affects neuronal spiking dynamics</a>. As a preview demonstration consider these two plots.</p><p><div class="SIntrapara">Examples:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">require</span><span class="hspace">&nbsp;</span><span class="RktVal">"./code/morris-lecar.rkt"</span><span class="RktPn">)</span></td></tr><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">hc-append</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">try-a-new-phi</span><span class="hspace">&nbsp;</span><span class="RktVal">0.03</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">poor-mans-phase-plot</span><span class="hspace">&nbsp;</span><span class="RktVal">0.03</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_12.png" alt="image" width="806" height="406"/></p></td></tr></table></blockquote></div></p><h5><a name="(part._ref~3adyn)"></a>Dynamics Bibliography</h5><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td><p><span class="Autobibtarget"><a name="(autobib._.David._.Terman.An._.Introduction._to._.Dynamical._.Systems._and._.Neuronal._.Dynamics.In._.Tutorials._in._.Mathematical._.Biosciences._.I~3a._.Mathematical._.Neuroscience,._pp..._21--68._.Springer._.Berlin._.Heidelberg2005https~3a//doi..org/10..1007/978-3-540-31544-5_2)"></a><span class="Autobibentry">David Terman. An Introduction to Dynamical Systems and Neuronal Dynamics. In <span style="font-style: italic">Tutorials in Mathematical Biosciences I: Mathematical Neuroscience</span>, pp. 21&ndash;68 Springer Berlin Heidelberg, 2005. <a href="https://doi.org/10.1007/978-3-540-31544-5_2"><span class="url">https://doi.org/10.1007/978-3-540-31544-5_2</span></a></span></span></p></td></tr></table></p><h4>3.5<tt>&nbsp;</tt><a name="(part._.Morris_.Lecar_.Model)"></a>Morris Lecar Model</h4><h5>3.5.1<tt>&nbsp;</tt><a name="(part._.Work_in_.Progress)"></a>Work in Progress</h5><p>This section is a work in progress.</p><p>I want to discuss the benefits of simplifying models as well as show the use of direction plots and phase space diagrams.</p><p>At the moment I have working racket code, but I have not written up this section yet. I am adding this section now (Oct 2022) to have a way to make the <a href="./../code/morris-lecar.rkt">code</a> discoverable.</p><h3>4<tt>&nbsp;</tt><a name="(part._.Neural._.Networks)"></a>Neural Networks</h3><h4>4.1<tt>&nbsp;</tt><a name="(part._.Introduction_to_.Linear_.Algebra_and_.Neural_.Networks)"></a>Introduction to Linear Algebra and Neural Networks</h4><h5>4.1.1<tt>&nbsp;</tt><a name="(part._.Linear_.Algebra_.Goals)"></a>Linear Algebra Goals</h5><p><div class="SIntrapara">Our goal for the next few lessons is to come to understand
</div><div class="SIntrapara"><ul><li><p>What is a neural network?</p></li><li><p>What mathematics are needed to build a neural network?</p></li><li><p>How can neural networks help us understand cognition?</p></li></ul></div></p><p>As a first illustration of some of the key ideas we will execute a simple cellular automata rule. What I hope to emphasize through this exercise is that whenever you can get the computer to do a repetitive task do so. It will do it much better than you. And even if it takes you days to get the program right for many task you will quickly save the time in the long run. Second, we are using a simple rule (as you will shortly see). But even though the rule is local it yields impressive global structure. And very slight tweaks in this local rule can lead to large macroscopic changes. While the variation in our rule is very limited the array of behaviors we can observe is vast. <span class="refelem"><span class="refcolumn"><span class="refcontent">Match these features to facts about neurons. Extend them to what you believe will be their application in neural networks.</span></span></span></p><h5>4.1.2<tt>&nbsp;</tt><a name="(part._.Drawing_.Cellular_.Automata)"></a>Drawing Cellular Automata</h5><p>This activity has several stages. For the first stage make sure you can load the file <a href="&quot;./../code/ca.rkt&quot;"></a> into Dr Racket and that it runs.</p><p>Next, pick a number between 0 and 255 inclusive. In your interactive window use the function <span class="stt">rule-tester</span> to generate the input-output pairing for your rule like so.</p><p><div class="SIntrapara">Testing Rule 22</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">rule-tester</span><span class="hspace">&nbsp;</span><span class="RktVal">22</span><span class="hspace">&nbsp;</span><span class="RktSym">test-set</span><span class="RktPn">)</span></td></tr><tr><td><table cellspacing="0" cellpadding="0"><tr><td><p><span class="RktOut">in (w w w) out w</span></p></td></tr><tr><td><p><span class="RktOut">in (w w b) out b</span></p></td></tr><tr><td><p><span class="RktOut">in (w b w) out b</span></p></td></tr><tr><td><p><span class="RktOut">in (w b b) out w</span></p></td></tr><tr><td><p><span class="RktOut">in (b w w) out b</span></p></td></tr><tr><td><p><span class="RktOut">in (b w b) out w</span></p></td></tr><tr><td><p><span class="RktOut">in (b b w) out w</span></p></td></tr><tr><td><p><span class="RktOut">in (b b b) out w</span></p></td></tr></table></td></tr></table></blockquote></div></p><p>Use your rule and a piece of graph paper to implement your rule.</p><p>Color a single black square in the middle of the top row. Then moving down one row and working left to right implement your rule by coloring in the appropriate square.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="grid.png" alt="" width="359" height="240"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3agrid-automata))" x-target-lift="Figure"></a>Figure&nbsp;5: </span>Nearest Neighbors in the Grid</span></p></blockquote><p>For example, if the boxes 1, 2, and 3 were &rsquo;w, &rsquo;w, and &rsquo;b I could color the square with the question mark black. Then I would move one to the right and square 2 would become my new number 1 and so on.</p><p>Complete several rows following your rule.</p><p>What you have probably noticed is that this is tedious and mistake prone, but your rule is a good example of a function. A function can be conceived as a set of pairs. The first element of the pair is the input, and the second element of the pair is the output. Functions require that each input element be unique. Implementing your rule  makes you the metaphorical neuron deciding whether or not to fire (color the square black) based on the input you receive from neighboring neurons.</p><p>Having learned how tedious and error prone this process explore some of the other rules using the functions in <span class="stt">ca.rkt</span>. The simplest method is to use the function <span class="stt">d-r-a &lt;some-rule-number&gt;</span>. You can adjust the size and scale with various optional arguments and even print it to a file if you find one you like. Here is one of my favorites as a demonstration.</p><p><div class="SIntrapara">Rule 110</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">d-r-a</span><span class="hspace">&nbsp;</span><span class="RktVal">110</span><span class="hspace">&nbsp;</span><span class="RktPn">#:num-rows</span><span class="hspace">&nbsp;</span><span class="RktVal">100</span><span class="hspace">&nbsp;</span><span class="RktPn">#:num-cols</span><span class="hspace">&nbsp;</span><span class="RktVal">100</span><span class="hspace">&nbsp;</span><span class="RktPn">#:scale</span><span class="hspace">&nbsp;</span><span class="RktVal">3</span><span class="RktPn">)</span></td></tr><tr><td><p><img src="pict_13.png" alt="image" width="300" height="303"/></p></td></tr></table></blockquote></div></p><p><div class="SIntrapara">What are the lessons learned from this exercise?
</div><div class="SIntrapara"><ol><li><p>Repetitive actions are hard. We (humans) make mistakes following even simple rules for a large number of repeated steps. Better to let the computer do it since that is where its strengths lie.</p></li><li><p>Complex global patterns can emerge from local actions. Each neuron is only responding to its immediate right and left yet global structures emerge.</p></li><li><p>These characteristics seem similar to brain activity. Each neuron in the brain is just one of many. Whether a neuron spikes or not is a consequence of its own state and its inputs (like the neighbors in the grid example).</p></li><li><p>From each neuron making a local computation, global patterns of complex activity can emerge.</p></li><li><p>Maybe by programming something similar to this system we can get insights into brain activity.</p></li></ol></div></p><h5>4.1.2.1<tt>&nbsp;</tt><a name="(part._.Comments_on_the_programmatic_implementation)"></a>Comments on the programmatic implementation</h5><p>The code in <span class="stt">ca.rkt</span> involves a lot of looping. I used <span style="font-style: italic">for</span> loops extensively, though sometimes these were <span class="stt">for/fold</span> variants. We need to inch along the columns and down the rows. The plotting used the built in functionality of <span style="font-weight: bold">racket</span> for generating pictures as output.</p><p>The potentially more tricky part was going from a number (in decimal) to a binary representation that had the right number of places occupied. I ended going back and forth between strings and lists to get what I wanted. This was undoubtedly a kludge, but there is a slogan to first get it working, and then make it better. Trying to be too perfect and too elegant can cost you time in the long run. It is often easier to revise a functioning program then write one from the start.</p><p>Initially I did not have all the testing code, because I was adapting code I had written in the past. However, when things did not work it turned out I went faster by slowing down and writing code that allowed me to inspect the state of my various variables, and individually try out the small functions on test input.</p><h5>4.1.3<tt>&nbsp;</tt><a name="(part._.More_.Lessons_from_.Cellular_.Automata)"></a>More Lessons from Cellular Automata</h5><p>Cellular automata demonstrate some basic lessons that we will make use of when thinking about neural networks. One of these points is that there may be simple representations for complex entities. If we can find the right language for representation we may get concision and repeatability as by-products. This is demonstrated by the <a href="https://plato.stanford.edu/entries/cellular-automata/supplement.html">naming convention for the rules of cellular automata</a>.</p><p>In emphasizing that local decisions can produce interesting global effects it may be interesting to examine other similar uses of the cellular automata idea. One famous and visually pleasing one is the <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Game of Life</a>.</p><p>The analogy of automata to simple neurons may be deeper than at first it appears. Some very famous thinkers connected the two. One of the most brilliant people of all time, John von Neumann, was working on a book about automata and the brain at the time of his death. I have linked to a commentary in case you are interested in reading further see <a href="http://www.ams.org/bull/1958-64-03/S0002-9904-1958-10214-1/S0002-9904-1958-10214-1.pdf">Claude Shannon (pdf)</a> as well as to a pdf <a href="https://complexityexplorer.s3.amazonaws.com/supplemental_materials/5.6+Artificial+Life/The+Computer+and+The+Brain_text.pdf">copy</a> of the book: <a href="https://ocul-wtl.primo.exlibrisgroup.com/permalink/01OCUL_WTL/vk29fk/alma994863683505162">The Computer and the Brain</a>).</p><p>A contemporary mathematician and the inventor of the Mathematica software system also believes that cellular automata may be a theory of everything. See what Stephen Wolfram <a href="http://www.wolframscience.com">thinks</a>.</p><h4>4.2<tt>&nbsp;</tt><a name="(part._.The_.Math_.That_.Underlies_.Neural_.Networks_)"></a>The Math That Underlies Neural Networks?</h4><h5>4.2.1<tt>&nbsp;</tt><a name="(part._.Linear_.Algebra)"></a>Linear Algebra</h5><p>The math at the heart of neural networks and their computer implementation is <span style="font-style: italic"><span style="font-weight: bold">linear algebra</span></span>. For us, the section of linear algebra we are going to need is mostly limited to vectors, matrices and how to add and multiply them.</p><h5>4.2.1.1<tt>&nbsp;</tt><a name="(part._.Important_.Objects_and_.Operations)"></a>Important Objects and Operations</h5><ol><li><p>Vectors</p></li><li><p>Matrices</p></li><li><p>Scalars</p></li><li><p>Addition</p></li><li><p>Multiplication (scalar and matrix)</p></li><li><p>Transposition</p></li><li><p>Inverse</p></li></ol><h5>4.2.1.1.1<tt>&nbsp;</tt><a name="(part._.Adding_.Matrices)"></a>Adding Matrices</h5><p>To gain some hands on familiarity with the manipulation of matrices and vectors we will try to do some hand and programming exercises for some of the fundamental operations of addition and multiplication. We will also thereby learn that some of the rules we learned for numbers (such as a * b = b * a) do not always apply in other mathematical realms.</p><p>There are in fact many ways to think about what a vector is.</p><p>It can be thought of as a column (or row of numbers).
More abstractly it is an object (arrow) with magnitude and direction.
Most abstractly it is anything that obeys the requirements of a vector space.</p><p>For particular circumstances one or another of the different definitions may serve our purposes better. In application to neural networks we often just use the first definition, a column of numbers, but the second can be more helpful for developing our geometric intuitions about what various learning rules are doing and how they do it.</p><p>Similarly, we often just consider a matrix as a collection of vectors or as a rectangular (2-D) collection of numbers.</p><h5>4.2.1.1.2<tt>&nbsp;</tt><a name="(part._.Activity)"></a>Activity</h5><p>Look up how racket handles <a href="https://docs.racket-lang.org/math/matrices.html">matrices and vectors</a>. Here is a very simple <a href="./../code/la-demo.rkt">file</a> to try and get started.</p><p><span style="font-weight: bold">Important</span>: vectors are a special datatype in Racket, and the vector type is probably not what you want to be using. Look for matrices and linear algebra.</p><p>Make two arrays and make them the same size<span class="refelem"><span class="refcolumn"><span class="refcontent">What is the <span style="font-style: italic">size</span> of a matrix?</span></span></span>.</p><p>Add them together in both orders (A + B and B + A). How does one add an array that itself has numerous different numbers?</p><p>Then do the same for multiplication. Note that there are particular requirements for the sizes of matrices in order that it is possible to multiply them in both directions. What is that rule?</p><p>What is the name for the property of having A*B = B*A?</p><h5>4.2.1.2<tt>&nbsp;</tt><a name="(part._.Common_.Notational_.Conventions_for_.Vectors_and_.Matrices)"></a>Common Notational Conventions for Vectors and Matrices</h5><p>Vectors tend to be notated as <span style="font-style: italic">lower case</span> letters, often in bold, such
as <span class="math">\mathbf{a}</span>. They are also occasionally represented with little
arrows on top such as <span class="math">\overrightarrow{\textbf{a}}</span>.</p><p>Matrices tend to be notated as <span style="font-style: italic">upper case</span> letters, typically in bold,
such as <span class="math">\mathbf{M}</span>.</p><p>Good things to know: what is an <span style="font-style: italic">inner product</span>? How do you compute it in racket?</p><h5>4.2.2<tt>&nbsp;</tt><a name="(part._.What_is_a_.Neural_.Network_)"></a>What is a Neural Network?</h5><p>What is a Neural Network? It is a brain inspired computational approach
in which "neurons" compute functions of their inputs and pass on a
<span style="font-style: italic">weighted</span> proportion to the next neuron in the chain.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="nn.png" alt="" width="400" height="136"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig-nn))" x-target-lift="Figure"></a>Figure&nbsp;6: </span>simple schematic of the basics of a neural network. This is an image for a single neuron. The input has three elements and each of these connects to the same neuron ("node 1"). The activity at those nodes is filtered by the weights, which are specific for each of the inputs. These three processed inputs are combined to generate the output from this neuron. For multiple layers this output becomes an input for the next neuron along the chain.</span></p></blockquote><h5>4.2.2.1<tt>&nbsp;</tt><a name="(part._.Non-linearities)"></a>Non-linearities</h5><p>The spiking of a biological neuron is non-linear. You saw this in both the integrate and fire and Hodgkin and Huxley models you programmed. The lines on those plots you created are not, for the most part, straight. Perhaps the simplest way to incorporate a non-linearity into our artificial neuron is to give it a threshold, like we did for the integrate and fire model. When activity exceeds the threshold (which we will usually designate with a capital Greek Theta <span class="math">\Theta</span> then the neuron is set to 1 and if it is not firing it is set to 0 (like the "w" &#8594; 0; "b" &#8594; 1 mapping we used for the cellular automata).</p><p><div class="math">\begin{equation}
\mbox{if } I_1 \times w_{1,1} + I_2 \times w_{2,1} + I_3 \times w_{3,1} &gt; \Theta \mbox{ then } Output = 1
\end{equation}</div></p><p>What this equation shows is that Inputs (the <span class="math">I</span>s) are passed to a neuron. Those inputs have something like a synapse. That is designated by the w&rsquo;s. Those weights are how tightly the input and internal activity of our artificial neuron is coupled. The reason for all the subscripts is to try and help you see the similarity between this equation and the inner product and matrix multiplication rules you just worked on programming. The activity of the neuron is a sort of internal state, and then, based on the comparison of that activity to the threshold, you can envision the neuron spiking or not, meaning it has value 1 or 0. Mathematically, the weighted sum is fed into a threshold function that compares the value to a threshold <span class="math">\Theta</span>, and passes on the value 1 if it is greater than the threshold and 0 (sometimes <span class="math">-1</span> rather than zero is chosen for the inactive state because there are certain computational conveniences in doing so).</p><p>To prepare you for the next steps in writing a simple perceptron (the earliest form of artificial neural network), you should try to answer the following questions.</p><p><div class="SIntrapara">Questions:
</div><div class="SIntrapara"><ol><li><p>What, geometrically speaking, is a plane?</p></li><li><p>What is a hyperplane?</p></li><li><p>What is linearly separability and how does that relate to planes and
hyperplanes?</p></li></ol></div></p><p>One of our first efforts will be to code a <span style="font-style: italic">perceptron</span> to solve the XOR problem. In order for this to happen you need to know a bit about <span style="font-style: italic">Boolean</span> functions and what an XOR problem actually is.</p><p><span style="font-weight: bold">Examples of Boolean Functions and How They Map onto our Neural Network Intuitions</span></p><p>The "AND" Operation/Function</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="pict_14.png" alt="image" width="400" height="400"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3aand))" x-target-lift="Figure"></a>Figure&nbsp;7: </span>The <span style="font-style: italic">and</span> operation is true when both its inputs are true.</span></p></blockquote><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="pict_15.png" alt="image" width="400" height="400"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3aor))" x-target-lift="Figure"></a>Figure&nbsp;8: </span>The <span style="font-style: italic">or</span> operation is true if either or both of its inputs are true.</span></p></blockquote><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="pict_16.png" alt="image" width="400" height="400"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3axor))" x-target-lift="Figure"></a>Figure&nbsp;9: </span>The <span style="font-style: italic">xor</span> is true when one or the other, but not both of the inputs are true. It is exclusively an or function.</span></p></blockquote><p>This short <a href="https://media.nature.com/m685/nature-assets/nbt/journal/v26/n2/images/nbt1386-F1.gif">article</a> provides a nice example of linear separability and some basics of what a neural network is.</p><h5>4.2.2.1.1<tt>&nbsp;</tt><a name="(part._.Exercise_.X.O.R)"></a>Exercise XOR</h5><p>Using only <span style="font-style: italic">not</span>, <span style="font-style: italic">and</span>, and <span style="font-style: italic">or</span> operations draw the diagram that allows you to compute in two steps the <span style="font-style: italic">xor</span> operation. You will need this to code it up as a perceptron.</p><h5>4.2.2.2<tt>&nbsp;</tt><a name="(part._.Connections)"></a>Connections</h5><p>Can neural networks encode logic? Is the processing zeros and ones enough to capture the richness of human intellectual activity?</p><p>There is a long tradition of representing human thought as the consequence of some sort of calculation of two values (true or false). If you have two values you can swap out 1&rsquo;s and 0&rsquo;s for the true and false in your calculation. They even seem to obey similar laws. If you the conjunction (AND) of two true things it is only true when both are true. If you take T = 1, then T &#8743; T is the same as <span class="math">1~\times~1</span>.</p><p>We will next build up a simple threshold neural unit and try to calculate some of these truth functions with our neuron. We will build simple neurons for truth tables (like those that follow), and string them together into an argument. Then we can feed values of T and F into our network and let it calculate the XOR problem.</p><h5>4.2.2.3<tt>&nbsp;</tt><a name="(part._.Boolean_.Logic)"></a>Boolean Logic</h5><p>George Boole, Author of the <span style="font-style: italic">Laws of Thought</span></p><ul><li><p>Read the <a href="https://archive.org/details/investigationofl00boolrich">book</a> on Archive.org</p></li><li><p>Read about <a href="https://plato.stanford.edu/entries/boole/#LifWor">George Boole</a></p></li></ul><h5>4.2.2.4<tt>&nbsp;</tt><a name="(part.__.First_.Order_.Logic_-_.Truth_.Tables)"></a> First Order Logic - Truth Tables</h5><p><span style="font-weight: bold">Or</span></p><p><table cellspacing="0" cellpadding="0"><tr><td><p><span style="font-weight: bold">Pr A</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Pr B</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Or</span></p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr></table></p><p><span style="font-weight: bold">And</span></p><p><table cellspacing="0" cellpadding="0"><tr><td><p><span style="font-weight: bold">Pr A</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Pr B</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">AND</span></p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr></table></p><p><span style="font-weight: bold">Nand</span></p><p><table cellspacing="0" cellpadding="0"><tr><td><p><span style="font-weight: bold">Pr A</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Pr B</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">NAND</span></p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr></table></p><h4>4.3<tt>&nbsp;</tt><a name="(part._.Perceptrons)"></a>Perceptrons</h4><h5>4.3.1<tt>&nbsp;</tt><a name="(part._.Goals)"></a>Goals</h5><p>The goal for this file is to share the idea of a perceptron, the mathematical formula for updating one, and initiate the process of coding a simple implementation that we will adapt to the delta rule.</p><h5>4.3.2<tt>&nbsp;</tt><a name="(part._.Perceptron_.History_and_.Implementation)"></a>Perceptron History and Implementation</h5><p>The perceptron was the invention of a psychologist, <a href="http://dspace.library.cornell.edu/bitstream/1813/18965/2/Rosenblatt_Frank_1971.pdf">Frank Rosenblatt</a>.  He was not a computer scientist. Though he obviously had a bit of the mathematician in him.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="Mark_I_perceptron.jpeg" alt=""/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3amark.I))" x-target-lift="Figure"></a>Figure&nbsp;10: </span>The Perceptron Mark I</span></p></blockquote><p>Details to be found on the <a href="https://en.wikipedia.org/wiki/Perceptron">wikipedia page</a>.</p><p>Those interested in some interesting background reading could consult his over 600 page book entitled <a href="https://babel.hathitrust.org/cgi/pt?id=mdp.39015039846566&amp;view=1up&amp;seq=9">Principles of Neurodynamics</a> or this <a href="https://link.springer.com/book/10.1007/978-3-642-70911-1">historical review</a>.</p><p>From the foreword of that book we have the following quote:</p><p>"For this writer, the perceptron program is not primarily concerned with the invention of devices for "artificial intelligence", but rather with investigating the physical structures and neurodynamic principles which under lie "natural intelligence". A perceptron is first and fore most a brain model, not an invention for pattern recognition. As a brain model, its utility is in enabling us to determine the physical conditions for the emergence of various psychological properties."</p><h5>4.3.3<tt>&nbsp;</tt><a name="(part._.The_.Perceptron_.Rules)"></a>The Perceptron Rules</h5><p>The perceptron rules are the equations that characterize what a perceptron is, and what it does in contact with experience, so that it can learn and revise its behavior. A lot can be done with these simple equations.</p><p><span class="math">I = \sum_{i=1}^{n} w_i~x_i</span></p><p>If <span class="math">I \ge T</span> then <span class="math">y = +1</span> else if <span class="math">I &lt; T</span> then <span class="math">y = -1</span></p><p>If the answer was correct, then <span class="math">\beta = +1</span>, else if the
answer was incorrect then <span class="math">\beta = -1</span>.</p><p>The "T" in the above equation refers to the threshold. This is a user defined value that is conveniently, and often made, to be zero.</p><p>Updating is done by <span class="math">\mathbf{w_{new}} =
\mathbf{w_{old}} + \beta y \mathbf{x}</span></p><h5>4.3.4<tt>&nbsp;</tt><a name="(part._.You_.Are_.The_.Perceptron)"></a>You Are The Perceptron</h5><p>This is a pencil and paper exercise. Before coding it is often a good idea to try and work the basics out by hand. This may be a flow chart or a simple hand worked example. This both gives you a simple test case to compare your code against, but more importantly makes sure that you understand what you are trying to code. Let&rsquo;s make sure you understand how to compute the perceptron learning rule, but doing a simple case by hand.</p><p>Beginning with an input of <span class="math">\begin{bmatrix}0.3 \\ 0.7 \end{bmatrix}</span>, an initial set of weights of <span class="math">\begin{bmatrix}-0.6 \\ 0.8 \end{bmatrix}</span>, and a <span style="font-weight: bold">class</span> of 1. Compute the value of the new weight vector with pen and paper.</p><h5>4.3.4.1<tt>&nbsp;</tt><a name="(part._.A_simple_data_set)"></a>A simple data set</h5><p>For these data there are two dimensions or features (the first and second columns) and the third colum represents their <span style="font-style: italic">class</span>.</p><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">matrix</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktPn">[</span><span class="hspace">&nbsp;</span><span class="RktVal">0.3</span><span class="hspace">&nbsp;</span><span class="RktVal">0.7</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktVal"><span class="nobreak">-0</span>.5</span><span class="hspace">&nbsp;</span><span class="RktVal">0.3</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span>.0</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktVal">0.7</span><span class="hspace">&nbsp;</span><span class="RktVal">0.3</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span><span class="RktPn">]</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">[</span><span class="RktVal"><span class="nobreak">-0</span>.2</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-0</span>.8</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span>.0</span><span class="RktPn">]</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr></table></blockquote><p>Using the starting weight above write code to iteratively compute a new weight from each input and it&rsquo;s class and using the current weight. If you can, save each updated weight so you can see how they change, but if you can&rsquo;t still try to use a for construct to iterate through these data and see how the weights change.</p><p>In broad outlines you will need to decide on a data structure. You can use a matrix as I have here, but it may be easier to just use a list to start. For example <span class="RktPn">(</span><span class="RktSym">list</span><span class="stt"> </span><span class="RktPn">(</span><span class="RktSym">list</span><span class="stt"> </span><span class="RktVal">0.3</span><span class="stt"> </span><span class="RktVal">0.7</span><span class="RktPn">)</span><span class="stt"> </span><span class="RktVal">1.0</span><span class="RktPn">)</span>. The first element of the list would be the input data and the last item the desired class. You could create a list of list of such elements to capture the matrix I have displayed above.</p><p>This progressive updating of the weight vector is the <span style="font-style: italic">learning</span>. Note that sometimes our initial weight vector classifies incorrectly. How does it do after one complete cycle through all the training examples?</p><p><div class="SIntrapara">Checking our Learned Weight For One Input</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">let</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">in-class</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span>.0</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">if</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">=</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">if</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">&gt;=</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">matrix-ref</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">matrix*</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">row-matrix</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktVal"><span class="nobreak">-0</span>.6</span><span class="hspace">&nbsp;</span><span class="RktVal">0.3</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">col-matrix</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktVal">1.2</span><span class="hspace">&nbsp;</span><span class="RktVal">2.3</span><span class="RktPn">]</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="hspace">&nbsp;</span><span class="RktVal">0</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">0.0</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span></span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktSym">in-class</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">"correct"</span><span class="hspace">&nbsp;</span><span class="RktVal">"incorrect"</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><span class="RktRes">"correct"</span></p></td></tr></table></blockquote></div></p><h5>4.3.4.2<tt>&nbsp;</tt><a name="(part._.What_does_it_all_mean__.How_is_the_.Perceptron_.Learning_)"></a>What does it all mean? How is the Perceptron Learning?</h5><p><div class="SIntrapara">Changing Weights as Vectors</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="RktPn">(</span><span class="RktSym">wt-plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">one-loop-through-data</span><span class="hspace">&nbsp;</span><span class="RktSym">my-data</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">col-matrix</span><span class="hspace">&nbsp;</span><span class="RktPn">[</span><span class="RktVal"><span class="nobreak">-0</span>.6</span><span class="hspace">&nbsp;</span><span class="RktVal">0.8</span><span class="RktPn">]</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><p><img src="pict_17.png" alt="image" width="400" height="400"/></p></td></tr></table></blockquote></div></p><p>These functions and the <span class="stt">my-data</span> are in the file <a href="./../code/perceptron-rule.rkt">perceptron-rule.rkt</a>. Each time through the perceptron rule I compute the new weights and use the first position as the &rsquo;x&rsquo; value and the second position as the &rsquo;y&rsquo; value to plot vectors on an &rsquo;x-y&rsquo; plane. You can imagine that as we iterate through the data we are rotating the vectors around an origin. The decision plane is perpendicular to the vectors and anchored at the bottom of the arrows. If you compare this to the location of the data points (which you can add to the plot by editing the functions in the linked file) you will see that the rule is learning to find the decision plane that puts all of one class on one side of the line and all of the other class on the other side. That is why it is limited to problems that are linearly separable!</p><h5>4.3.4.3<tt>&nbsp;</tt><a name="(part._.Bias)"></a>Bias</h5><p>These data were selected such that the base of the vector could separate them while anchored at zero. However, for many data sets you not only need to learn what direction to point the vector, but you also need to learn where to anchor the vector. This is done by including a <span style="font-weight: bold">bias weight</span>. Add an extra dimension to your weight vector and your inputs. For the inputs it will just be a constant value of 1.0, but this extra, bias weight, will also be learned and allows you to achieve, effectively, a translation away from the origin to be able to separate points that are more heterogeneously scattered.</p><h5>4.3.4.3.1<tt>&nbsp;</tt><a name="(part._.Geometrical_.Thinking)"></a>Geometrical Thinking</h5><ul><li><p>What is the relation between the inner product of two vectors and the cosine of the angle between them?</p></li><li><p>What is the *sign* for the cosine of angles less than 90 degrees and those greater than 90 degrees?</p></li><li><p> How do these facts help us to answer the question above?</p></li><li><p> Why does this reinforce the advice to think /geometrically/ when thinking about networks and weight vectors?</p></li></ul><h5>4.3.5<tt>&nbsp;</tt><a name="(part._.The_.Delta_.Rule_-_.Homework)"></a>The Delta Rule - Homework</h5><p>The <span style="font-weight: bold"><span style="font-style: italic">Delta Rule</span></span> is another simple learning rule that is a minimal variation on the perceptron rule. It is used more frequently, and it has the spirit of Hebbian learning, which we will learn more about soon. The homework asks you to write code to test and train an artificial neuron using the delta learning rule.</p><ol><li><p>For an easy start create some pseudo random linearly separable points on a a sheet of paper. Label one population as <span style="font-weight: bold">1</span> and the other population as <span style="font-weight: bold">-1</span>.</p></li><li><p>For a more challenging set-up create the data programmatically using random numbers and some method that allows you to vary how close or distant the points are to the line of separation, and how many points there are to train on.</p></li><li><p>The Delta Learning rule is: <div class="math">\Delta~w_i = x_i~\eta(desired - observed)</div></p></li><li><p>Submit your code that has your test data in it. Start with an initial random weight and use the delta rule to learn the correct weighting to solve all your training examples. Then test on a new set of points that you did <span style="font-weight: bold">not</span> test on but that are classified according to the same rule. Your code should assess how well the <span style="font-style: italic">trained</span> rule classifies the <span style="font-style: italic">test</span> data.</p></li></ol><p>I have <a href="./../code/perceptron-rule.rkt">some code</a> for the perceptron that might give you some code you could adapt if you have trouble getting started.</p><h5><a name="(part._ref~3aperceptron)"></a>Perceptron Bibliography</h5><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td></td></tr></table></p><h4>4.4<tt>&nbsp;</tt><a name="(part._.Hopfield_.Networks)"></a>Hopfield Networks</h4><h5>4.4.1<tt>&nbsp;</tt><a name="(part._.Not_all_.Networks_are_the_.Same)"></a>Not all Networks are the Same</h5><ul><li><p>Feedforward</p></li><li><p>Recurrent</p></li><li><p>Convolutional</p></li><li><p>Multilevel</p></li><li><p>Supervised</p></li><li><p>Unsupervised</p></li></ul><p>The Hopfield network<span class="Autobibref">&nbsp;(<a href="#%28autobib._.J..._.J..._.Hopfield.Neural._networks._and._physical._systems._with._emergent._collective._computational._abilities...P.N.A.S._79%2C._pp..._2554--25581982https~3a%2F%2Fwww..pnas..org%2Fdoi%2Fabs%2F10..1073%2Fpnas..79..8..2554%29" class="AutobibLink" data-pltdoc="x">Hopfield</a> <a href="#%28autobib._.J..._.J..._.Hopfield.Neural._networks._and._physical._systems._with._emergent._collective._computational._abilities...P.N.A.S._79%2C._pp..._2554--25581982https~3a%2F%2Fwww..pnas..org%2Fdoi%2Fabs%2F10..1073%2Fpnas..79..8..2554%29" class="AutobibLink" data-pltdoc="x">1982</a>)</span> has taught many lessons, both practical and conceptual. Hopfield showed physicists a new realm for their skills and added recurrent (i.e. feedback) connections to network design (output becomes input). He changed the focus from network architecture to that of a dynamical system. Hopfield showed that the network could remember and it could do some error correction, it could reconstruct the "right" answer from faulty input.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="pict_18.png" alt="image" width="400" height="400"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3ahopfield-net))" x-target-lift="Figure"></a>Figure&nbsp;11: </span>Hopfield Recurrent Connections</span></p></blockquote><h5>4.4.1.1<tt>&nbsp;</tt><a name="(part._.How_does_a_network_like_this_work_)"></a>How does a network like this work?</h5><ul><li><p>Each node has a value.</p></li><li><p>Each of those arrowheads has an associated weight.</p></li><li><p>The line with the "x" indicates that there are no self connections.</p></li><li><p>All other connections for all other units are present and go in both directions.</p></li></ul><h5>4.4.1.2<tt>&nbsp;</tt><a name="(part._.Test_your_understanding_)"></a>Test your understanding:</h5><ol><li><p>Tell me what the input for a network like this with four nodes should look like it terms of the linear algebra constructs we have talked about.</p></li><li><p>A weight is a number associated to each connection. Tell me what the weights should look like in terms of the linear algebra constructs.</p></li><li><p>How might we conceive of "running" the network for one cycle in terms
of the above.</p></li></ol><h5>4.4.1.3<tt>&nbsp;</tt><a name="(part._.A_.Worked_.Example)"></a>A Worked Example</h5><p>Inputs can be thought of as vectors. Although I have drawn the network like a square that shape is really independent of the structure of data flow. Each node needs an input and each node will need a weighted contact to all the other nodes. Consider the following two input patterns and the following weight matrix.
  <div class="math">A = \{1,0,1,0\}^T</div></p><p><div class="math">B = \{0,1,0,1\}^T</div></p><p><div class="math">weights =  \begin{bmatrix}
  0 &amp; -3 &amp; 3 &amp; -3\\
  -3 &amp; 0 &amp; -3 &amp; 3\\
  3 &amp; -3 &amp; 0 &amp; -3\\
  -3 &amp; 3 &amp; -3 &amp; 0\\
  \end{bmatrix}</div></p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Ask yourself, how do I compute the output? Which comes first: the matrix or the input vector and why?</p></blockquote></blockquote></blockquote><p>Hopfield networks use a threshold rule. This non-linearity is, at least metaphorically, like the threshold that says whether a neuron in the brain or in our integrate and fire model fires. For the Hopfield network our threshold rule says:</p><p><div class="math">output(t)=\{\begin{array}{c} 1\; \mbox{if } t \geq \Theta\\ 0\; \mbox{if } t &lt; \Theta \end{array}</div></p><p><span class="math">\Theta</span> will represent the value of our threshold and for now let&rsquo;s set <span class="math">\Theta = 0</span>.</p><p>To make sure you understand the mechanics of this type of network you should first calculate the output to each of the two input patterns.</p><p>Then, to test your intuition, you should guess what output you would get for an input of <span class="math">\{1,0,0,0\}^T</span>. Calculate it.</p><p>To understand why this is the case, ask yourself whether A or B is <span style="font-weight: bold">closer</span> to this test input? This will hopefully lead you to reflect on what it means, in this context, for one vector to be "closer" to another.</p><h5>4.4.1.3.1<tt>&nbsp;</tt><a name="(part._.Distance_.Metrics)"></a>Distance Metrics</h5><p>Metrics relate to measurement. For some operation to be a distance metric it should meet three intuitive requirements and one that is maybe not as obvious. To measure the distance between two things we need an operation that is binary. That is, it takes two inputs. In this case that would be our two vectors. It&rsquo;s result should always be <span style="font-weight: bold">Non-negative</span>. A negative distance would clearly be meaningless. Our output should be <span style="font-weight: bold">symmetric</span>. Meaning that <span class="math">d(A,B)~d(B,A)</span>. The distance from Waterloo to Toronto ought to come out as the same as going from Toronto to Waterloo. Our metric should be <span style="font-weight: bold">reflexive</span>. The distance from anything to itself ought to be zero. Lastly, to be a distance metric, our operation must obey the <a href="https://en.wikipedia.org/wiki/Triangle_inequality"><span style="font-weight: bold">triangle inequality</span></a></p><p>Now, to understand what the network did, consider your distance measure to be the number of mismatched bits. This metric is called the Hamming distance.</p><p><span style="font-style: italic">Reminder</span>: Don&rsquo;t forget to think about geometry and dynamics.</p><p>For perceptrons we talked about how the weight vector moved the direction it pointed. Here we don&rsquo;t have the weight vector moving, but you can visualize what is happening as updating a point in space. When we first input our four element vector we have a location in 4-D space. We multiply the first row of our weight matrix against our column of the input vector and we see, in effect, what is the effect on our first element (node) of all the other weighted inputs coming in to it. We then "update" that location. Maybe we flip it from a 1 to a zero (or vice versa). Then we try the next row of the weight matrix to see what happens to the second element. As we change the values of our nodes we are creating new points. The sequence of points is a trajectory that we are tracing in the input space. In this simple situation here we only require one pass to reach the final location, but in other settings we might not. In that case we just keep repeating the process until we do. One of the wonderful insights that Hopfield had was that by conceptualizing this process as an "energy" he could mathematically prove that the process would always reach a resting place.</p><h5>4.4.1.4<tt>&nbsp;</tt><a name="(part._.Hebb_s_.Outer_.Product_.Rule)"></a>Hebb&rsquo;s <a href="https://en.wikipedia.org/wiki/Outer_product">Outer Product</a> Rule</h5><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Why is this learning rule called "Hebb&rsquo;s"? And if you don&rsquo;t know who Hebb is let&rsquo;s take a moment to figure that out.</p></blockquote></blockquote></blockquote><p><div class="SIntrapara">The strength of a change in a connection is proportionate to the product of the input and outputs, i.e. <div class="math">\Delta A[i,j] = \eta f[j]g[i]</div> and <div class="math">g[i] = \sum_j~A[i,j]~f[j]</div> therefore, <div class="math">\vec{g} = \mathbf{Af}</div>. </div><div class="SIntrapara"><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>Does it matter that the (\mathbf{W}) comes first?</p></blockquote></blockquote></blockquote></div></p><blockquote class="refpara"><blockquote class="refcolumn"><blockquote class="refcontent"><p>What is an outer product? Can you compute one with racket?</p></blockquote></blockquote></blockquote><h5>4.4.2<tt>&nbsp;</tt><a name="(part._.Hopfield_.Homework_.Description__.Robustness_to_.Noise)"></a>Hopfield Homework Description: Robustness to Noise</h5><p><div class="SIntrapara">Overview of the steps to take:
</div><div class="SIntrapara"><ol><li><p>Create a small set of random data for input patterns.</p></li><li><p>Generate the weights necessary to properly decode the inputs.</p></li><li><p>Conceive of a way to randomly corrupt the inputs. Perhaps by flipping some bits and show that your network does correctly decode the uncorrupted inputs.</p></li><li><p>Report the accuracy of the output. Explore how the length of the input vector and the number of bits your "flip" impact performance.</p></li></ol></div></p><p><div class="SIntrapara">Detailed instructions:
</div><div class="SIntrapara"><ol><li><p>Make the input patterns 2-d, square and of size "n".</p></li><li><p>Use a bipolar system and have, roughly, equal numbers of +1s and -1s in your patterns.</p></li><li><p>Make a few of them and store them in some sort of data structure.</p></li><li><p>Using those patterns, compute the weight matrix with the following equation:
<div class="math">w_{ij} =\frac{1}{N} \sum_{\mu} value^\mu_i \times value^\mu_j</div>
Where N is the size of the patterns, that is how many "neurons". <span class="math">\mu</span> is an index for each of the patterns, and <span class="math">i</span> and <span class="math">j</span> refer to the neurons in the pattern <span class="math">\mu</span>. Do this <span style="font-weight: bold">in code</span>. The computer is good   for this manual, repetitive sort of stuff.</p></li><li><p>Program an <span style="font-weight: bold">asynchronous</span> updating rule, run your network until it stabilizes, and then show that you get back what you put in.</p></li><li><p>Then do the same for at least one disrupted pattern (where you   flipped a couple of bits around.)</p></li></ol></div></p><h5><a name="(part._ref~3ahopfield)"></a>Hopfield Bibliography</h5><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td><p><span class="Autobibtarget"><a name="(autobib._.J..._.J..._.Hopfield.Neural._networks._and._physical._systems._with._emergent._collective._computational._abilities...P.N.A.S._79,._pp..._2554--25581982https~3a//www..pnas..org/doi/abs/10..1073/pnas..79..8..2554)"></a><span class="Autobibentry">J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. <span style="font-style: italic">PNAS</span> 79, pp. 2554&ndash;2558, 1982. <a href="https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554"><span class="url">https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554</span></a></span></span></p></td></tr></table></p><h4>4.5<tt>&nbsp;</tt><a name="(part._.Backpropagation)"></a>Backpropagation</h4><h5>4.5.1<tt>&nbsp;</tt><a name="(part._.Warm_up_questions)"></a>Warm up questions</h5><ol><li><p>What is a neural network?</p></li><li><p>What is the difference between supervised and unsupervised learning? Give an example of each?</p></li><li><p>What is the <span style="font-style: italic">activation function</span> we have used for the perceptron and delta rule networks?</p></li><li><p>What role does "error" play in the perceptron and delta learning rules?</p></li><li><p>For a multilayer network how do you know how much of the "error" to pass back into the deeper layers of the network?</p></li></ol><h5>4.5.2<tt>&nbsp;</tt><a name="(part._.Sigmoid_.Functions)"></a>Sigmoid Functions</h5><p>Our prior networks have been forms of threshold units. We check to see if our activation cleared a certain hurdle, and if so, set its value to 1 or -1.</p><p>While this step-function approach was used originally, it is more common now to scale the output continuously between a lower and upper bound. One of the intuitions is that this is like a probability that the neuron might fire.</p><p><div class="SIntrapara">Example:</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">sig</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">/</span><span class="hspace">&nbsp;</span><span class="RktVal">1.0</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">+</span><span class="hspace">&nbsp;</span><span class="RktVal">1</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">exp</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">*</span><span class="hspace">&nbsp;</span><span class="RktVal"><span class="nobreak">-1</span>.0</span><span class="hspace">&nbsp;</span><span class="RktSym">x</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">plot</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">function</span><span class="hspace">&nbsp;</span><span class="RktSym">sig</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym"><span class="nobreak">-</span></span><span class="hspace">&nbsp;</span><span class="RktVal">5</span><span class="RktPn">)</span><span class="hspace">&nbsp;</span><span class="RktVal">5</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">#:title</span><span class="hspace">&nbsp;</span><span class="RktVal">"A sigmoid function."</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><p><img style="vertical-align: 0px; margin: -3px -3px -3px -3px;" src="pict_19.png" alt="image" width="406" height="406"/></p></td></tr></table></blockquote></div></p><p><span class="math">\frac{1}{1+e^{-z}}</span></p><h5>4.5.2.1<tt>&nbsp;</tt><a name="(part._.A_few_questions_about_sigmoid_functions)"></a>A few questions about sigmoid functions</h5><ol><li><p>Why is it called "sigmoid?"</p></li><li><p>What advantage does it offer over a threshold function?</p></li><li><p>Is it the only "sigmoid" function? Does it have other names?</p></li><li><p>Can you guess an an advantage to this particular form of the equation?</p></li><li><p>How do use this with a neural network, i.e. what is <span class="math">z</span>?</p></li></ol><p>Getting ready to put things together. Can you write a small snippet of racket code that takes a vector of inputs, appends a bias input, combines this with a suitable weight vector using the scalar product and pipes the result though the sigmoid function?
Think about equations qualitatively.</p><p>Remember, one of the goals of computational modeling is to get an insight into the implications of our ideas and theories. Sometimes this means running a model to see what comes out of it. But it can also mean that we look at the equations that go into the model and think about their "behavior" to get some sense of how things will behave that have particular functional forms.</p><p>How might you do that here? Think about how it the process just described is the same as, and different from, the threshold based rules we have been using up until now. Think about extreme values: what happens at the extremes? How is that like (or different from) our older threshold rules?</p><p>Remember that derivatives are rates of change. If we want to know how the error changes as we change something else we will need a derivative. What problem does this approach run into when using a threshold unit?</p><p>In the sigmoid illustrated above where is the derivative maximal? What happens if the dot product of a weight vector and input vector are large? Or very small (and what does small mean here)? What about negative extremes and positive extremes.</p><p>Can you think of a function that would give us an even simpler derivative and why might we want (or not want) to use it?</p><p>Why are we starting this discussion of the backpropagation algorithm with all this discussion of activation functions?</p><p>In summary, we want to understand ...
1. What is being backpropagated?
2. What is it we want our network to do?
3. How do we guide it?</p><p>Many networks have a cost function. We may want to know more than just whether you were right or wrong, but how wrong? In a continuous case being "right" might not even really be possible - what is the value of <span class="math">\pi</span> ? Our computers cannot be precise. There is not a single "right" cost function either, but what might you suggest that we use, and why?</p><p>What would you suggest as the cost function?</p><h5>4.5.2.1.1<tt>&nbsp;</tt><a name="(part._.Mean_.Squared_.Error)"></a>Mean Squared Error</h5><p>It&rsquo;s always a good guess and a reasonable starting point</p><p><div class="math">C(\mathbf{w}) = \frac{1}{2\mathrm{n}}\sum_\mathbf{x} \lVert \mathbf{y}(\mathbf{x}) - \mathbf{a}\rVert^2</div></p><p>Some Questions:
Why isn&rsquo;t this a function of <span class="math">\mathbf{x}</span> and <span class="math">\mathbf{y}</span> too?</p><p>What is the <span style="font-style: italic">dimensionality</span> of the part of the equation inside the double lines?</p><p>What do you call the operation characterized by the double lines?</p><p>Why is adjusting weights for a multilayer network hard?</p><h5>4.5.3<tt>&nbsp;</tt><a name="(part._.Backpropagation_1)"></a>Backpropagation 1</h5><p>We learned in implementing the XOR function that we can solve complex (i.e. non-linearly separable ones) problems if we use a <span style="font-style: italic">multi-layer</span> network. However we have a problem. In a single layer network it is clear how our output error depends on the weights, but how do we apportion out the error to earlier layers when we are in a multi-layer situation?</p><p>If you think about it the only thing we are really free to change are the weights. Sure, our error will change if we change the output to make it closer to the input, but in the common scenarios for which we use such neural networks we want to achieve a particular input-output mapping. For that reason as well, we can&rsquo;t alter our inputs. They are our data. We have to accept them as given.</p><p>If weights are the only thing we can change we have to discover, if one exists, an algorithm for apportioning out the error to early weights. This is the achievement of <a href="http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf">backpropagation algorithm</a>. If you look you will find that you can read this article. It does not use any mathematical concepts that we have not already covered. You have all the notation, language, and concepts. Note that the abstract makes sense to you.</p><p><span style="font-style: italic"><span style="font-weight: bold">Class Question?</span></span>
Is backpropagation biologically plausible?</p><p>Some intuition can help to understand the ideas behind the backpropagation algorithm even if the math gets too intricate for you (and it is more an issues of intricacy than concepts). If we get a wrong answer we might want to change the contributions from a node that is very active. This is because that even if we have a node that is badly weighted if its total activation is small it can&rsquo;t be contributing much to the error. We want to concentrate on nodes and weights were the activity is large and thus small changes will have big effects on errors. This should suggest the idea of a derivative. We want to put most of our change at locations where the ratio of improved output to small changes of weights is high. It is there we get the best return from our adjustment.</p><h5>4.5.3.1<tt>&nbsp;</tt><a name="(part._.Some_.Details)"></a>Some Details</h5><p>The mathematics behind the backpropagation algorithm involves derivatives. These derivatives are usually "partial". We study the rate of change of our dependent variable as a function of one of many possible independent variables. If we want to study how the error changes as we change one specific weight in our network we are looking at the partial derivative. This is typically notated with a sort of curly d like <span class="math">\partial</span>. This means that we could write our rate of change of the error as a function of the change in a particular weight in layer l connecting the kth neuron in the l-1 layer to the jth neuron in the l layer as
<span class="math">\frac{\partial E}{\partial w_{jk}^l}</span>. Note this ordering maybe backwards from your intuition.</p><p>We do not have an equation that directly specifies the change of error in terms of a specific weight, but we can tell how the error changes if we change the output of the last layer. That output is determined by our activation function which is determined in part by the input. By looking how this chain of relationships change we can track our way back to a dependency on the weights. In calculus there is a rule for navigating such chains. It is called the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a>.</p><p>The details of all this back tracking is tedious, but collapses into two different classes. One is for the output layer where we have a direct comparison to the error. The second is all the earlier layers, the so-called "hidden" layers, where we have to say our a current weights change depends on what went before. Thus to know what to do at layer l-1 we need to know facts about layer l. But we only need the immediately preceding layer. So, if we start at the top and work our way back layer by layer we can backpropagate the error. Doing the same thing over and over again is what computers are good at and people are bad, so if we can we want to write a program do this repetitive computation for us.</p><p>Today, there are many nice libraries that have been written to scale nicely, and to run efficiently. We do not have to write this algorithm ourselves. One of the most popular is the python library <a href="https://pytorch.org/">pyTorch</a>. If you plan to use backpropagation for any real application you should probably not write your own implementation. It is an error prone and frustrating process that will probably not run as fast or reliably as the use of an external library. Check your language for a suitable implementation.</p><h5>4.5.3.2<tt>&nbsp;</tt><a name="(part._.Learning_.About_.Backpropagation)"></a>Learning About Backpropagation</h5><p>While the above is true for a professional use case, it is not true from a learning perspective. There are many benefits from working through some of the math yourself, and trying to write your own simple implementation. The following are intended as bread crumbs if you decide to follow that route.</p><h5>4.5.3.2.1<tt>&nbsp;</tt><a name="(part._.Bread_.Crumbs)"></a>Bread Crumbs</h5><p>If you decide to try and follow the chain rule chain to see how weights in early layers can be updated based on backpropagated errors start with a single linear line of nodes that each one connects to the next with a single weight. This is not a useful network for computing anything, but it is a nice simple system for exploring the mathematical relationships without worrying too much about subscripts.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="pict_20.png" alt="image" width="400" height="400"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3alinear-net))" x-target-lift="Figure"></a>Figure&nbsp;12: </span>A simple linear network that can be useful for tracking the chain rule derivations.</span></p></blockquote><p>I found this <a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">this page</a> to give a very nice overview of the derivatives and how they relate as you expand them via the chain rule. There are also some simple numerical examples that you can work by hand to check your understanding. This site does not include code, which can be a nice way to focus on the logic before worrying about how to implement it.</p><p>Often you will see the "sigma" character in on line discussion. This sigma is generally whatever sigmoid, roughly s-shaped, function is being used. As such, the specific derivative will depend on that choice. Don&rsquo;t assume that it is always going to be the logistic function, though this is a common choice.</p><p>How would you write as an equation with the "sigma" (<span class="math">\sigma</span> summation sign the value activation of a single <span style="font-style: italic">arbitrary</span> neuron in an <span style="font-style: italic">arbitrary</span> layer of a multi-layer network?</p><p><span style="font-weight: bold">The activation</span></p><p>To help you check your understanding try to describe in words what is happening here:
<div class="math">a^l_j = \sigma \left ( \sum_k w_{jk}^l~a^{l-1}_k \right )</div></p><p>One of the reasons for this type of equation with all its formatting as subscripts and superscripts is that the coding of the backpropagation algorithm often uses multi-dimensional arrays. All the inputs are treated as vectors and loaded into a matrix where each row (or column) is one pattern, and the collection is a matrix. The weights between one layer and the next are going to be a matrix as well with one dimension the number of nodes in the first layer and the other dimension the number of nodes in the next layer. Each row/column intersection holds the value of one weight. To collect all the weight matrices into a single structure we need to aggregate them into some sort of three dimensional structure where each matrix can be thought to be stacked on the one that came before. If this sounds complicated to think about imagine trying to code it. It is a project, and it does not map easily on to the logic of the neural network that we learn about as layers and nodes serially connected.</p><p>In addition, there are other arrays that are needed. We must keep track of the errors that we backpropagate and the inputs that are going forward. Depending on your implementation you may need an array for inputs, one for weights, one for activations, one for "deltas", and then you will need to progressively loop over all the layers from beginning to end to get the feedforward output, and then backwards to apply the weight adjustments from end to beginning. This requires careful book-keeping and making sure you orient the various matrices correctly.</p><p>If you are looking for a step by step approach to coding this algorithm, one that uses an object oriented orientation, this <a href="https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/">version</a> in python is accessible.</p><p><div class="SIntrapara">Here is a pseudo-code summary:
</div><div class="SIntrapara"><ol><li><p>Fix the inputs of the first layer to the input pattern <span class="math">x</span></p></li><li><p>Compute the weighted input to each neuron of the next layer using the input, weights and biases.</p></li><li><p>Compute the weighted cost function error vector for the last layer.</p></li><li><p>Backpropagate the error</p></li><li><p>Use the backpropagated error to update the weights</p></li></ol></div></p><p>I wrote a version in racket that seems to work for simple cases. As I only tested it in a few limited cases you are encouraged to probe it for bugs and logic errors and suggest corrections.</p><p><div class="SIntrapara">Illustrating Backpropagation Code</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">begin</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">displayln</span><span class="hspace">&nbsp;</span><span class="RktVal">"Before Training"</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">for</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">first</span><span class="hspace">&nbsp;</span><span class="RktSym">data-xor</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">displayln</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">test-learning</span><span class="hspace">&nbsp;</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktSym">test-net</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">define</span><span class="hspace">&nbsp;</span><span class="RktSym">many-loops-bp</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">bp-loop</span><span class="hspace">&nbsp;</span><span class="RktSym">data-xor</span><span class="hspace">&nbsp;</span><span class="RktSym">test-net</span><span class="hspace">&nbsp;</span><span class="RktPn">#:loop-no</span><span class="hspace">&nbsp;</span><span class="RktVal">1000</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">displayln</span><span class="hspace">&nbsp;</span><span class="RktVal">"After Training 1000 loops"</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">for</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktPn">[</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">map</span><span class="hspace">&nbsp;</span><span class="RktSym">first</span><span class="hspace">&nbsp;</span><span class="RktSym">data-xor</span><span class="RktPn">)</span><span class="RktPn">]</span><span class="RktPn">)</span></td></tr><tr><td><span class="hspace">&nbsp;&nbsp;</span><span class="hspace">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="RktPn">(</span><span class="RktSym">displayln</span><span class="hspace">&nbsp;</span><span class="RktPn">(</span><span class="RktSym">test-learning</span><span class="hspace">&nbsp;</span><span class="RktSym">i</span><span class="hspace">&nbsp;</span><span class="RktSym">many-loops-bp</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span><span class="RktPn">)</span></td></tr></table></td></tr><tr><td><table cellspacing="0" cellpadding="0"><tr><td><p><span class="RktOut">Before Training</span></p></td></tr><tr><td><p><span class="RktOut">(0.6486284472389824)</span></p></td></tr><tr><td><p><span class="RktOut">(0.5840047763549285)</span></p></td></tr><tr><td><p><span class="RktOut">(0.5733271693623475)</span></p></td></tr><tr><td><p><span class="RktOut">(0.5136514924489283)</span></p></td></tr><tr><td><p><span class="RktOut">After Training 1000 loops</span></p></td></tr><tr><td><p><span class="RktOut">(0.12072334767610929)</span></p></td></tr><tr><td><p><span class="RktOut">(0.8761859519113474)</span></p></td></tr><tr><td><p><span class="RktOut">(0.8772814094933333)</span></p></td></tr><tr><td><p><span class="RktOut">(0.15098879903621978)</span></p></td></tr></table></td></tr></table></blockquote></div></p><h5>4.5.4<tt>&nbsp;</tt><a name="(part._.Homework)"></a>Homework</h5><p><div class="SIntrapara">The homework will only require you to use the library provided in order to explore some of the features of the backpropagation algorithm. It will acquaint you with some of the terminology and some of the practical considerations.
</div><div class="SIntrapara"><ol><li><p>Does a backpropagation network always get the same answer? Create at least three random networks. Train them for the same number of trials and compare their accuracy at the end and inspect the weights of the last layer. Are they the same?</p></li><li><p>Does the number of neurons matter or the number of layers? Should you need more than one layer? Compare a 2 - 5 - 5 - 1 to a 2 - 10 - 1 network and report your observations.</p></li><li><p>What is a global minimum and how does it differ from a local minimum. Which are you guaranteed to get with backprop?</p></li><li><p>Test your network for catastrophic forgetting. In my code I train on each of the four XOR inputs one after the other over and over. Test just one pattern for the same number of loops. Then, using those weights as your ending verify you are getting the correct answer. Then train on the second pattern starting from that network. Now go back and test on the original input pattern. Report on your observations.</p></li></ol></div></p><h5>4.5.4.1<tt>&nbsp;</tt><a name="(part._.Additional_.Readings)"></a>Additional Readings</h5><p><div class="SIntrapara">In years past some student have recommend other sources they like.
</div><div class="SIntrapara"><ul><li><p><a href="http://neuralnetworksanddeeplearning.com/chap1.html">On line deep learning textbook</a></p></li><li><p><a href="https://youtu.be/bxe2T-V8XRs?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU">Youtube video on backpropagation coding</a></p></li></ul></div></p><h5><a name="(part._ref~3abackprop)"></a>Backpropagation Bibliography</h5><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td></td></tr></table></p><h3>5<tt>&nbsp;</tt><a name="(part._.Projects)"></a>Topics for Final Projects</h3><h4>5.1<tt>&nbsp;</tt><a name="(part._.Agent_.Based_.Modeling)"></a>Agent Based Modeling</h4><p>Provide an overview of the idea behind agent based models in psychology. Give a demonstration by implementing in racket a simple version (including graphics) of the classic work of Schelling <span class="Autobibref">&nbsp;(<a href="#%28autobib._.Thomas._.C..._.Schelling.Dynamic._models._of._segregation.The._.Journal._of._.Mathematical._.Sociology._1%2C._pp..._143--1861971https~3a%2F%2Fdoi..org%2F10..1080%2F0022250.X..1971..9989794%29" class="AutobibLink" data-pltdoc="x">Schelling</a> <a href="#%28autobib._.Thomas._.C..._.Schelling.Dynamic._models._of._segregation.The._.Journal._of._.Mathematical._.Sociology._1%2C._pp..._143--1861971https~3a%2F%2Fdoi..org%2F10..1080%2F0022250.X..1971..9989794%29" class="AutobibLink" data-pltdoc="x">1971</a>)</span>.</p><h4>5.2<tt>&nbsp;</tt><a name="(part._.Genetic_.Algorithms)"></a>Genetic Algorithms</h4><p>Genetic algorithms are algorithms where the "learning" takes place via selection and recombination in analogy to evolution. It has been used to select the parameters of otherwise conventional neural networks, but can also be used on its own. If you select this project you will need to explain what a genetic algorithm is, make the case that it is relevant to some aspect of neural or psychological modeling, and provide some racket code implementing some examples. A <a href="https://www.whitman.edu/Documents/Academics/Mathematics/2014/carrjk.pdf">tutorial paper (pdf)</a> with MATLAB&#174; code is available. A more academic treatment (and much deeper treatment) is available as a review <span class="Autobibref">&nbsp;(<a href="#%28autobib._.Kenneth._.O..._.Stanley%2C._.Jeff._.Clune%2C._.Joel._.Lehman%2C._and._.Risto._.Mikkulainen.Designing._neural._networks._thought._neuroevolution.Nature._.Machine._.Intelligence._1%2C._pp..._24--352019https~3a%2F%2Fwww..researchgate..net%2Fprofile%2F.Jeff-.Clune%2Fpublication%2F330203191_.Designing_neural_networks_through_neuroevolution%2Flinks%2F5e7243fc92851c93e0ac18ea%2F.Designing-neural-networks-through-neuroevolution..pdf~3f_sg~255.B0~255.D~3dstarted_experiment_milestone~26_sg~255.B1~255.D~3dstarted_experiment_milestone~26origin~3djournal.Detail%29" class="AutobibLink" data-pltdoc="x">Stanley et al<span class="Sendabbrev">.</span></a> <a href="#%28autobib._.Kenneth._.O..._.Stanley%2C._.Jeff._.Clune%2C._.Joel._.Lehman%2C._and._.Risto._.Mikkulainen.Designing._neural._networks._thought._neuroevolution.Nature._.Machine._.Intelligence._1%2C._pp..._24--352019https~3a%2F%2Fwww..researchgate..net%2Fprofile%2F.Jeff-.Clune%2Fpublication%2F330203191_.Designing_neural_networks_through_neuroevolution%2Flinks%2F5e7243fc92851c93e0ac18ea%2F.Designing-neural-networks-through-neuroevolution..pdf~3f_sg~255.B0~255.D~3dstarted_experiment_milestone~26_sg~255.B1~255.D~3dstarted_experiment_milestone~26origin~3djournal.Detail%29" class="AutobibLink" data-pltdoc="x">2019</a>)</span>. A recent podcast features a discussion with one of the authors, <a href="https://thegradientpub.substack.com/p/joel-lehman-open-endedness-and-evolution#details">Joel Lehman</a>.</p><h4>5.3<tt>&nbsp;</tt><a name="(part._.Quantum_.Probability)"></a>Quantum Probability</h4><p>Developed for quantum mechanics the formalism of quantum probability has been suggested to be a better approach to human decision making than conventional, classical probability<span class="Autobibref">&nbsp;(<a href="#%28autobib._.Peter._.D..._.Bruza%2C._.Zheng._.Wang%2C._and._.Jerome._.R..._.Busemeyer.Quantum._cognition~3a._a._new._theoretical._approach._to._psychology.Trends._in._.Cognitive._.Science._19%2C._pp..._383--3932015http~3a%2F%2Fdx..doi..org%2F10..1016%2Fj..tics..2015..05..001%29" class="AutobibLink" data-pltdoc="x">Bruza et al<span class="Sendabbrev">.</span></a> <a href="#%28autobib._.Peter._.D..._.Bruza%2C._.Zheng._.Wang%2C._and._.Jerome._.R..._.Busemeyer.Quantum._cognition~3a._a._new._theoretical._approach._to._psychology.Trends._in._.Cognitive._.Science._19%2C._pp..._383--3932015http~3a%2F%2Fdx..doi..org%2F10..1016%2Fj..tics..2015..05..001%29" class="AutobibLink" data-pltdoc="x">2015</a>)</span>; <span class="Autobibref">&nbsp;(<a href="#%28autobib._.Pothos%2C._.Emmanuel._.M..._and._.Busemeyer%2C._.Jerome._.R...Quantum._.Cognition.Annual._.Review._of._.Psychology._73%2C._pp..._749--7782022https~3a%2F%2Fdx..doi..org%2F10..1146%2Fannurev-psych-033020-123501%29" class="AutobibLink" data-pltdoc="x">Pothos, Emmanuel M. and Busemeyer, Jerome R.</a> <a href="#%28autobib._.Pothos%2C._.Emmanuel._.M..._and._.Busemeyer%2C._.Jerome._.R...Quantum._.Cognition.Annual._.Review._of._.Psychology._73%2C._pp..._749--7782022https~3a%2F%2Fdx..doi..org%2F10..1146%2Fannurev-psych-033020-123501%29" class="AutobibLink" data-pltdoc="x">2022</a>)</span>. It seems that one of the authors has some MATLAB&#174; <a href="https://jbusemey.pages.iu.edu/quantum/HilbertSpaceModelPrograms.htm">programs</a> available for some of the published models. For this project you will need to provide <span style="font-weight: bold">one</span> example of a different prediction between classical and quantum probability, review the empirical results in favor of the latter, and show a racket program that computes some of the key values.
An additional review came out recently <span class="Autobibref">&nbsp;(<a href="#%28autobib._.Andrei._.Khrennikov.Open._.Systems%2C._.Quantum._.Probability._and._.Logic._for._.Quantum-.Like._.Modeling._in._.Biology%2C._.Cognition%2C._and._.Decision._.Making.Co.R.R2023http~3a%2F%2Farxiv..org%2Fabs%2F2304..08599v1%29" class="AutobibLink" data-pltdoc="x">Khrennikov</a> <a href="#%28autobib._.Andrei._.Khrennikov.Open._.Systems%2C._.Quantum._.Probability._and._.Logic._for._.Quantum-.Like._.Modeling._in._.Biology%2C._.Cognition%2C._and._.Decision._.Making.Co.R.R2023http~3a%2F%2Farxiv..org%2Fabs%2F2304..08599v1%29" class="AutobibLink" data-pltdoc="x">2023</a>)</span>.</p><h4>5.4<tt>&nbsp;</tt><a name="(part._.Vector_.Symbolic_.Architectures)"></a>Vector Symbolic Architectures</h4><p>Provide an overview of vector symbolic architectures and provide a short racket implementation of Kanerva&rsquo;s "what is Mexico&rsquo;s dollar?" example <span class="Autobibref">&nbsp;(<a href="#%28autobib._.Pentti._.Kanerva.What._.We._.Mean._.When._.We._.Say._.%27.What%27s._the._.Dollar._of._.Mexico~3f.%27._~3a._.Prototypes._and._.Mapping._in._.Concept._.Space.In._.Proc..._.A.A.A.I._.Fall._.Symposium._.Series2010https~3a%2F%2Fwww..aaai..org%2Focs%2Findex..php%2F.F.S.S%2F.F.S.S10%2Fpaper%2Fview%2F2243%29" class="AutobibLink" data-pltdoc="x">Kanerva</a> <a href="#%28autobib._.Pentti._.Kanerva.What._.We._.Mean._.When._.We._.Say._.%27.What%27s._the._.Dollar._of._.Mexico~3f.%27._~3a._.Prototypes._and._.Mapping._in._.Concept._.Space.In._.Proc..._.A.A.A.I._.Fall._.Symposium._.Series2010https~3a%2F%2Fwww..aaai..org%2Focs%2Findex..php%2F.F.S.S%2F.F.S.S10%2Fpaper%2Fview%2F2243%29" class="AutobibLink" data-pltdoc="x">2010</a>)</span>.</p><h4>5.5<tt>&nbsp;</tt><a name="(part._.Linear_.Ballistic_.Accumulators)"></a>Linear Ballistic Accumulators</h4><p>Many models of human decision making envision the process as one of evidence accumulation that drifts us towards a threshold. Hit one border and you decide "no"; the other direction and you decide "yes". The amount of time it takes is a proxy for reaction time. The proportion of times you hit one border is a proxy for accuracy. The full drift diffusion models have some complex mathematics so the <span style="font-style: italic">Linear Ballistic Accumulator Model</span><span class="Autobibref">&nbsp;(<a href="#%28autobib._.Scott._.D..._.Brown._and._.Andrew._.Heathcote.The._simplest._complete._model._of._choice._response._time~3a._.Linear._ballistic._accumulation.Cognitive._.Psychology._57%2C._pp..._153--1782008https~3a%2F%2Fdx..doi..org%2F10..1016%2Fj..cogpsych..2007..12..002%29" class="AutobibLink" data-pltdoc="x">Brown and Heathcote</a> <a href="#%28autobib._.Scott._.D..._.Brown._and._.Andrew._.Heathcote.The._simplest._complete._model._of._choice._response._time~3a._.Linear._ballistic._accumulation.Cognitive._.Psychology._57%2C._pp..._153--1782008https~3a%2F%2Fdx..doi..org%2F10..1016%2Fj..cogpsych..2007..12..002%29" class="AutobibLink" data-pltdoc="x">2008</a>)</span> was proposed as a much simpler alternative. For this project you will explain the LBA model and demonstrate an implementation in Racket. It would be nice to be able to show how to fit data to this model, but that will be more challenging. There are some statistics functions in Racket for <a href="https://docs.racket-lang.org/math/stats.html#%28def._%28%28lib._math%2Fstatistics..rkt%29._statistics%29%29">simulation</a>, but this will take more work to get working right.</p><h4>5.6<tt>&nbsp;</tt><a name="(part._.Fitzhugh-.Nagamo_.Neuron_.Model)"></a>Fitzhugh-Nagamo Neuron Model</h4><p>This is a <a href="https://en.wikipedia.org/wiki/FitzHugh%E2%80%93Nagumo_model">model</a> of neuronal firing that is very commonly used as a simpler, but informative, example of neuronal dynamics. For this project you will implement this model and demonstrate how to visualize the effects of parameter manipulations, e.g. by using <a href="https://docs.racket-lang.org/plot/renderer2d.html#%28def._%28%28lib._plot%2Fmain..rkt%29._vector-field%29%29">vector field</a> plots.</p><h4><a name="(part._ref~3aprojects)"></a>Project References</h4><p><table cellspacing="0" cellpadding="0" class="AutoBibliography"><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Scott._.D..._.Brown._and._.Andrew._.Heathcote.The._simplest._complete._model._of._choice._response._time~3a._.Linear._ballistic._accumulation.Cognitive._.Psychology._57,._pp..._153--1782008https~3a//dx..doi..org/10..1016/j..cogpsych..2007..12..002)"></a><span class="Autobibentry">Scott D. Brown and Andrew Heathcote. The simplest complete model of choice response time: Linear ballistic accumulation. <span style="font-style: italic">Cognitive Psychology</span> 57, pp. 153&ndash;178, 2008. <a href="https://dx.doi.org/10.1016/j.cogpsych.2007.12.002"><span class="url">https://dx.doi.org/10.1016/j.cogpsych.2007.12.002</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Peter._.D..._.Bruza,._.Zheng._.Wang,._and._.Jerome._.R..._.Busemeyer.Quantum._cognition~3a._a._new._theoretical._approach._to._psychology.Trends._in._.Cognitive._.Science._19,._pp..._383--3932015http~3a//dx..doi..org/10..1016/j..tics..2015..05..001)"></a><span class="Autobibentry">Peter D. Bruza, Zheng Wang, and Jerome R. Busemeyer. Quantum cognition: a new theoretical approach to psychology. <span style="font-style: italic">Trends in Cognitive Science</span> 19, pp. 383&ndash;393, 2015. <a href="http://dx.doi.org/10.1016/j.tics.2015.05.001"><span class="url">http://dx.doi.org/10.1016/j.tics.2015.05.001</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Pentti._.Kanerva.What._.We._.Mean._.When._.We._.Say._.'.What's._the._.Dollar._of._.Mexico~3f.'._~3a._.Prototypes._and._.Mapping._in._.Concept._.Space.In._.Proc..._.A.A.A.I._.Fall._.Symposium._.Series2010https~3a//www..aaai..org/ocs/index..php/.F.S.S/.F.S.S10/paper/view/2243)"></a><span class="Autobibentry">Pentti Kanerva. What We Mean When We Say "What&rsquo;s the Dollar of Mexico?" : Prototypes and Mapping in Concept Space. In <span style="font-style: italic">Proc. AAAI Fall Symposium Series</span>, 2010. <a href="https://www.aaai.org/ocs/index.php/FSS/FSS10/paper/view/2243"><span class="url">https://www.aaai.org/ocs/index.php/FSS/FSS10/paper/view/2243</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Andrei._.Khrennikov.Open._.Systems,._.Quantum._.Probability._and._.Logic._for._.Quantum-.Like._.Modeling._in._.Biology,._.Cognition,._and._.Decision._.Making.Co.R.R2023http~3a//arxiv..org/abs/2304..08599v1)"></a><span class="Autobibentry">Andrei Khrennikov. Open Systems, Quantum Probability and Logic for Quantum-Like Modeling in Biology, Cognition, and Decision Making. <span style="font-style: italic">CoRR</span>, 2023. <a href="http://arxiv.org/abs/2304.08599v1"><span class="url">http://arxiv.org/abs/2304.08599v1</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Pothos,._.Emmanuel._.M..._and._.Busemeyer,._.Jerome._.R...Quantum._.Cognition.Annual._.Review._of._.Psychology._73,._pp..._749--7782022https~3a//dx..doi..org/10..1146/annurev-psych-033020-123501)"></a><span class="Autobibentry">Pothos, Emmanuel M. and Busemeyer, Jerome R. Quantum Cognition. <span style="font-style: italic">Annual Review of Psychology</span> 73, pp. 749&ndash;778, 2022. <a href="https://dx.doi.org/10.1146/annurev-psych-033020-123501"><span class="url">https://dx.doi.org/10.1146/annurev-psych-033020-123501</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Thomas._.C..._.Schelling.Dynamic._models._of._segregation.The._.Journal._of._.Mathematical._.Sociology._1,._pp..._143--1861971https~3a//doi..org/10..1080/0022250.X..1971..9989794)"></a><span class="Autobibentry">Thomas C. Schelling. Dynamic models of segregation. <span style="font-style: italic">The Journal of Mathematical Sociology</span> 1, pp. 143&ndash;186, 1971. <a href="https://doi.org/10.1080/0022250X.1971.9989794"><span class="url">https://doi.org/10.1080/0022250X.1971.9989794</span></a></span></span></p></td></tr><tr><td><p><span class="Autobibtarget"><a name="(autobib._.Kenneth._.O..._.Stanley,._.Jeff._.Clune,._.Joel._.Lehman,._and._.Risto._.Mikkulainen.Designing._neural._networks._thought._neuroevolution.Nature._.Machine._.Intelligence._1,._pp..._24--352019https~3a//www..researchgate..net/profile/.Jeff-.Clune/publication/330203191_.Designing_neural_networks_through_neuroevolution/links/5e7243fc92851c93e0ac18ea/.Designing-neural-networks-through-neuroevolution..pdf~3f_sg~255.B0~255.D~3dstarted_experiment_milestone~26_sg~255.B1~255.D~3dstarted_experiment_milestone~26origin~3djournal.Detail)"></a><span class="Autobibentry">Kenneth O. Stanley, Jeff Clune, Joel Lehman, and Risto Mikkulainen. Designing neural networks thought neuroevolution. <span style="font-style: italic">Nature Machine Intelligence</span> 1, pp. 24&ndash;35, 2019. <a href="https://www.researchgate.net/profile/Jeff-Clune/publication/330203191_Designing_neural_networks_through_neuroevolution/links/5e7243fc92851c93e0ac18ea/Designing-neural-networks-through-neuroevolution.pdf?_sg%5B0%5D=started_experiment_milestone&amp;_sg%5B1%5D=started_experiment_milestone&amp;origin=journalDetail"><span class="url">https://www.researchgate.net/profile/Jeff-Clune/publication/330203191_Designing_neural_networks_through_neuroevolution/links/5e7243fc92851c93e0ac18ea/Designing-neural-networks-through-neuroevolution.pdf?_sg%5B0%5D=started_experiment_milestone&amp;_sg%5B1%5D=started_experiment_milestone&amp;origin=journalDetail</span></a></span></span></p></td></tr></table></p></div></div><div id="contextindicator">&nbsp;</div></body></html>