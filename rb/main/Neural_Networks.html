<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=0.8"/><title>4&nbsp;Neural Networks</title><link rel="stylesheet" type="text/css" href="scribble.css" title="default"/><link rel="stylesheet" type="text/css" href="racket.css" title="default"/><link rel="stylesheet" type="text/css" href="figure.css" title="default"/><link rel="stylesheet" type="text/css" href="scribble-style.css" title="default"/><script type="text/javascript" src="scribble-common.js"></script><script type="text/javascript" src="figure.js"></script><script type="text/javascript">
(function() {document.write('<scr' + 'ipt type="text/javascript" src="MathJax/MathJax.js?config=default"></scr' + 'ipt>');})();
</script><!--[if IE 6]><style type="text/css">.SIEHidden { overflow: hidden; }</style><![endif]--></head><body id="scribble-racket-lang-org"><div class="tocset"><div class="tocview"><div class="tocviewlist tocviewlisttopspace"><div class="tocviewtitle"><table cellspacing="0" cellpadding="0"><tr><td style="width: 1em;"><a href="javascript:void(0);" title="Expand/Collapse" class="tocviewtoggle" onclick="TocviewToggle(this,&quot;tocview_0&quot;);">&#9660;</a></td><td></td><td><a href="index.html" class="tocviewlink" data-pltdoc="x">Computational Modeling for Psychology</a></td></tr></table></div><div class="tocviewsublisttop" style="display: block;" id="tocview_0"><table cellspacing="0" cellpadding="0"><tr><td align="right">1&nbsp;</td><td><a href="Introductory_Material.html" class="tocviewlink" data-pltdoc="x">Introduction and Computing Requirements</a></td></tr><tr><td align="right">2&nbsp;</td><td><a href="Computation_and_Cognition.html" class="tocviewlink" data-pltdoc="x">What is Computation and is Cognition Computable?</a></td></tr><tr><td align="right">3&nbsp;</td><td><a href="DEs_and_Spikes.html" class="tocviewlink" data-pltdoc="x">Differential Equations and Spiking Neuron Models</a></td></tr><tr><td align="right">4&nbsp;</td><td><a href="" class="tocviewselflink" data-pltdoc="x">Neural Networks</a></td></tr><tr><td align="right">5&nbsp;</td><td><a href="Topics_for_Final_Projects.html" class="tocviewlink" data-pltdoc="x">Topics for Final Projects</a></td></tr></table></div></div><div class="tocviewlist"><table cellspacing="0" cellpadding="0"><tr><td style="width: 1em;"><a href="javascript:void(0);" title="Expand/Collapse" class="tocviewtoggle" onclick="TocviewToggle(this,&quot;tocview_1&quot;);">&#9658;</a></td><td>4&nbsp;</td><td><a href="" class="tocviewselflink" data-pltdoc="x">Neural Networks</a></td></tr></table><div class="tocviewsublistbottom" style="display: none;" id="tocview_1"><table cellspacing="0" cellpadding="0"><tr><td align="right">4.1&nbsp;</td><td><a href="#%28part._.Introduction_to_.Linear_.Algebra_and_.Neural_.Networks%29" class="tocviewlink" data-pltdoc="x">Introduction to Linear Algebra and Neural Networks</a></td></tr><tr><td align="right">4.2&nbsp;</td><td><a href="#%28part._.The_.Math_.That_.Underlies_.Neural_.Networks_%29" class="tocviewlink" data-pltdoc="x">The Math That Underlies Neural Networks?</a></td></tr><tr><td align="right">4.3&nbsp;</td><td><a href="#%28part._.Perceptrons%29" class="tocviewlink" data-pltdoc="x">Perceptrons</a></td></tr></table></div></div></div><div class="tocsub"><div class="tocsubtitle">On this page:</div><table class="tocsublist" cellspacing="0"><tr><td><span class="tocsublinknumber">4.1<tt>&nbsp;</tt></span><a href="#%28part._.Introduction_to_.Linear_.Algebra_and_.Neural_.Networks%29" class="tocsubseclink" data-pltdoc="x">Introduction to Linear Algebra and Neural Networks</a></td></tr><tr><td><span class="tocsublinknumber">4.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Linear_.Algebra_.Goals%29" class="tocsubseclink" data-pltdoc="x">Linear Algebra Goals</a></td></tr><tr><td><span class="tocsublinknumber">4.1.2<tt>&nbsp;</tt></span><a href="#%28part._.Drawing_.Cellular_.Automata%29" class="tocsubseclink" data-pltdoc="x">Drawing Cellular Automata</a></td></tr><tr><td><span class="tocsublinknumber">4.1.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Comments_on_the_programmatic_implementation%29" class="tocsubseclink" data-pltdoc="x">Comments on the programmatic implementation</a></td></tr><tr><td><span class="tocsublinknumber">4.1.3<tt>&nbsp;</tt></span><a href="#%28part._.More_.Lessons_from_.Cellular_.Automata%29" class="tocsubseclink" data-pltdoc="x">More Lessons from Cellular Automata</a></td></tr><tr><td><span class="tocsublinknumber">4.2<tt>&nbsp;</tt></span><a href="#%28part._.The_.Math_.That_.Underlies_.Neural_.Networks_%29" class="tocsubseclink" data-pltdoc="x">The Math That Underlies Neural Networks?</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Linear_.Algebra%29" class="tocsubseclink" data-pltdoc="x">Linear Algebra</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Important_.Objects_and_.Operations%29" class="tocsubseclink" data-pltdoc="x">Important Objects and Operations</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.1.1<tt>&nbsp;</tt></span><a href="#%28part._.Adding_.Matrices%29" class="tocsubseclink" data-pltdoc="x">Adding Matrices</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.1.2<tt>&nbsp;</tt></span><a href="#%28part._.Activity%29" class="tocsubseclink" data-pltdoc="x">Activity</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.2<tt>&nbsp;</tt></span><a href="#%28part._.What_is_a_.Neural_.Network_%29" class="tocsubseclink" data-pltdoc="x">What is a Neural Network?</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.2.1<tt>&nbsp;</tt></span><a href="#%28part._.Non-linearities%29" class="tocsubseclink" data-pltdoc="x">Non-<wbr></wbr>linearities</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.2.2<tt>&nbsp;</tt></span><a href="#%28part._.Exercise_.X.O.R%29" class="tocsubseclink" data-pltdoc="x">Exercise XOR</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.3<tt>&nbsp;</tt></span><a href="#%28part._.Connections%29" class="tocsubseclink" data-pltdoc="x">Connections</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.4<tt>&nbsp;</tt></span><a href="#%28part._.Boolean_.Logic%29" class="tocsubseclink" data-pltdoc="x">Boolean Logic</a></td></tr><tr><td><span class="tocsublinknumber">4.2.1.5<tt>&nbsp;</tt></span><a href="#%28part.__.First_.Order_.Logic_-_.Truth_.Tables%29" class="tocsubseclink" data-pltdoc="x"> First Order Logic -<wbr></wbr> Truth Tables</a></td></tr><tr><td><span class="tocsublinknumber">4.3<tt>&nbsp;</tt></span><a href="#%28part._.Perceptrons%29" class="tocsubseclink" data-pltdoc="x">Perceptrons</a></td></tr><tr><td><span class="tocsublinknumber">4.3.1<tt>&nbsp;</tt></span><a href="#%28part._.Goals%29" class="tocsubseclink" data-pltdoc="x">Goals</a></td></tr><tr><td><span class="tocsublinknumber">4.3.2<tt>&nbsp;</tt></span><a href="#%28part._.Perceptron_.History_and_.Implementation%29" class="tocsubseclink" data-pltdoc="x">Perceptron History and Implementation</a></td></tr><tr><td><span class="tocsublinknumber">4.3.3<tt>&nbsp;</tt></span><a href="#%28part._.The_.Perceptron_.Rules%29" class="tocsubseclink" data-pltdoc="x">The Perceptron Rules</a></td></tr><tr><td><span class="tocsublinknumber">4.3.4<tt>&nbsp;</tt></span><a href="#%28part._.You_.Are_.The_.Perceptron%29" class="tocsubseclink" data-pltdoc="x">You Are The Perceptron</a></td></tr></table></div></div><div class="maincolumn"><div class="main"><div class="navsettop"><span class="navleft"><div class="nosearchform"></div>&nbsp;&nbsp;<span class="tocsettoggle">&nbsp;&nbsp;<a href="javascript:void(0);" title="show/hide table of contents" onclick="TocsetToggle();">contents</a></span></span><span class="navright">&nbsp;&nbsp;<a href="DEs_and_Spikes.html" title="backward to &quot;3 Differential Equations and Spiking Neuron Models&quot;" data-pltdoc="x">&larr; prev</a>&nbsp;&nbsp;<a href="index.html" title="up to &quot;Computational Modeling for Psychology&quot;" data-pltdoc="x">up</a>&nbsp;&nbsp;<a href="Topics_for_Final_Projects.html" title="forward to &quot;5 Topics for Final Projects&quot;" data-pltdoc="x">next &rarr;</a></span>&nbsp;</div><h3>4<tt>&nbsp;</tt><a name="(part._.Neural._.Networks)"></a>Neural Networks</h3><h4>4.1<tt>&nbsp;</tt><a name="(part._.Introduction_to_.Linear_.Algebra_and_.Neural_.Networks)"></a>Introduction to Linear Algebra and Neural Networks</h4><h5>4.1.1<tt>&nbsp;</tt><a name="(part._.Linear_.Algebra_.Goals)"></a>Linear Algebra Goals</h5><p><div class="SIntrapara">Our goal for the next few lessons is to come to understand
</div><div class="SIntrapara"><ul><li><p>What is a neural network?</p></li><li><p>What mathematics are needed to build a neural network?</p></li><li><p>How can neural networks help us understand cognition?</p></li></ul></div></p><p>As a first illustration of some of the key ideas we will execute a simple cellular automata rule. What I hope to emphasize through this exercise is that whenever you can get the computer to do a repetitive task do so. It will do it much better than you. And even if it takes you days to get the program right for many task you will quickly save the time in the long run. Second, we are using a simple rule (as you will shortly see). But even though the rule is local it yields impressive global structure. And very slight tweaks in this local rule can lead to large macroscopic changes. While the variation in our rule is very limited the array of behaviors we can observe is vast. <span class="refelem"><span class="refcolumn"><span class="refcontent">Match these features to facts about neurons. Extend them to what you believe will be their application in neural networks.</span></span></span></p><h5>4.1.2<tt>&nbsp;</tt><a name="(part._.Drawing_.Cellular_.Automata)"></a>Drawing Cellular Automata</h5><p>This activity has several stages. For the first stage make sure you can load the file <a href="&quot;./../code/ca.rkt&quot;"></a> into Dr Racket and that it runs.</p><p>Next, pick a number between 0 and 255 inclusive. In your interactive window use the function <span class="stt">rule-tester</span> to generate the input-output pairing for your rule like so.</p><p><div class="SIntrapara">Testing Rule 22</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">rule-tester</span><span class="hspace">&nbsp;</span><span class="RktVal">22</span><span class="hspace">&nbsp;</span><span class="RktSym">test-set</span><span class="RktPn">)</span></td></tr><tr><td><table cellspacing="0" cellpadding="0"><tr><td><p><span class="RktOut">in (w w w) out w</span></p></td></tr><tr><td><p><span class="RktOut">in (w w b) out b</span></p></td></tr><tr><td><p><span class="RktOut">in (w b w) out b</span></p></td></tr><tr><td><p><span class="RktOut">in (w b b) out w</span></p></td></tr><tr><td><p><span class="RktOut">in (b w w) out b</span></p></td></tr><tr><td><p><span class="RktOut">in (b w b) out w</span></p></td></tr><tr><td><p><span class="RktOut">in (b b w) out w</span></p></td></tr><tr><td><p><span class="RktOut">in (b b b) out w</span></p></td></tr></table></td></tr></table></blockquote></div></p><p>Use your rule and a piece of graph paper to implement your rule.</p><p>Color a single black square in the middle of the top row. Then moving down one row and working left to right implement your rule by coloring in the appropriate square.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="grid.png" alt="" width="359" height="240"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3agrid-automata))" x-target-lift="Figure"></a>Figure&nbsp;5: </span>Nearest Neighbors in the Grid</span></p></blockquote><p>For example, if the boxes 1, 2, and 3 were &rsquo;w, &rsquo;w, and &rsquo;b I could color the square with the question mark black. Then I would move one to the right and square 2 would become my new number 1 and so on.</p><p>Complete several rows following your rule.</p><p>What you have probably noticed is that this is tedious and mistake prone, but your rule is a good example of a function. A function can be conceived as a set of pairs. The first element of the pair is the input, and the second element of the pair is the output. Functions require that each input element be unique. Implementing your rule  makes you the metaphorical neuron deciding whether or not to fire (color the square black) based on the input you receive from neighboring neurons.</p><p>Having learned how tedious and error prone this process explore some of the other rules using the functions in <span class="stt">ca.rkt</span>. The simplest method is to use the function <span class="stt">d-r-a &lt;some-rule-number&gt;</span>. You can adjust the size and scale with various optional arguments and even print it to a file if you find one you like. Here is one of my favorites as a demonstration.</p><p><div class="SIntrapara">Rule 110</div><div class="SIntrapara"><blockquote class="SCodeFlow"><table cellspacing="0" cellpadding="0" class="RktBlk"><tr><td><span class="stt">&gt; </span><span class="RktPn">(</span><span class="RktSym">d-r-a</span><span class="hspace">&nbsp;</span><span class="RktVal">110</span><span class="hspace">&nbsp;</span><span class="RktPn">#:num-rows</span><span class="hspace">&nbsp;</span><span class="RktVal">100</span><span class="hspace">&nbsp;</span><span class="RktPn">#:num-cols</span><span class="hspace">&nbsp;</span><span class="RktVal">100</span><span class="hspace">&nbsp;</span><span class="RktPn">#:scale</span><span class="hspace">&nbsp;</span><span class="RktVal">3</span><span class="RktPn">)</span></td></tr><tr><td><p><img src="pict_8.png" alt="image" width="300" height="303"/></p></td></tr></table></blockquote></div></p><p><div class="SIntrapara">What are the lessons learned from this exercise?
</div><div class="SIntrapara"><ol><li><p>Repetitive actions are hard. We (humans) make mistakes following even simple rules for a large number of repeated steps. Better to let the computer do it since that is where its strengths lie.</p></li><li><p>Complex global patterns can emerge from local actions. Each neuron is only responding to its immediate right and left yet global structures emerge.</p></li><li><p>These characteristics seem similar to brain activity. Each neuron in the brain is just one of many. Whether a neuron spikes or not is a consequence of its own state and its inputs (like the neighbors in the grid example).</p></li><li><p>From each neuron making a local computation, global patterns of complex activity can emerge.</p></li><li><p>Maybe by programming something similar to this system we can get insights into brain activity.</p></li></ol></div></p><h5>4.1.2.1<tt>&nbsp;</tt><a name="(part._.Comments_on_the_programmatic_implementation)"></a>Comments on the programmatic implementation</h5><p>The code in <span class="stt">ca.rkt</span> involves a lot of looping. I used <span style="font-style: italic">for</span> loops extensively, though sometimes these were <span class="stt">for/fold</span> variants. We need to inch along the columns and down the rows. The plotting used the built in functionality of <span style="font-weight: bold">racket</span> for generating pictures as output.</p><p>The potentially more tricky part was going from a number (in decimal) to a binary representation that had the right number of places occupied. I ended going back and forth between strings and lists to get what I wanted. This was undoubtedly a kludge, but there is a slogan to first get it working, and then make it better. Trying to be too perfect and too elegant can cost you time in the long run. It is often easier to revise a functioning program then write one from the start.</p><p>Initially I did not have all the testing code, because I was adapting code I had written in the past. However, when things did not work it turned out I went faster by slowing down and writing code that allowed me to inspect the state of my various variables, and individually try out the small functions on test input.</p><h5>4.1.3<tt>&nbsp;</tt><a name="(part._.More_.Lessons_from_.Cellular_.Automata)"></a>More Lessons from Cellular Automata</h5><p>Cellular automata demonstrate some basic lessons that we will make use of when thinking about neural networks. One of these points is that there may be simple representations for complex entities. If we can find the right language for representation we may get concision and repeatability as by-products. This is demonstrated by the <a href="https://plato.stanford.edu/entries/cellular-automata/supplement.html">naming convention for the rules of cellular automata</a>.</p><p>In emphasizing that local decisions can produce interesting global effects it may be interesting to examine other similar uses of the cellular automata idea. One famous and visually pleasing one is the <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Game of Life</a>.</p><p>The analogy of automata to simple neurons may be deeper than at first it appears. Some very famous thinkers connected the two. One of the most brilliant people of all time, John von Neumann, was working on a book about automata and the brain at the time of his death. I have linked to a commentary in case you are interested in reading further see <a href="http://www.ams.org/bull/1958-64-03/S0002-9904-1958-10214-1/S0002-9904-1958-10214-1.pdf">Claude Shannon (pdf)</a> as well as to a pdf <a href="https://complexityexplorer.s3.amazonaws.com/supplemental_materials/5.6+Artificial+Life/The+Computer+and+The+Brain_text.pdf">copy</a> of the book: <a href="https://ocul-wtl.primo.exlibrisgroup.com/permalink/01OCUL_WTL/vk29fk/alma994863683505162">The Computer and the Brain</a>).</p><p>A contemporary mathematician and the inventor of the Mathematica software system also believes that cellular automata may be a theory of everything. See what Stephen Wolfram <a href="http://www.wolframscience.com">thinks</a>.</p><h4>4.2<tt>&nbsp;</tt><a name="(part._.The_.Math_.That_.Underlies_.Neural_.Networks_)"></a>The Math That Underlies Neural Networks?</h4><h5>4.2.1<tt>&nbsp;</tt><a name="(part._.Linear_.Algebra)"></a>Linear Algebra</h5><p>The math at the heart of neural networks and their computer implementation is <span style="font-style: italic"><span style="font-weight: bold">linear algebra</span></span>. For us, the section of linear algebra we are going to need is mostly limited to vectors, matrices and how to add and multiply them.</p><h5>4.2.1.1<tt>&nbsp;</tt><a name="(part._.Important_.Objects_and_.Operations)"></a>Important Objects and Operations</h5><ol><li><p>Vectors</p></li><li><p>Matrices</p></li><li><p>Scalars</p></li><li><p>Addition</p></li><li><p>Multiplication (scalar and matrix)</p></li><li><p>Transposition</p></li><li><p>Inverse</p></li></ol><h5>4.2.1.1.1<tt>&nbsp;</tt><a name="(part._.Adding_.Matrices)"></a>Adding Matrices</h5><p>To gain some hands on familiarity with the manipulation of matrices and vectors we will try to do some hand and programming exercises for some of the fundamental operations of addition and multiplication. We will also thereby learn that some of the rules we learned for numbers (such as a * b = b * a) do not always apply in other mathematical realms.</p><p>There are in fact many ways to think about what a vector is.</p><p>It can be thought of as a column (or row of numbers).
More abstractly it is an object (arrow) with magnitude and direction.
Most abstractly it is anything that obeys the requirements of a vector space.</p><p>For particular circumstances one or another of the different definitions may serve our purposes better. In application to neural networks we often just use the first definition, a column of numbers, but the second can be more helpful for developing our geometric intuitions about what various learning rules are doing and how they do it.</p><p>Similarly, we often just consider a matrix as a collection of vectors or as a rectangular (2-D) collection of numbers.</p><h5>4.2.1.1.2<tt>&nbsp;</tt><a name="(part._.Activity)"></a>Activity</h5><p>Look up how racket handles <a href="https://docs.racket-lang.org/math/matrices.html">matrices and vectors</a>. Here is a very simple <a href="./../code/la-demo.rkt">file</a> to try and get started.</p><p>{<span style="font-weight: bold">Important</span>: vectors are a special datatype in Racket, and the vector type is probably not what you want to be using. Look for matrices and linear algebra}</p><p>Make two arrays and make them the same size<span class="refelem"><span class="refcolumn"><span class="refcontent">What is the <span style="font-style: italic">size</span> of a matrix?</span></span></span>.</p><p>Add them together in both orders (A + B and B + A). How does one add an array that itself has numerous different numbers?</p><p>Then do the same for multiplication. Note that there are particular requirements for the sizes of matrices in order that it is possible to multiply them in both directions. What is that rule?</p><p>What is the name for the property of having A*B = B*A?</p><p>** Common Notational Conventions for Vectors and Matrices</p><p>Vectors tend to be notated as <span style="font-style: italic">lower case</span> letters, often in bold, such
as <span class="math">\mathbf{a}</span>. They are also occasionally represented with little
arrows on top such as <span class="math">\overrightarrow{\textbf{a}}</span>.</p><p>Matrices tend to be notated as <span style="font-style: italic">upper case</span> letters, typically in bold,
such as <span class="math">\mathbf{M}</span>.</p><p>Good things to know: what is an <span style="font-style: italic">inner product</span>? How do you compute it in racket?</p><h5>4.2.1.2<tt>&nbsp;</tt><a name="(part._.What_is_a_.Neural_.Network_)"></a>What is a Neural Network?</h5><p>What is a Neural Network? It is a brain inspired computational approach
in which "neurons" compute functions of their inputs and pass on a
<span style="font-style: italic">weighted</span> proportion to the next neuron in the chain.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="nn.png" alt="" width="400" height="136"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig-nn))" x-target-lift="Figure"></a>Figure&nbsp;6: </span>simple schematic of the basics of a neural network. This is an image for a single neuron. The input has three elements and each of these connects to the same neuron ("node 1"). The activity at those nodes is filtered by the weights, which are specific for each of the inputs. These three processed inputs are combined to generate the output from this neuron. For multiple layers this output becomes an input for the next neuron along the chain.</span></p></blockquote><h5>4.2.1.2.1<tt>&nbsp;</tt><a name="(part._.Non-linearities)"></a>Non-linearities</h5><p>The spiking of a biological neuron is non-linear. You saw this in both the integrate and fire and Hodgkin and Huxley models you programmed. The lines on those plots you created are not, for the most part, straight. Perhaps the simplest way to incorporate a non-linearity into our artificial neuron is to give it a threshold, like we did for the integrate and fire model. When activity exceeds the threshold (which we will usually designate with a capital Greek Theta <span class="math">\Theta</span> then the neuron is set to 1 and if it is not firing it is set to 0 (like the "w" &#8594; 0; "b" &#8594; 1 mapping we used for the cellular automata).</p><p><div class="math">\begin{equation}
\mbox{if } I_1 \times w_{1,1} + I_2 \times w_{2,1} + I_3 \times w_{3,1} &gt; \Theta \mbox{ then } Output = 1
\end{equation}</div></p><p>What this equation shows is that Inputs (the <span class="math">I</span>s) are passed to a neuron. Those inputs have something like a synapse. That is designated by the w&rsquo;s. Those weights are how tightly the input and internal activity of our artificial neuron is coupled. The reason for all the subscripts is to try and help you see the similarity between this equation and the inner product and matrix multiplication rules you just worked on programming. The activity of the neuron is a sort of internal state, and then, based on the comparison of that activity to the threshold, you can envision the neuron spiking or not, meaning it has value 1 or 0. Mathematically, the weighted sum is fed into a threshold function that compares the value to a threshold <span class="math">\Theta</span>, and passes on the value 1 if it is greater than the threshold and 0 (sometimes <span class="math">-1</span> rather than zero is chosen for the inactive state because there are certain computational conveniences in doing so).</p><p>To prepare you for the next steps in writing a simple percetron (the earliest form of artificial neural network), you should try to answer the followign questons.</p><p><div class="SIntrapara">Questions:
</div><div class="SIntrapara"><ol><li><p>What, geometrically speaking, is a plane?</p></li><li><p>What is a hyperplane?</p></li><li><p>What is linearly separability and how does that relate to planes and
hyperplanes?</p></li></ol></div></p><p>One of our first efforts will be to code a <span style="font-style: italic">perceptron</span> to solve the XOR problem. In order for this to happen you need to know a bit about <span style="font-style: italic">Boolean</span> functions and what an XOR problem actually is.</p><p><span style="font-weight: bold">Examples of Boolean Functions and How They Map onto our Neural Network Intuitions</span></p><p>The "AND" Operation/Function</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="pict_9.png" alt="image" width="400" height="400"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3aand))" x-target-lift="Figure"></a>Figure&nbsp;7: </span>The <span style="font-style: italic">and</span> operation is true when both its inputs are true.</span></p></blockquote><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="pict_10.png" alt="image" width="400" height="400"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3aor))" x-target-lift="Figure"></a>Figure&nbsp;8: </span>The <span style="font-style: italic">or</span> operation is true if either or both of its inputs are true.</span></p></blockquote><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="pict_11.png" alt="image" width="400" height="400"/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3axor))" x-target-lift="Figure"></a>Figure&nbsp;9: </span>The <span style="font-style: italic">xor</span> is true when one or the other, but not both of the inputs are true. It is exclusively an or function.</span></p></blockquote><p>This short <a href="https://media.nature.com/m685/nature-assets/nbt/journal/v26/n2/images/nbt1386-F1.gif">article</a> provides a nice example of linear separability and some basics of what a neural network is.</p><h5>4.2.1.2.2<tt>&nbsp;</tt><a name="(part._.Exercise_.X.O.R)"></a>Exercise XOR</h5><p>Using only <span style="font-style: italic">not</span>, <span style="font-style: italic">and</span>, and <span style="font-style: italic">or</span> operations draw the diagram that allows you to compute in two steps the <span style="font-style: italic">xor</span> operation. You will need this to code it up as a perceptron.</p><h5>4.2.1.3<tt>&nbsp;</tt><a name="(part._.Connections)"></a>Connections</h5><p>Can neural networks encode logic? Is the processing zeros and ones enough to capture the richness of human intellectual activity?</p><p>There is a long tradition of representing human thought as the consequence of some sort of calculation of two values (true or false). If you have two values you can swap out 1&rsquo;s and 0&rsquo;s for the true and false in your calculation. They even seem to obey similar laws. If you the conjunction (AND) of two true things it is only true when both are true. If you take T = 1, then T &#8743; T is the same as <span class="math">1~\times~1</span>.</p><p>We will next build up a simple threshold neural unit and try to calculate some of these truth functions with our neuron. We will build simple neurons for truth tables (like those that follow), and string them together into an argument. Then we can feed values of T and F into our network and let it calculate the XOR problem.</p><h5>4.2.1.4<tt>&nbsp;</tt><a name="(part._.Boolean_.Logic)"></a>Boolean Logic</h5><p>George Boole, Author of the <span style="font-style: italic">Laws of Thought</span></p><ul><li><p>Read the <a href="https://archive.org/details/investigationofl00boolrich">book</a> on Archive.org</p></li><li><p>Read about <a href="https://plato.stanford.edu/entries/boole/#LifWor">George Boole</a></p></li></ul><h5>4.2.1.5<tt>&nbsp;</tt><a name="(part.__.First_.Order_.Logic_-_.Truth_.Tables)"></a> First Order Logic - Truth Tables</h5><p><span style="font-weight: bold">Or</span></p><p><table cellspacing="0" cellpadding="0"><tr><td><p><span style="font-weight: bold">Pr A</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Pr B</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Or</span></p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr></table></p><p><span style="font-weight: bold">And</span></p><p><table cellspacing="0" cellpadding="0"><tr><td><p><span style="font-weight: bold">Pr A</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Pr B</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Or</span></p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr></table></p><p><span style="font-weight: bold">Nand</span></p><p><table cellspacing="0" cellpadding="0"><tr><td><p><span style="font-weight: bold">Pr A</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Pr B</span></p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p><span style="font-weight: bold">Or</span></p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td></tr><tr><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>1</p></td><td><p><span class="hspace">&nbsp;</span></p></td><td><p>0</p></td></tr></table></p><h4>4.3<tt>&nbsp;</tt><a name="(part._.Perceptrons)"></a>Perceptrons</h4><h5>4.3.1<tt>&nbsp;</tt><a name="(part._.Goals)"></a>Goals</h5><p>The goal for this file is to share the idea of a perceptron, the mathematical formula for updating one, and iniate the process of coding a simple implementation that we will adapt to the delta rule.</p><h5>4.3.2<tt>&nbsp;</tt><a name="(part._.Perceptron_.History_and_.Implementation)"></a>Perceptron History and Implementation</h5><p>The perceptron was the invention of a psychologist, <a href="http://dspace.library.cornell.edu/bitstream/1813/18965/2/Rosenblatt_Frank_1971.pdf">Frank Rosenblatt</a>.  He was not a computer scientist. Though he obviously had a bit of the mathematician in him.</p><blockquote class="Figure"><blockquote class="Centerfigure"><blockquote class="FigureInside"><p><img src="Mark_I_perceptron.jpeg" alt=""/></p></blockquote></blockquote><p class="Centertext"><span class="Legend"><span class="FigureTarget"><a name="(counter._(figure._fig~3amark.I))" x-target-lift="Figure"></a>Figure&nbsp;10: </span>The Perceptron Mark I</span></p></blockquote><p>Details to be found on the <a href="https://en.wikipedia.org/wiki/Perceptron">wikipedia page</a>.</p><p>Those interested in some interesting background reading could consult his over 600 page book entitled <a href="https://babel.hathitrust.org/cgi/pt?id=mdp.39015039846566&amp;view=1up&amp;seq=9">Principles of Neurodynamics</a> or this <a href="https://link.springer.com/book/10.1007/978-3-642-70911-1">historical review</a>.</p><p>From the foreward of that book we have the following quote:</p><p>"For this writer, the perceptron program is not primarily concerned with the invention of devices for "artificial intelligence", but rather with investigating the physical structures and neurodynamic principles which under lie "natural intelligence". A perceptron is first and fore most a brain model, not an invention for pattern recognition. As a brain model, its utility is in enabling us to determine the physical conditions for the emergence of various psychological properties."</p><h5>4.3.3<tt>&nbsp;</tt><a name="(part._.The_.Perceptron_.Rules)"></a>The Perceptron Rules</h5><p>The perceptron rules are the equations that characterize what a perceptron is, and what it does in contact with experience, so that it can learn and revise its behavior. A lot can be done with these simple equations.</p><p><span class="math">I = i\sum_{i=1}^{n} w_i~x_i</span></p><p>If <span class="math">I \ge T</span> then <span class="math">y = +1</span> else if <span class="math">I &lt; T</span> then <span class="math">y = -1</span></p><p>If the answer was correct, then <span class="math">\beta = +1</span>, else if the
answer was incorrect then <span class="math">\beta = -1</span>.</p><p>Updating is done by <span class="math">\mathbf{w_{new}} =
\mathbf{w_{old}} + \beta y \mathbf{x}</span></p><h5>4.3.4<tt>&nbsp;</tt><a name="(part._.You_.Are_.The_.Perceptron)"></a>You Are The Perceptron</h5><p>This is a pencil and paper exercise. Before coding it is often a good idea to try and work the basics out by hand. This may be a flow chart or a simple hand worked example. This both gives you a simple test case to compare your code against, but more importantly makes sure that you understand what you are trying to code. Let&rsquo;s make sure you understand how to compute the perceptron learning rule, but doing a simple case by hand.</p><p>Beginning with an input of <span class="math">\begin{bmatrix}0.3 \\ 0.7 \end{bmatrix}</span>, an initial set of weights of <span class="math">\begin{bmatrix}-0.6 \\ 0.8 \end{bmatrix}</span>, and a <span style="font-weight: bold">class</span> of 1. Compute the value of the new weight vector with pen and paper.</p><p>IAMHERE - consult perceptron.org for next steps.</p><div class="navsetbottom"><span class="navleft"><div class="nosearchform"></div>&nbsp;&nbsp;<span class="tocsettoggle">&nbsp;&nbsp;<a href="javascript:void(0);" title="show/hide table of contents" onclick="TocsetToggle();">contents</a></span></span><span class="navright">&nbsp;&nbsp;<a href="DEs_and_Spikes.html" title="backward to &quot;3 Differential Equations and Spiking Neuron Models&quot;" data-pltdoc="x">&larr; prev</a>&nbsp;&nbsp;<a href="index.html" title="up to &quot;Computational Modeling for Psychology&quot;" data-pltdoc="x">up</a>&nbsp;&nbsp;<a href="Topics_for_Final_Projects.html" title="forward to &quot;5 Topics for Final Projects&quot;" data-pltdoc="x">next &rarr;</a></span>&nbsp;</div></div></div><div id="contextindicator">&nbsp;</div></body></html>