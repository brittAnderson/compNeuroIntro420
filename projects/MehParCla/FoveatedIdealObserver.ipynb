{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Recognition \n",
    "#### *Jordan Mehravar, Matthew Clappison, Karisa Parkington*\n",
    "\n",
    "\n",
    "# **Agenda**\n",
    "* *Introduction*\n",
    "* *Demonstration*\n",
    "* *Current Models*\n",
    "\n",
    "# **Facial Recognition**\n",
    "* Human face is like a complex fingerprint\n",
    "* Evolutionary Purpose\n",
    "* Humans vs. Machines\n",
    "* Complexity and robustness of the human visual-recognition system\n",
    "  * Fusiform face area\n",
    "  * Speed at which information is processed\n",
    "  * Combines many mental processes such as **perception, memory and judgment**\n",
    "  * Problems that impact the accuracy of facial recognition\n",
    "   * **Pose**\n",
    "   * **Age**\n",
    "   * **Lighting**\n",
    "   * **Emotional State**\n",
    "* Applications\n",
    " \n",
    "\n",
    "# Background \n",
    "#### <u>Fovea</u>- The fovea is a portion of the retina where the visual resolution is highest. The center of your vision is where the fovea is focused.\n",
    "![alt text](https://i.imgur.com/2B8kEdA.png width=100)\n",
    "#### <u>Saccade</u>- A rapid eye movement between two points. During this time the visual system is not able to take in information. The saccade reaches speeds of 900 degrees/second.\n",
    "\n",
    "# Original Work- *How does the human visual recognition system work?*\n",
    "\n",
    "\n",
    "## Nigel D. Haig (1985) \n",
    "**How faces differ a new comparative technique - Where do humans look to accuratly identify faces?**\n",
    "* Designed a program (before the time of eye tracking software) to determine location of optimal fixation point\n",
    "* Humans show a strong preference for the eyes, eye brows, upper lip and mouth area with majority of the focus on the eyes\n",
    "\n",
    "\n",
    "# Demonstration\n",
    "\n",
    "\n",
    "## Schyns et al. (2002)\n",
    "**Understanding Recognition From the use of visual information**\n",
    "* Determined how much information is required to reach a correct conclussion 75% of the time across 3 tasks of identity, gender and expressivness \n",
    "* Demonstrated that different recognition tasks also benefit from different sets of data ( more focus on the mouth when idenitfying gender )\n",
    "![alt text](https://www.researchgate.net/profile/Caroline_Blais/publication/45440659/figure/fig2/AS:340928754667529@1458295157799/Visual-information-used-effectively-to-identify-faces-a-in-our-study-B-in-Schyns-et.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Peterson & Eckstein (2012, 2013) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](https://i.imgur.com/sCjMk9Z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spatially Variant Contrast-Sensitivity Function (SVCSF) \n",
    "\n",
    "A computational model which simulates human vision.\n",
    "<br> <br>\n",
    "\n",
    "![alt text](http://home.deib.polimi.it/boracchi/Projects/Foveation/Lena_Foveated.png)\n",
    "\n",
    "<br>\n",
    "Each face image is divided into bins and Contrast-Sensitivity Function applied to each bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spatially Variant Contrast-Sensitivity Function (SVCSF)\n",
    "<br>\n",
    "# $$SVCSF (f,r,\\theta) = c_0f^{a_0}e^{-b_0f - d_0(\\theta)rf}$$\n",
    "<br>\n",
    "**$a_o, b_o, c_o$** = predetermined constants <br>\n",
    "*Bandpass filter of basic vision spatial frequencies <br>\n",
    "Achieve a peak constrast sensitivity of 1 & a foveal peak frequency of 4 Hz* \n",
    "<br><br>\n",
    "**$f$** = image spatial frequency (Hz) --> variable <br>\n",
    "*Changes with face image presentation, but not across fixations*\n",
    "<br><br>\n",
    "**$d_o$** = eccentricity factor --> constant (0.3-0.5) <br> *Exponential decay rate of the visual percept across the human visual field*\n",
    "<br><br>\n",
    "**$r$** = eccentricity from fovea (degree of visual angle) --> variable <br>\n",
    "*Changes with each fixation*\n",
    "<br><br>\n",
    "**$\\theta$** = coordinate direction from fixation --> vector/matrix <br>\n",
    "*Rotational angle from horizontal axis which can have horizontal, upward, and downward direction <br>\n",
    "Changes with each fixation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Foveated Ideal Observer (FIO)\n",
    "<br>\n",
    "A computational model which determines the identity of a face, given visual fixation information. <br> <br>\n",
    "Based on trained data, what is the likihood that face *i* is shown, given fixation *k*? <br>\n",
    "*Supervised learning* <br><br>\n",
    "Input face signal = filtered underlying signal ($\\textbf{s}_0$ - based on SVCSF) + Gaussian noise ($\\textbf{n}_{ex}$) + unfiltered internal noise ($\\textbf{n}_{in}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](https://i.imgur.com/2TBHtFN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Foveated Ideal Observer (FIO)\n",
    "<br>\n",
    "# $$\\ell_{i,k} = e^{-\\frac{1}{2}{{(\\textbf{r}_k - {\\boldsymbol\\mu}_{i,k})}}^T{\\Sigma}^{-1}_k{(\\textbf{r}_k - {\\boldsymbol\\mu_{i,k}})}}$$\n",
    "<br><br>\n",
    "$\\ell_{i,k}$ = likelihood that face $i$ is presented <br>\n",
    "In the end, the FIO will take the maximum likelihood (i.e., the face that is most likely to be presented given all fixation information) <br><br>\n",
    "$\\textbf{r}_k$ = dot product of template responses --> vector <br>\n",
    "*Filtered underlying signal and filtered noise-free templates <br>\n",
    "$$\\textbf{r}_k = [\\textbf{SVCSF}_k\\textbf{s}_1,...,\\textbf{SVCSF}_k\\textbf{s}_n]^T(\\textbf{SVCSF}_k(\\textbf{s}_0 + \\textbf{n}_{ex}) + \\textbf{n}_{in})$$\n",
    "<br><br>\n",
    "*$\\boldsymbol\\mu_{i,k}$* = expected mean response --> vector <br> \n",
    "*Assuming face $i$ is presented* <br>\n",
    "$${\\boldsymbol\\mu}_{i,f,k} = E[r_{i,f,k}]$$ <br>\n",
    "where E[...] indicates the expected value operator\n",
    "<br><br>\n",
    "Note the transposition (*T*) components. <br>\n",
    "*Vector multiplication*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Research Implications\n",
    "<br>\n",
    "Classifying and recognizing identity under different conditions (e.g., facial expression, gaze direction, gender, ethnicity, occluded/missing features, etc.)\n",
    "<br><br>\n",
    "Quantifying individual differences in face recognition performance and fixation viewing strategies\n",
    "<br><br>\n",
    "Quantifying face recognition abilities and fixation viewing strategies in clinical populations who show face recognition impairments (e.g., prosopagnosia/faceblindness, autism spectrum disorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](https://i.imgur.com/BYYCZeD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Klin et al. (2002); Pelphreys et al. (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](https://i.imgur.com/O0TiF7s.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](https://i.imgur.com/lvkQg01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
